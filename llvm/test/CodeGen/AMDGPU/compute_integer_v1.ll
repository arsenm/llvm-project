; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 2
; RUN: llc -mtriple=amdgcn-amd-amdpal -mcpu=gfx90a < %s | FileCheck -check-prefixes=GFX9,GFX90A %s

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(argmem: write)
define amdgpu_kernel void @compute_integer_v1(ptr addrspace(1) nocapture noundef writeonly align 4 %ptr, i32 noundef %_A) #0 {
; GFX9-LABEL: compute_integer_v1:
; GFX9:       ; %bb.0: ; %entry
; GFX9-NEXT:    s_load_dword s8, s[0:1], 0x8
; GFX9-NEXT:    s_load_dwordx2 s[4:5], s[0:1], 0x0
; GFX9-NEXT:    s_load_dword s3, s[0:1], 0x18
; GFX9-NEXT:    s_load_dwordx2 s[6:7], s[0:1], 0x34
; GFX9-NEXT:    s_waitcnt lgkmcnt(0)
; GFX9-NEXT:    s_add_i32 s8, s8, 1
; GFX9-NEXT:    v_mul_lo_u32 v1, s8, v0
; GFX9-NEXT:    v_add_u32_e32 v2, s8, v1
; GFX9-NEXT:    v_mul_lo_u32 v2, v2, v0
; GFX9-NEXT:    v_add_u32_e32 v1, 1, v1
; GFX9-NEXT:    v_mul_lo_u32 v3, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v1, v3, v1
; GFX9-NEXT:    v_mul_lo_u32 v1, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v1, v2, v1
; GFX9-NEXT:    v_add_u32_e32 v2, 1, v3
; GFX9-NEXT:    v_mul_lo_u32 v3, v1, v2
; GFX9-NEXT:    v_add_u32_e32 v2, v3, v2
; GFX9-NEXT:    v_mul_lo_u32 v2, v2, v1
; GFX9-NEXT:    v_mad_u64_u32 v[4:5], s[0:1], v2, v3, v[2:3]
; GFX9-NEXT:    v_mad_u64_u32 v[2:3], s[0:1], v4, v2, v[4:5]
; GFX9-NEXT:    s_and_b32 s0, s3, 0xffff
; GFX9-NEXT:    s_mul_i32 s2, s2, s0
; GFX9-NEXT:    v_add_u32_e32 v0, s2, v0
; GFX9-NEXT:    v_mov_b32_e32 v1, s7
; GFX9-NEXT:    v_add_co_u32_e32 v0, vcc, s6, v0
; GFX9-NEXT:    v_addc_co_u32_e32 v1, vcc, 0, v1, vcc
; GFX9-NEXT:    v_lshlrev_b64 v[0:1], 2, v[0:1]
; GFX9-NEXT:    v_mov_b32_e32 v3, s5
; GFX9-NEXT:    v_add_co_u32_e32 v0, vcc, s4, v0
; GFX9-NEXT:    v_addc_co_u32_e32 v1, vcc, v3, v1, vcc
; GFX9-NEXT:    global_store_dword v[0:1], v2, off
; GFX9-NEXT:    s_endpgm
entry:
  %i = tail call i32 @llvm.amdgcn.workitem.id.x(), !range !0
  %conv10239 = add i32 %_A, 1
  %add = mul i32 %conv10239, %i
  %mul110240 = add i32 %add, %conv10239
  %add2 = mul i32 %mul110240, %i
  %add210241 = add i32 %add, 1
  %add4 = mul i32 %add2, %add210241
  %mul510242 = add i32 %add4, %add210241
  %add6 = mul i32 %mul510242, %add2
  %add610243 = add i32 %add4, 1
  %add8 = mul i32 %add6, %add610243
  %mul910244 = add i32 %add8, %add610243
  %add10 = mul i32 %mul910244, %add6
  %add1010245 = add i32 %add8, 1
  %add12 = mul i32 %add10, %add1010245
  %mul1310246 = add i32 %add12, %add1010245
  %add14 = mul i32 %mul1310246, %add10
  %add1410247 = add i32 %add12, 1
  %add16 = mul i32 %add14, %add1410247
  %mul1710248 = add i32 %add16, %add1410247
  %add18 = mul i32 %mul1710248, %add14
  %add1810249 = add i32 %add16, 1
  %add20 = mul i32 %add18, %add1810249
  %mul2110250 = add i32 %add20, %add1810249
  %add22 = mul i32 %mul2110250, %add18
  %add2210251 = add i32 %add20, 1
  %add24 = mul i32 %add22, %add2210251
  %mul2510252 = add i32 %add24, %add2210251
  %add26 = mul i32 %mul2510252, %add22
  %add2610253 = add i32 %add24, 1
  %add28 = mul i32 %add26, %add2610253
  %mul2910254 = add i32 %add28, %add2610253
  %add30 = mul i32 %mul2910254, %add26
  %add3010255 = add i32 %add28, 1
  %add32 = mul i32 %add30, %add3010255
  %mul3310256 = add i32 %add32, %add3010255
  %add34 = mul i32 %mul3310256, %add30
  %add3410257 = add i32 %add32, 1
  %add36 = mul i32 %add34, %add3410257
  %mul3710258 = add i32 %add36, %add3410257
  %add38 = mul i32 %mul3710258, %add34
  %add3810259 = add i32 %add36, 1
  %add40 = mul i32 %add38, %add3810259
  %mul4110260 = add i32 %add40, %add3810259
  %add42 = mul i32 %mul4110260, %add38
  %add4210261 = add i32 %add40, 1
  %add44 = mul i32 %add42, %add4210261
  %mul4510262 = add i32 %add44, %add4210261
  %add46 = mul i32 %mul4510262, %add42
  %add4610263 = add i32 %add44, 1
  %add48 = mul i32 %add46, %add4610263
  %mul4910264 = add i32 %add48, %add4610263
  %add50 = mul i32 %mul4910264, %add46
  %add5010265 = add i32 %add48, 1
  %add52 = mul i32 %add50, %add5010265
  %mul5310266 = add i32 %add52, %add5010265
  %add54 = mul i32 %mul5310266, %add50
  %add5410267 = add i32 %add52, 1
  %add56 = mul i32 %add54, %add5410267
  %mul5710268 = add i32 %add56, %add5410267
  %add58 = mul i32 %mul5710268, %add54
  %add5810269 = add i32 %add56, 1
  %add60 = mul i32 %add58, %add5810269
  %mul6110270 = add i32 %add60, %add5810269
  %add62 = mul i32 %mul6110270, %add58
  %add6210271 = add i32 %add60, 1
  %add64 = mul i32 %add62, %add6210271
  %mul6510272 = add i32 %add64, %add6210271
  %add66 = mul i32 %mul6510272, %add62
  %add6610273 = add i32 %add64, 1
  %add68 = mul i32 %add66, %add6610273
  %mul6910274 = add i32 %add68, %add6610273
  %add70 = mul i32 %mul6910274, %add66
  %add7010275 = add i32 %add68, 1
  %add72 = mul i32 %add70, %add7010275
  %mul7310276 = add i32 %add72, %add7010275
  %add74 = mul i32 %mul7310276, %add70
  %add7410277 = add i32 %add72, 1
  %add76 = mul i32 %add74, %add7410277
  %mul7710278 = add i32 %add76, %add7410277
  %add78 = mul i32 %mul7710278, %add74
  %add7810279 = add i32 %add76, 1
  %add80 = mul i32 %add78, %add7810279
  %mul8110280 = add i32 %add80, %add7810279
  %add82 = mul i32 %mul8110280, %add78
  %add8210281 = add i32 %add80, 1
  %add84 = mul i32 %add82, %add8210281
  %mul8510282 = add i32 %add84, %add8210281
  %add86 = mul i32 %mul8510282, %add82
  %add8610283 = add i32 %add84, 1
  %add88 = mul i32 %add86, %add8610283
  %mul8910284 = add i32 %add88, %add8610283
  %add90 = mul i32 %mul8910284, %add86
  %add9010285 = add i32 %add88, 1
  %add92 = mul i32 %add90, %add9010285
  %mul9310286 = add i32 %add92, %add9010285
  %add94 = mul i32 %mul9310286, %add90
  %add9410287 = add i32 %add92, 1
  %add96 = mul i32 %add94, %add9410287
  %mul9710288 = add i32 %add96, %add9410287
  %add98 = mul i32 %mul9710288, %add94
  %add9810289 = add i32 %add96, 1
  %add100 = mul i32 %add98, %add9810289
  %mul10110290 = add i32 %add100, %add9810289
  %add102 = mul i32 %mul10110290, %add98
  %add10210291 = add i32 %add100, 1
  %add104 = mul i32 %add102, %add10210291
  %mul10510292 = add i32 %add104, %add10210291
  %add106 = mul i32 %mul10510292, %add102
  %add10610293 = add i32 %add104, 1
  %add108 = mul i32 %add106, %add10610293
  %mul10910294 = add i32 %add108, %add10610293
  %add110 = mul i32 %mul10910294, %add106
  %add11010295 = add i32 %add108, 1
  %add112 = mul i32 %add110, %add11010295
  %mul11310296 = add i32 %add112, %add11010295
  %add114 = mul i32 %mul11310296, %add110
  %add11410297 = add i32 %add112, 1
  %add116 = mul i32 %add114, %add11410297
  %mul11710298 = add i32 %add116, %add11410297
  %add118 = mul i32 %mul11710298, %add114
  %add11810299 = add i32 %add116, 1
  %add120 = mul i32 %add118, %add11810299
  %mul12110300 = add i32 %add120, %add11810299
  %add122 = mul i32 %mul12110300, %add118
  %add12210301 = add i32 %add120, 1
  %add124 = mul i32 %add122, %add12210301
  %mul12510302 = add i32 %add124, %add12210301
  %add126 = mul i32 %mul12510302, %add122
  %add12610303 = add i32 %add124, 1
  %add128 = mul i32 %add126, %add12610303
  %mul12910304 = add i32 %add128, %add12610303
  %add130 = mul i32 %mul12910304, %add126
  %add13010305 = add i32 %add128, 1
  %add132 = mul i32 %add130, %add13010305
  %mul13310306 = add i32 %add132, %add13010305
  %add134 = mul i32 %mul13310306, %add130
  %add13410307 = add i32 %add132, 1
  %add136 = mul i32 %add134, %add13410307
  %mul13710308 = add i32 %add136, %add13410307
  %add138 = mul i32 %mul13710308, %add134
  %add13810309 = add i32 %add136, 1
  %add140 = mul i32 %add138, %add13810309
  %mul14110310 = add i32 %add140, %add13810309
  %add142 = mul i32 %mul14110310, %add138
  %add14210311 = add i32 %add140, 1
  %add144 = mul i32 %add142, %add14210311
  %mul14510312 = add i32 %add144, %add14210311
  %add146 = mul i32 %mul14510312, %add142
  %add14610313 = add i32 %add144, 1
  %add148 = mul i32 %add146, %add14610313
  %mul14910314 = add i32 %add148, %add14610313
  %add150 = mul i32 %mul14910314, %add146
  %add15010315 = add i32 %add148, 1
  %add152 = mul i32 %add150, %add15010315
  %mul15310316 = add i32 %add152, %add15010315
  %add154 = mul i32 %mul15310316, %add150
  %add15410317 = add i32 %add152, 1
  %add156 = mul i32 %add154, %add15410317
  %mul15710318 = add i32 %add156, %add15410317
  %add158 = mul i32 %mul15710318, %add154
  %add15810319 = add i32 %add156, 1
  %add160 = mul i32 %add158, %add15810319
  %mul16110320 = add i32 %add160, %add15810319
  %add162 = mul i32 %mul16110320, %add158
  %add16210321 = add i32 %add160, 1
  %add164 = mul i32 %add162, %add16210321
  %mul16510322 = add i32 %add164, %add16210321
  %add166 = mul i32 %mul16510322, %add162
  %add16610323 = add i32 %add164, 1
  %add168 = mul i32 %add166, %add16610323
  %mul16910324 = add i32 %add168, %add16610323
  %add170 = mul i32 %mul16910324, %add166
  %add17010325 = add i32 %add168, 1
  %add172 = mul i32 %add170, %add17010325
  %mul17310326 = add i32 %add172, %add17010325
  %add174 = mul i32 %mul17310326, %add170
  %add17410327 = add i32 %add172, 1
  %add176 = mul i32 %add174, %add17410327
  %mul17710328 = add i32 %add176, %add17410327
  %add178 = mul i32 %mul17710328, %add174
  %add17810329 = add i32 %add176, 1
  %add180 = mul i32 %add178, %add17810329
  %mul18110330 = add i32 %add180, %add17810329
  %add182 = mul i32 %mul18110330, %add178
  %add18210331 = add i32 %add180, 1
  %add184 = mul i32 %add182, %add18210331
  %mul18510332 = add i32 %add184, %add18210331
  %add186 = mul i32 %mul18510332, %add182
  %add18610333 = add i32 %add184, 1
  %add188 = mul i32 %add186, %add18610333
  %mul18910334 = add i32 %add188, %add18610333
  %add190 = mul i32 %mul18910334, %add186
  %add19010335 = add i32 %add188, 1
  %add192 = mul i32 %add190, %add19010335
  %mul19310336 = add i32 %add192, %add19010335
  %add194 = mul i32 %mul19310336, %add190
  %add19410337 = add i32 %add192, 1
  %add196 = mul i32 %add194, %add19410337
  %mul19710338 = add i32 %add196, %add19410337
  %add198 = mul i32 %mul19710338, %add194
  %add19810339 = add i32 %add196, 1
  %add200 = mul i32 %add198, %add19810339
  %mul20110340 = add i32 %add200, %add19810339
  %add202 = mul i32 %mul20110340, %add198
  %add20210341 = add i32 %add200, 1
  %add204 = mul i32 %add202, %add20210341
  %mul20510342 = add i32 %add204, %add20210341
  %add206 = mul i32 %mul20510342, %add202
  %add20610343 = add i32 %add204, 1
  %add208 = mul i32 %add206, %add20610343
  %mul20910344 = add i32 %add208, %add20610343
  %add210 = mul i32 %mul20910344, %add206
  %add21010345 = add i32 %add208, 1
  %add212 = mul i32 %add210, %add21010345
  %mul21310346 = add i32 %add212, %add21010345
  %add214 = mul i32 %mul21310346, %add210
  %add21410347 = add i32 %add212, 1
  %add216 = mul i32 %add214, %add21410347
  %mul21710348 = add i32 %add216, %add21410347
  %add218 = mul i32 %mul21710348, %add214
  %add21810349 = add i32 %add216, 1
  %add220 = mul i32 %add218, %add21810349
  %mul22110350 = add i32 %add220, %add21810349
  %add222 = mul i32 %mul22110350, %add218
  %add22210351 = add i32 %add220, 1
  %add224 = mul i32 %add222, %add22210351
  %mul22510352 = add i32 %add224, %add22210351
  %add226 = mul i32 %mul22510352, %add222
  %add22610353 = add i32 %add224, 1
  %add228 = mul i32 %add226, %add22610353
  %mul22910354 = add i32 %add228, %add22610353
  %add230 = mul i32 %mul22910354, %add226
  %add23010355 = add i32 %add228, 1
  %add232 = mul i32 %add230, %add23010355
  %mul23310356 = add i32 %add232, %add23010355
  %add234 = mul i32 %mul23310356, %add230
  %add23410357 = add i32 %add232, 1
  %add236 = mul i32 %add234, %add23410357
  %mul23710358 = add i32 %add236, %add23410357
  %add238 = mul i32 %mul23710358, %add234
  %add23810359 = add i32 %add236, 1
  %add240 = mul i32 %add238, %add23810359
  %mul24110360 = add i32 %add240, %add23810359
  %add242 = mul i32 %mul24110360, %add238
  %add24210361 = add i32 %add240, 1
  %add244 = mul i32 %add242, %add24210361
  %mul24510362 = add i32 %add244, %add24210361
  %add246 = mul i32 %mul24510362, %add242
  %add24610363 = add i32 %add244, 1
  %add248 = mul i32 %add246, %add24610363
  %mul24910364 = add i32 %add248, %add24610363
  %add250 = mul i32 %mul24910364, %add246
  %add25010365 = add i32 %add248, 1
  %add252 = mul i32 %add250, %add25010365
  %mul25310366 = add i32 %add252, %add25010365
  %add254 = mul i32 %mul25310366, %add250
  %add25410367 = add i32 %add252, 1
  %add256 = mul i32 %add254, %add25410367
  %mul25710368 = add i32 %add256, %add25410367
  %add258 = mul i32 %mul25710368, %add254
  %add25810369 = add i32 %add256, 1
  %add260 = mul i32 %add258, %add25810369
  %mul26110370 = add i32 %add260, %add25810369
  %add262 = mul i32 %mul26110370, %add258
  %add26210371 = add i32 %add260, 1
  %add264 = mul i32 %add262, %add26210371
  %mul26510372 = add i32 %add264, %add26210371
  %add266 = mul i32 %mul26510372, %add262
  %add26610373 = add i32 %add264, 1
  %add268 = mul i32 %add266, %add26610373
  %mul26910374 = add i32 %add268, %add26610373
  %add270 = mul i32 %mul26910374, %add266
  %add27010375 = add i32 %add268, 1
  %add272 = mul i32 %add270, %add27010375
  %mul27310376 = add i32 %add272, %add27010375
  %add274 = mul i32 %mul27310376, %add270
  %add27410377 = add i32 %add272, 1
  %add276 = mul i32 %add274, %add27410377
  %mul27710378 = add i32 %add276, %add27410377
  %add278 = mul i32 %mul27710378, %add274
  %add27810379 = add i32 %add276, 1
  %add280 = mul i32 %add278, %add27810379
  %mul28110380 = add i32 %add280, %add27810379
  %add282 = mul i32 %mul28110380, %add278
  %add28210381 = add i32 %add280, 1
  %add284 = mul i32 %add282, %add28210381
  %mul28510382 = add i32 %add284, %add28210381
  %add286 = mul i32 %mul28510382, %add282
  %add28610383 = add i32 %add284, 1
  %add288 = mul i32 %add286, %add28610383
  %mul28910384 = add i32 %add288, %add28610383
  %add290 = mul i32 %mul28910384, %add286
  %add29010385 = add i32 %add288, 1
  %add292 = mul i32 %add290, %add29010385
  %mul29310386 = add i32 %add292, %add29010385
  %add294 = mul i32 %mul29310386, %add290
  %add29410387 = add i32 %add292, 1
  %add296 = mul i32 %add294, %add29410387
  %mul29710388 = add i32 %add296, %add29410387
  %add298 = mul i32 %mul29710388, %add294
  %add29810389 = add i32 %add296, 1
  %add300 = mul i32 %add298, %add29810389
  %mul30110390 = add i32 %add300, %add29810389
  %add302 = mul i32 %mul30110390, %add298
  %add30210391 = add i32 %add300, 1
  %add304 = mul i32 %add302, %add30210391
  %mul30510392 = add i32 %add304, %add30210391
  %add306 = mul i32 %mul30510392, %add302
  %add30610393 = add i32 %add304, 1
  %add308 = mul i32 %add306, %add30610393
  %mul30910394 = add i32 %add308, %add30610393
  %add310 = mul i32 %mul30910394, %add306
  %add31010395 = add i32 %add308, 1
  %add312 = mul i32 %add310, %add31010395
  %mul31310396 = add i32 %add312, %add31010395
  %add314 = mul i32 %mul31310396, %add310
  %add31410397 = add i32 %add312, 1
  %add316 = mul i32 %add314, %add31410397
  %mul31710398 = add i32 %add316, %add31410397
  %add318 = mul i32 %mul31710398, %add314
  %add31810399 = add i32 %add316, 1
  %add320 = mul i32 %add318, %add31810399
  %mul32110400 = add i32 %add320, %add31810399
  %add322 = mul i32 %mul32110400, %add318
  %add32210401 = add i32 %add320, 1
  %add324 = mul i32 %add322, %add32210401
  %mul32510402 = add i32 %add324, %add32210401
  %add326 = mul i32 %mul32510402, %add322
  %add32610403 = add i32 %add324, 1
  %add328 = mul i32 %add326, %add32610403
  %mul32910404 = add i32 %add328, %add32610403
  %add330 = mul i32 %mul32910404, %add326
  %add33010405 = add i32 %add328, 1
  %add332 = mul i32 %add330, %add33010405
  %mul33310406 = add i32 %add332, %add33010405
  %add334 = mul i32 %mul33310406, %add330
  %add33410407 = add i32 %add332, 1
  %add336 = mul i32 %add334, %add33410407
  %mul33710408 = add i32 %add336, %add33410407
  %add338 = mul i32 %mul33710408, %add334
  %add33810409 = add i32 %add336, 1
  %add340 = mul i32 %add338, %add33810409
  %mul34110410 = add i32 %add340, %add33810409
  %add342 = mul i32 %mul34110410, %add338
  %add34210411 = add i32 %add340, 1
  %add344 = mul i32 %add342, %add34210411
  %mul34510412 = add i32 %add344, %add34210411
  %add346 = mul i32 %mul34510412, %add342
  %add34610413 = add i32 %add344, 1
  %add348 = mul i32 %add346, %add34610413
  %mul34910414 = add i32 %add348, %add34610413
  %add350 = mul i32 %mul34910414, %add346
  %add35010415 = add i32 %add348, 1
  %add352 = mul i32 %add350, %add35010415
  %mul35310416 = add i32 %add352, %add35010415
  %add354 = mul i32 %mul35310416, %add350
  %add35410417 = add i32 %add352, 1
  %add356 = mul i32 %add354, %add35410417
  %mul35710418 = add i32 %add356, %add35410417
  %add358 = mul i32 %mul35710418, %add354
  %add35810419 = add i32 %add356, 1
  %add360 = mul i32 %add358, %add35810419
  %mul36110420 = add i32 %add360, %add35810419
  %add362 = mul i32 %mul36110420, %add358
  %add36210421 = add i32 %add360, 1
  %add364 = mul i32 %add362, %add36210421
  %mul36510422 = add i32 %add364, %add36210421
  %add366 = mul i32 %mul36510422, %add362
  %add36610423 = add i32 %add364, 1
  %add368 = mul i32 %add366, %add36610423
  %mul36910424 = add i32 %add368, %add36610423
  %add370 = mul i32 %mul36910424, %add366
  %add37010425 = add i32 %add368, 1
  %add372 = mul i32 %add370, %add37010425
  %mul37310426 = add i32 %add372, %add37010425
  %add374 = mul i32 %mul37310426, %add370
  %add37410427 = add i32 %add372, 1
  %add376 = mul i32 %add374, %add37410427
  %mul37710428 = add i32 %add376, %add37410427
  %add378 = mul i32 %mul37710428, %add374
  %add37810429 = add i32 %add376, 1
  %add380 = mul i32 %add378, %add37810429
  %mul38110430 = add i32 %add380, %add37810429
  %add382 = mul i32 %mul38110430, %add378
  %add38210431 = add i32 %add380, 1
  %add384 = mul i32 %add382, %add38210431
  %mul38510432 = add i32 %add384, %add38210431
  %add386 = mul i32 %mul38510432, %add382
  %add38610433 = add i32 %add384, 1
  %add388 = mul i32 %add386, %add38610433
  %mul38910434 = add i32 %add388, %add38610433
  %add390 = mul i32 %mul38910434, %add386
  %add39010435 = add i32 %add388, 1
  %add392 = mul i32 %add390, %add39010435
  %mul39310436 = add i32 %add392, %add39010435
  %add394 = mul i32 %mul39310436, %add390
  %add39410437 = add i32 %add392, 1
  %add396 = mul i32 %add394, %add39410437
  %mul39710438 = add i32 %add396, %add39410437
  %add398 = mul i32 %mul39710438, %add394
  %add39810439 = add i32 %add396, 1
  %add400 = mul i32 %add398, %add39810439
  %mul40110440 = add i32 %add400, %add39810439
  %add402 = mul i32 %mul40110440, %add398
  %add40210441 = add i32 %add400, 1
  %add404 = mul i32 %add402, %add40210441
  %mul40510442 = add i32 %add404, %add40210441
  %add406 = mul i32 %mul40510442, %add402
  %add40610443 = add i32 %add404, 1
  %add408 = mul i32 %add406, %add40610443
  %mul40910444 = add i32 %add408, %add40610443
  %add410 = mul i32 %mul40910444, %add406
  %add41010445 = add i32 %add408, 1
  %add412 = mul i32 %add410, %add41010445
  %mul41310446 = add i32 %add412, %add41010445
  %add414 = mul i32 %mul41310446, %add410
  %add41410447 = add i32 %add412, 1
  %add416 = mul i32 %add414, %add41410447
  %mul41710448 = add i32 %add416, %add41410447
  %add418 = mul i32 %mul41710448, %add414
  %add41810449 = add i32 %add416, 1
  %add420 = mul i32 %add418, %add41810449
  %mul42110450 = add i32 %add420, %add41810449
  %add422 = mul i32 %mul42110450, %add418
  %add42210451 = add i32 %add420, 1
  %add424 = mul i32 %add422, %add42210451
  %mul42510452 = add i32 %add424, %add42210451
  %add426 = mul i32 %mul42510452, %add422
  %add42610453 = add i32 %add424, 1
  %add428 = mul i32 %add426, %add42610453
  %mul42910454 = add i32 %add428, %add42610453
  %add430 = mul i32 %mul42910454, %add426
  %add43010455 = add i32 %add428, 1
  %add432 = mul i32 %add430, %add43010455
  %mul43310456 = add i32 %add432, %add43010455
  %add434 = mul i32 %mul43310456, %add430
  %add43410457 = add i32 %add432, 1
  %add436 = mul i32 %add434, %add43410457
  %mul43710458 = add i32 %add436, %add43410457
  %add438 = mul i32 %mul43710458, %add434
  %add43810459 = add i32 %add436, 1
  %add440 = mul i32 %add438, %add43810459
  %mul44110460 = add i32 %add440, %add43810459
  %add442 = mul i32 %mul44110460, %add438
  %add44210461 = add i32 %add440, 1
  %add444 = mul i32 %add442, %add44210461
  %mul44510462 = add i32 %add444, %add44210461
  %add446 = mul i32 %mul44510462, %add442
  %add44610463 = add i32 %add444, 1
  %add448 = mul i32 %add446, %add44610463
  %mul44910464 = add i32 %add448, %add44610463
  %add450 = mul i32 %mul44910464, %add446
  %add45010465 = add i32 %add448, 1
  %add452 = mul i32 %add450, %add45010465
  %mul45310466 = add i32 %add452, %add45010465
  %add454 = mul i32 %mul45310466, %add450
  %add45410467 = add i32 %add452, 1
  %add456 = mul i32 %add454, %add45410467
  %mul45710468 = add i32 %add456, %add45410467
  %add458 = mul i32 %mul45710468, %add454
  %add45810469 = add i32 %add456, 1
  %add460 = mul i32 %add458, %add45810469
  %mul46110470 = add i32 %add460, %add45810469
  %add462 = mul i32 %mul46110470, %add458
  %add46210471 = add i32 %add460, 1
  %add464 = mul i32 %add462, %add46210471
  %mul46510472 = add i32 %add464, %add46210471
  %add466 = mul i32 %mul46510472, %add462
  %add46610473 = add i32 %add464, 1
  %add468 = mul i32 %add466, %add46610473
  %mul46910474 = add i32 %add468, %add46610473
  %add470 = mul i32 %mul46910474, %add466
  %add47010475 = add i32 %add468, 1
  %add472 = mul i32 %add470, %add47010475
  %mul47310476 = add i32 %add472, %add47010475
  %add474 = mul i32 %mul47310476, %add470
  %add47410477 = add i32 %add472, 1
  %add476 = mul i32 %add474, %add47410477
  %mul47710478 = add i32 %add476, %add47410477
  %add478 = mul i32 %mul47710478, %add474
  %add47810479 = add i32 %add476, 1
  %add480 = mul i32 %add478, %add47810479
  %mul48110480 = add i32 %add480, %add47810479
  %add482 = mul i32 %mul48110480, %add478
  %add48210481 = add i32 %add480, 1
  %add484 = mul i32 %add482, %add48210481
  %mul48510482 = add i32 %add484, %add48210481
  %add486 = mul i32 %mul48510482, %add482
  %add48610483 = add i32 %add484, 1
  %add488 = mul i32 %add486, %add48610483
  %mul48910484 = add i32 %add488, %add48610483
  %add490 = mul i32 %mul48910484, %add486
  %add49010485 = add i32 %add488, 1
  %add492 = mul i32 %add490, %add49010485
  %mul49310486 = add i32 %add492, %add49010485
  %add494 = mul i32 %mul49310486, %add490
  %add49410487 = add i32 %add492, 1
  %add496 = mul i32 %add494, %add49410487
  %mul49710488 = add i32 %add496, %add49410487
  %add498 = mul i32 %mul49710488, %add494
  %add49810489 = add i32 %add496, 1
  %add500 = mul i32 %add498, %add49810489
  %mul50110490 = add i32 %add500, %add49810489
  %add502 = mul i32 %mul50110490, %add498
  %add50210491 = add i32 %add500, 1
  %add504 = mul i32 %add502, %add50210491
  %mul50510492 = add i32 %add504, %add50210491
  %add506 = mul i32 %mul50510492, %add502
  %add50610493 = add i32 %add504, 1
  %add508 = mul i32 %add506, %add50610493
  %mul50910494 = add i32 %add508, %add50610493
  %add510 = mul i32 %mul50910494, %add506
  %add51010495 = add i32 %add508, 1
  %add512 = mul i32 %add510, %add51010495
  %mul51310496 = add i32 %add512, %add51010495
  %add514 = mul i32 %mul51310496, %add510
  %add51410497 = add i32 %add512, 1
  %add516 = mul i32 %add514, %add51410497
  %mul51710498 = add i32 %add516, %add51410497
  %add518 = mul i32 %mul51710498, %add514
  %add51810499 = add i32 %add516, 1
  %add520 = mul i32 %add518, %add51810499
  %mul52110500 = add i32 %add520, %add51810499
  %add522 = mul i32 %mul52110500, %add518
  %add52210501 = add i32 %add520, 1
  %add524 = mul i32 %add522, %add52210501
  %mul52510502 = add i32 %add524, %add52210501
  %add526 = mul i32 %mul52510502, %add522
  %add52610503 = add i32 %add524, 1
  %add528 = mul i32 %add526, %add52610503
  %mul52910504 = add i32 %add528, %add52610503
  %add530 = mul i32 %mul52910504, %add526
  %add53010505 = add i32 %add528, 1
  %add532 = mul i32 %add530, %add53010505
  %mul53310506 = add i32 %add532, %add53010505
  %add534 = mul i32 %mul53310506, %add530
  %add53410507 = add i32 %add532, 1
  %add536 = mul i32 %add534, %add53410507
  %mul53710508 = add i32 %add536, %add53410507
  %add538 = mul i32 %mul53710508, %add534
  %add53810509 = add i32 %add536, 1
  %add540 = mul i32 %add538, %add53810509
  %mul54110510 = add i32 %add540, %add53810509
  %add542 = mul i32 %mul54110510, %add538
  %add54210511 = add i32 %add540, 1
  %add544 = mul i32 %add542, %add54210511
  %mul54510512 = add i32 %add544, %add54210511
  %add546 = mul i32 %mul54510512, %add542
  %add54610513 = add i32 %add544, 1
  %add548 = mul i32 %add546, %add54610513
  %mul54910514 = add i32 %add548, %add54610513
  %add550 = mul i32 %mul54910514, %add546
  %add55010515 = add i32 %add548, 1
  %add552 = mul i32 %add550, %add55010515
  %mul55310516 = add i32 %add552, %add55010515
  %add554 = mul i32 %mul55310516, %add550
  %add55410517 = add i32 %add552, 1
  %add556 = mul i32 %add554, %add55410517
  %mul55710518 = add i32 %add556, %add55410517
  %add558 = mul i32 %mul55710518, %add554
  %add55810519 = add i32 %add556, 1
  %add560 = mul i32 %add558, %add55810519
  %mul56110520 = add i32 %add560, %add55810519
  %add562 = mul i32 %mul56110520, %add558
  %add56210521 = add i32 %add560, 1
  %add564 = mul i32 %add562, %add56210521
  %mul56510522 = add i32 %add564, %add56210521
  %add566 = mul i32 %mul56510522, %add562
  %add56610523 = add i32 %add564, 1
  %add568 = mul i32 %add566, %add56610523
  %mul56910524 = add i32 %add568, %add56610523
  %add570 = mul i32 %mul56910524, %add566
  %add57010525 = add i32 %add568, 1
  %add572 = mul i32 %add570, %add57010525
  %mul57310526 = add i32 %add572, %add57010525
  %add574 = mul i32 %mul57310526, %add570
  %add57410527 = add i32 %add572, 1
  %add576 = mul i32 %add574, %add57410527
  %mul57710528 = add i32 %add576, %add57410527
  %add578 = mul i32 %mul57710528, %add574
  %add57810529 = add i32 %add576, 1
  %add580 = mul i32 %add578, %add57810529
  %mul58110530 = add i32 %add580, %add57810529
  %add582 = mul i32 %mul58110530, %add578
  %add58210531 = add i32 %add580, 1
  %add584 = mul i32 %add582, %add58210531
  %mul58510532 = add i32 %add584, %add58210531
  %add586 = mul i32 %mul58510532, %add582
  %add58610533 = add i32 %add584, 1
  %add588 = mul i32 %add586, %add58610533
  %mul58910534 = add i32 %add588, %add58610533
  %add590 = mul i32 %mul58910534, %add586
  %add59010535 = add i32 %add588, 1
  %add592 = mul i32 %add590, %add59010535
  %mul59310536 = add i32 %add592, %add59010535
  %add594 = mul i32 %mul59310536, %add590
  %add59410537 = add i32 %add592, 1
  %add596 = mul i32 %add594, %add59410537
  %mul59710538 = add i32 %add596, %add59410537
  %add598 = mul i32 %mul59710538, %add594
  %add59810539 = add i32 %add596, 1
  %add600 = mul i32 %add598, %add59810539
  %mul60110540 = add i32 %add600, %add59810539
  %add602 = mul i32 %mul60110540, %add598
  %add60210541 = add i32 %add600, 1
  %add604 = mul i32 %add602, %add60210541
  %mul60510542 = add i32 %add604, %add60210541
  %add606 = mul i32 %mul60510542, %add602
  %add60610543 = add i32 %add604, 1
  %add608 = mul i32 %add606, %add60610543
  %mul60910544 = add i32 %add608, %add60610543
  %add610 = mul i32 %mul60910544, %add606
  %add61010545 = add i32 %add608, 1
  %add612 = mul i32 %add610, %add61010545
  %mul61310546 = add i32 %add612, %add61010545
  %add614 = mul i32 %mul61310546, %add610
  %add61410547 = add i32 %add612, 1
  %add616 = mul i32 %add614, %add61410547
  %mul61710548 = add i32 %add616, %add61410547
  %add618 = mul i32 %mul61710548, %add614
  %add61810549 = add i32 %add616, 1
  %add620 = mul i32 %add618, %add61810549
  %mul62110550 = add i32 %add620, %add61810549
  %add622 = mul i32 %mul62110550, %add618
  %add62210551 = add i32 %add620, 1
  %add624 = mul i32 %add622, %add62210551
  %mul62510552 = add i32 %add624, %add62210551
  %add626 = mul i32 %mul62510552, %add622
  %add62610553 = add i32 %add624, 1
  %add628 = mul i32 %add626, %add62610553
  %mul62910554 = add i32 %add628, %add62610553
  %add630 = mul i32 %mul62910554, %add626
  %add63010555 = add i32 %add628, 1
  %add632 = mul i32 %add630, %add63010555
  %mul63310556 = add i32 %add632, %add63010555
  %add634 = mul i32 %mul63310556, %add630
  %add63410557 = add i32 %add632, 1
  %add636 = mul i32 %add634, %add63410557
  %mul63710558 = add i32 %add636, %add63410557
  %add638 = mul i32 %mul63710558, %add634
  %add63810559 = add i32 %add636, 1
  %add640 = mul i32 %add638, %add63810559
  %mul64110560 = add i32 %add640, %add63810559
  %add642 = mul i32 %mul64110560, %add638
  %add64210561 = add i32 %add640, 1
  %add644 = mul i32 %add642, %add64210561
  %mul64510562 = add i32 %add644, %add64210561
  %add646 = mul i32 %mul64510562, %add642
  %add64610563 = add i32 %add644, 1
  %add648 = mul i32 %add646, %add64610563
  %mul64910564 = add i32 %add648, %add64610563
  %add650 = mul i32 %mul64910564, %add646
  %add65010565 = add i32 %add648, 1
  %add652 = mul i32 %add650, %add65010565
  %mul65310566 = add i32 %add652, %add65010565
  %add654 = mul i32 %mul65310566, %add650
  %add65410567 = add i32 %add652, 1
  %add656 = mul i32 %add654, %add65410567
  %mul65710568 = add i32 %add656, %add65410567
  %add658 = mul i32 %mul65710568, %add654
  %add65810569 = add i32 %add656, 1
  %add660 = mul i32 %add658, %add65810569
  %mul66110570 = add i32 %add660, %add65810569
  %add662 = mul i32 %mul66110570, %add658
  %add66210571 = add i32 %add660, 1
  %add664 = mul i32 %add662, %add66210571
  %mul66510572 = add i32 %add664, %add66210571
  %add666 = mul i32 %mul66510572, %add662
  %add66610573 = add i32 %add664, 1
  %add668 = mul i32 %add666, %add66610573
  %mul66910574 = add i32 %add668, %add66610573
  %add670 = mul i32 %mul66910574, %add666
  %add67010575 = add i32 %add668, 1
  %add672 = mul i32 %add670, %add67010575
  %mul67310576 = add i32 %add672, %add67010575
  %add674 = mul i32 %mul67310576, %add670
  %add67410577 = add i32 %add672, 1
  %add676 = mul i32 %add674, %add67410577
  %mul67710578 = add i32 %add676, %add67410577
  %add678 = mul i32 %mul67710578, %add674
  %add67810579 = add i32 %add676, 1
  %add680 = mul i32 %add678, %add67810579
  %mul68110580 = add i32 %add680, %add67810579
  %add682 = mul i32 %mul68110580, %add678
  %add68210581 = add i32 %add680, 1
  %add684 = mul i32 %add682, %add68210581
  %mul68510582 = add i32 %add684, %add68210581
  %add686 = mul i32 %mul68510582, %add682
  %add68610583 = add i32 %add684, 1
  %add688 = mul i32 %add686, %add68610583
  %mul68910584 = add i32 %add688, %add68610583
  %add690 = mul i32 %mul68910584, %add686
  %add69010585 = add i32 %add688, 1
  %add692 = mul i32 %add690, %add69010585
  %mul69310586 = add i32 %add692, %add69010585
  %add694 = mul i32 %mul69310586, %add690
  %add69410587 = add i32 %add692, 1
  %add696 = mul i32 %add694, %add69410587
  %mul69710588 = add i32 %add696, %add69410587
  %add698 = mul i32 %mul69710588, %add694
  %add69810589 = add i32 %add696, 1
  %add700 = mul i32 %add698, %add69810589
  %mul70110590 = add i32 %add700, %add69810589
  %add702 = mul i32 %mul70110590, %add698
  %add70210591 = add i32 %add700, 1
  %add704 = mul i32 %add702, %add70210591
  %mul70510592 = add i32 %add704, %add70210591
  %add706 = mul i32 %mul70510592, %add702
  %add70610593 = add i32 %add704, 1
  %add708 = mul i32 %add706, %add70610593
  %mul70910594 = add i32 %add708, %add70610593
  %add710 = mul i32 %mul70910594, %add706
  %add71010595 = add i32 %add708, 1
  %add712 = mul i32 %add710, %add71010595
  %mul71310596 = add i32 %add712, %add71010595
  %add714 = mul i32 %mul71310596, %add710
  %add71410597 = add i32 %add712, 1
  %add716 = mul i32 %add714, %add71410597
  %mul71710598 = add i32 %add716, %add71410597
  %add718 = mul i32 %mul71710598, %add714
  %add71810599 = add i32 %add716, 1
  %add720 = mul i32 %add718, %add71810599
  %mul72110600 = add i32 %add720, %add71810599
  %add722 = mul i32 %mul72110600, %add718
  %add72210601 = add i32 %add720, 1
  %add724 = mul i32 %add722, %add72210601
  %mul72510602 = add i32 %add724, %add72210601
  %add726 = mul i32 %mul72510602, %add722
  %add72610603 = add i32 %add724, 1
  %add728 = mul i32 %add726, %add72610603
  %mul72910604 = add i32 %add728, %add72610603
  %add730 = mul i32 %mul72910604, %add726
  %add73010605 = add i32 %add728, 1
  %add732 = mul i32 %add730, %add73010605
  %mul73310606 = add i32 %add732, %add73010605
  %add734 = mul i32 %mul73310606, %add730
  %add73410607 = add i32 %add732, 1
  %add736 = mul i32 %add734, %add73410607
  %mul73710608 = add i32 %add736, %add73410607
  %add738 = mul i32 %mul73710608, %add734
  %add73810609 = add i32 %add736, 1
  %add740 = mul i32 %add738, %add73810609
  %mul74110610 = add i32 %add740, %add73810609
  %add742 = mul i32 %mul74110610, %add738
  %add74210611 = add i32 %add740, 1
  %add744 = mul i32 %add742, %add74210611
  %mul74510612 = add i32 %add744, %add74210611
  %add746 = mul i32 %mul74510612, %add742
  %add74610613 = add i32 %add744, 1
  %add748 = mul i32 %add746, %add74610613
  %mul74910614 = add i32 %add748, %add74610613
  %add750 = mul i32 %mul74910614, %add746
  %add75010615 = add i32 %add748, 1
  %add752 = mul i32 %add750, %add75010615
  %mul75310616 = add i32 %add752, %add75010615
  %add754 = mul i32 %mul75310616, %add750
  %add75410617 = add i32 %add752, 1
  %add756 = mul i32 %add754, %add75410617
  %mul75710618 = add i32 %add756, %add75410617
  %add758 = mul i32 %mul75710618, %add754
  %add75810619 = add i32 %add756, 1
  %add760 = mul i32 %add758, %add75810619
  %mul76110620 = add i32 %add760, %add75810619
  %add762 = mul i32 %mul76110620, %add758
  %add76210621 = add i32 %add760, 1
  %add764 = mul i32 %add762, %add76210621
  %mul76510622 = add i32 %add764, %add76210621
  %add766 = mul i32 %mul76510622, %add762
  %add76610623 = add i32 %add764, 1
  %add768 = mul i32 %add766, %add76610623
  %mul76910624 = add i32 %add768, %add76610623
  %add770 = mul i32 %mul76910624, %add766
  %add77010625 = add i32 %add768, 1
  %add772 = mul i32 %add770, %add77010625
  %mul77310626 = add i32 %add772, %add77010625
  %add774 = mul i32 %mul77310626, %add770
  %add77410627 = add i32 %add772, 1
  %add776 = mul i32 %add774, %add77410627
  %mul77710628 = add i32 %add776, %add77410627
  %add778 = mul i32 %mul77710628, %add774
  %add77810629 = add i32 %add776, 1
  %add780 = mul i32 %add778, %add77810629
  %mul78110630 = add i32 %add780, %add77810629
  %add782 = mul i32 %mul78110630, %add778
  %add78210631 = add i32 %add780, 1
  %add784 = mul i32 %add782, %add78210631
  %mul78510632 = add i32 %add784, %add78210631
  %add786 = mul i32 %mul78510632, %add782
  %add78610633 = add i32 %add784, 1
  %add788 = mul i32 %add786, %add78610633
  %mul78910634 = add i32 %add788, %add78610633
  %add790 = mul i32 %mul78910634, %add786
  %add79010635 = add i32 %add788, 1
  %add792 = mul i32 %add790, %add79010635
  %mul79310636 = add i32 %add792, %add79010635
  %add794 = mul i32 %mul79310636, %add790
  %add79410637 = add i32 %add792, 1
  %add796 = mul i32 %add794, %add79410637
  %mul79710638 = add i32 %add796, %add79410637
  %add798 = mul i32 %mul79710638, %add794
  %add79810639 = add i32 %add796, 1
  %add800 = mul i32 %add798, %add79810639
  %mul80110640 = add i32 %add800, %add79810639
  %add802 = mul i32 %mul80110640, %add798
  %add80210641 = add i32 %add800, 1
  %add804 = mul i32 %add802, %add80210641
  %mul80510642 = add i32 %add804, %add80210641
  %add806 = mul i32 %mul80510642, %add802
  %add80610643 = add i32 %add804, 1
  %add808 = mul i32 %add806, %add80610643
  %mul80910644 = add i32 %add808, %add80610643
  %add810 = mul i32 %mul80910644, %add806
  %add81010645 = add i32 %add808, 1
  %add812 = mul i32 %add810, %add81010645
  %mul81310646 = add i32 %add812, %add81010645
  %add814 = mul i32 %mul81310646, %add810
  %add81410647 = add i32 %add812, 1
  %add816 = mul i32 %add814, %add81410647
  %mul81710648 = add i32 %add816, %add81410647
  %add818 = mul i32 %mul81710648, %add814
  %add81810649 = add i32 %add816, 1
  %add820 = mul i32 %add818, %add81810649
  %mul82110650 = add i32 %add820, %add81810649
  %add822 = mul i32 %mul82110650, %add818
  %add82210651 = add i32 %add820, 1
  %add824 = mul i32 %add822, %add82210651
  %mul82510652 = add i32 %add824, %add82210651
  %add826 = mul i32 %mul82510652, %add822
  %add82610653 = add i32 %add824, 1
  %add828 = mul i32 %add826, %add82610653
  %mul82910654 = add i32 %add828, %add82610653
  %add830 = mul i32 %mul82910654, %add826
  %add83010655 = add i32 %add828, 1
  %add832 = mul i32 %add830, %add83010655
  %mul83310656 = add i32 %add832, %add83010655
  %add834 = mul i32 %mul83310656, %add830
  %add83410657 = add i32 %add832, 1
  %add836 = mul i32 %add834, %add83410657
  %mul83710658 = add i32 %add836, %add83410657
  %add838 = mul i32 %mul83710658, %add834
  %add83810659 = add i32 %add836, 1
  %add840 = mul i32 %add838, %add83810659
  %mul84110660 = add i32 %add840, %add83810659
  %add842 = mul i32 %mul84110660, %add838
  %add84210661 = add i32 %add840, 1
  %add844 = mul i32 %add842, %add84210661
  %mul84510662 = add i32 %add844, %add84210661
  %add846 = mul i32 %mul84510662, %add842
  %add84610663 = add i32 %add844, 1
  %add848 = mul i32 %add846, %add84610663
  %mul84910664 = add i32 %add848, %add84610663
  %add850 = mul i32 %mul84910664, %add846
  %add85010665 = add i32 %add848, 1
  %add852 = mul i32 %add850, %add85010665
  %mul85310666 = add i32 %add852, %add85010665
  %add854 = mul i32 %mul85310666, %add850
  %add85410667 = add i32 %add852, 1
  %add856 = mul i32 %add854, %add85410667
  %mul85710668 = add i32 %add856, %add85410667
  %add858 = mul i32 %mul85710668, %add854
  %add85810669 = add i32 %add856, 1
  %add860 = mul i32 %add858, %add85810669
  %mul86110670 = add i32 %add860, %add85810669
  %add862 = mul i32 %mul86110670, %add858
  %add86210671 = add i32 %add860, 1
  %add864 = mul i32 %add862, %add86210671
  %mul86510672 = add i32 %add864, %add86210671
  %add866 = mul i32 %mul86510672, %add862
  %add86610673 = add i32 %add864, 1
  %add868 = mul i32 %add866, %add86610673
  %mul86910674 = add i32 %add868, %add86610673
  %add870 = mul i32 %mul86910674, %add866
  %add87010675 = add i32 %add868, 1
  %add872 = mul i32 %add870, %add87010675
  %mul87310676 = add i32 %add872, %add87010675
  %add874 = mul i32 %mul87310676, %add870
  %add87410677 = add i32 %add872, 1
  %add876 = mul i32 %add874, %add87410677
  %mul87710678 = add i32 %add876, %add87410677
  %add878 = mul i32 %mul87710678, %add874
  %add87810679 = add i32 %add876, 1
  %add880 = mul i32 %add878, %add87810679
  %mul88110680 = add i32 %add880, %add87810679
  %add882 = mul i32 %mul88110680, %add878
  %add88210681 = add i32 %add880, 1
  %add884 = mul i32 %add882, %add88210681
  %mul88510682 = add i32 %add884, %add88210681
  %add886 = mul i32 %mul88510682, %add882
  %add88610683 = add i32 %add884, 1
  %add888 = mul i32 %add886, %add88610683
  %mul88910684 = add i32 %add888, %add88610683
  %add890 = mul i32 %mul88910684, %add886
  %add89010685 = add i32 %add888, 1
  %add892 = mul i32 %add890, %add89010685
  %mul89310686 = add i32 %add892, %add89010685
  %add894 = mul i32 %mul89310686, %add890
  %add89410687 = add i32 %add892, 1
  %add896 = mul i32 %add894, %add89410687
  %mul89710688 = add i32 %add896, %add89410687
  %add898 = mul i32 %mul89710688, %add894
  %add89810689 = add i32 %add896, 1
  %add900 = mul i32 %add898, %add89810689
  %mul90110690 = add i32 %add900, %add89810689
  %add902 = mul i32 %mul90110690, %add898
  %add90210691 = add i32 %add900, 1
  %add904 = mul i32 %add902, %add90210691
  %mul90510692 = add i32 %add904, %add90210691
  %add906 = mul i32 %mul90510692, %add902
  %add90610693 = add i32 %add904, 1
  %add908 = mul i32 %add906, %add90610693
  %mul90910694 = add i32 %add908, %add90610693
  %add910 = mul i32 %mul90910694, %add906
  %add91010695 = add i32 %add908, 1
  %add912 = mul i32 %add910, %add91010695
  %mul91310696 = add i32 %add912, %add91010695
  %add914 = mul i32 %mul91310696, %add910
  %add91410697 = add i32 %add912, 1
  %add916 = mul i32 %add914, %add91410697
  %mul91710698 = add i32 %add916, %add91410697
  %add918 = mul i32 %mul91710698, %add914
  %add91810699 = add i32 %add916, 1
  %add920 = mul i32 %add918, %add91810699
  %mul92110700 = add i32 %add920, %add91810699
  %add922 = mul i32 %mul92110700, %add918
  %add92210701 = add i32 %add920, 1
  %add924 = mul i32 %add922, %add92210701
  %mul92510702 = add i32 %add924, %add92210701
  %add926 = mul i32 %mul92510702, %add922
  %add92610703 = add i32 %add924, 1
  %add928 = mul i32 %add926, %add92610703
  %mul92910704 = add i32 %add928, %add92610703
  %add930 = mul i32 %mul92910704, %add926
  %add93010705 = add i32 %add928, 1
  %add932 = mul i32 %add930, %add93010705
  %mul93310706 = add i32 %add932, %add93010705
  %add934 = mul i32 %mul93310706, %add930
  %add93410707 = add i32 %add932, 1
  %add936 = mul i32 %add934, %add93410707
  %mul93710708 = add i32 %add936, %add93410707
  %add938 = mul i32 %mul93710708, %add934
  %add93810709 = add i32 %add936, 1
  %add940 = mul i32 %add938, %add93810709
  %mul94110710 = add i32 %add940, %add93810709
  %add942 = mul i32 %mul94110710, %add938
  %add94210711 = add i32 %add940, 1
  %add944 = mul i32 %add942, %add94210711
  %mul94510712 = add i32 %add944, %add94210711
  %add946 = mul i32 %mul94510712, %add942
  %add94610713 = add i32 %add944, 1
  %add948 = mul i32 %add946, %add94610713
  %mul94910714 = add i32 %add948, %add94610713
  %add950 = mul i32 %mul94910714, %add946
  %add95010715 = add i32 %add948, 1
  %add952 = mul i32 %add950, %add95010715
  %mul95310716 = add i32 %add952, %add95010715
  %add954 = mul i32 %mul95310716, %add950
  %add95410717 = add i32 %add952, 1
  %add956 = mul i32 %add954, %add95410717
  %mul95710718 = add i32 %add956, %add95410717
  %add958 = mul i32 %mul95710718, %add954
  %add95810719 = add i32 %add956, 1
  %add960 = mul i32 %add958, %add95810719
  %mul96110720 = add i32 %add960, %add95810719
  %add962 = mul i32 %mul96110720, %add958
  %add96210721 = add i32 %add960, 1
  %add964 = mul i32 %add962, %add96210721
  %mul96510722 = add i32 %add964, %add96210721
  %add966 = mul i32 %mul96510722, %add962
  %add96610723 = add i32 %add964, 1
  %add968 = mul i32 %add966, %add96610723
  %mul96910724 = add i32 %add968, %add96610723
  %add970 = mul i32 %mul96910724, %add966
  %add97010725 = add i32 %add968, 1
  %add972 = mul i32 %add970, %add97010725
  %mul97310726 = add i32 %add972, %add97010725
  %add974 = mul i32 %mul97310726, %add970
  %add97410727 = add i32 %add972, 1
  %add976 = mul i32 %add974, %add97410727
  %mul97710728 = add i32 %add976, %add97410727
  %add978 = mul i32 %mul97710728, %add974
  %add97810729 = add i32 %add976, 1
  %add980 = mul i32 %add978, %add97810729
  %mul98110730 = add i32 %add980, %add97810729
  %add982 = mul i32 %mul98110730, %add978
  %add98210731 = add i32 %add980, 1
  %add984 = mul i32 %add982, %add98210731
  %mul98510732 = add i32 %add984, %add98210731
  %add986 = mul i32 %mul98510732, %add982
  %add98610733 = add i32 %add984, 1
  %add988 = mul i32 %add986, %add98610733
  %mul98910734 = add i32 %add988, %add98610733
  %add990 = mul i32 %mul98910734, %add986
  %add99010735 = add i32 %add988, 1
  %add992 = mul i32 %add990, %add99010735
  %mul99310736 = add i32 %add992, %add99010735
  %add994 = mul i32 %mul99310736, %add990
  %add99410737 = add i32 %add992, 1
  %add996 = mul i32 %add994, %add99410737
  %mul99710738 = add i32 %add996, %add99410737
  %add998 = mul i32 %mul99710738, %add994
  %add99810739 = add i32 %add996, 1
  %add1000 = mul i32 %add998, %add99810739
  %mul100110740 = add i32 %add1000, %add99810739
  %add1002 = mul i32 %mul100110740, %add998
  %add100210741 = add i32 %add1000, 1
  %add1004 = mul i32 %add1002, %add100210741
  %mul100510742 = add i32 %add1004, %add100210741
  %add1006 = mul i32 %mul100510742, %add1002
  %add100610743 = add i32 %add1004, 1
  %add1008 = mul i32 %add1006, %add100610743
  %mul100910744 = add i32 %add1008, %add100610743
  %add1010 = mul i32 %mul100910744, %add1006
  %add101010745 = add i32 %add1008, 1
  %add1012 = mul i32 %add1010, %add101010745
  %mul101310746 = add i32 %add1012, %add101010745
  %add1014 = mul i32 %mul101310746, %add1010
  %add101410747 = add i32 %add1012, 1
  %add1016 = mul i32 %add1014, %add101410747
  %mul101710748 = add i32 %add1016, %add101410747
  %add1018 = mul i32 %mul101710748, %add1014
  %add101810749 = add i32 %add1016, 1
  %add1020 = mul i32 %add1018, %add101810749
  %mul102110750 = add i32 %add1020, %add101810749
  %add1022 = mul i32 %mul102110750, %add1018
  %add102210751 = add i32 %add1020, 1
  %add1024 = mul i32 %add1022, %add102210751
  %mul102510752 = add i32 %add1024, %add102210751
  %add1026 = mul i32 %mul102510752, %add1022
  %add102610753 = add i32 %add1024, 1
  %add1028 = mul i32 %add1026, %add102610753
  %mul102910754 = add i32 %add1028, %add102610753
  %add1030 = mul i32 %mul102910754, %add1026
  %add103010755 = add i32 %add1028, 1
  %add1032 = mul i32 %add1030, %add103010755
  %mul103310756 = add i32 %add1032, %add103010755
  %add1034 = mul i32 %mul103310756, %add1030
  %add103410757 = add i32 %add1032, 1
  %add1036 = mul i32 %add1034, %add103410757
  %mul103710758 = add i32 %add1036, %add103410757
  %add1038 = mul i32 %mul103710758, %add1034
  %add103810759 = add i32 %add1036, 1
  %add1040 = mul i32 %add1038, %add103810759
  %mul104110760 = add i32 %add1040, %add103810759
  %add1042 = mul i32 %mul104110760, %add1038
  %add104210761 = add i32 %add1040, 1
  %add1044 = mul i32 %add1042, %add104210761
  %mul104510762 = add i32 %add1044, %add104210761
  %add1046 = mul i32 %mul104510762, %add1042
  %add104610763 = add i32 %add1044, 1
  %add1048 = mul i32 %add1046, %add104610763
  %mul104910764 = add i32 %add1048, %add104610763
  %add1050 = mul i32 %mul104910764, %add1046
  %add105010765 = add i32 %add1048, 1
  %add1052 = mul i32 %add1050, %add105010765
  %mul105310766 = add i32 %add1052, %add105010765
  %add1054 = mul i32 %mul105310766, %add1050
  %add105410767 = add i32 %add1052, 1
  %add1056 = mul i32 %add1054, %add105410767
  %mul105710768 = add i32 %add1056, %add105410767
  %add1058 = mul i32 %mul105710768, %add1054
  %add105810769 = add i32 %add1056, 1
  %add1060 = mul i32 %add1058, %add105810769
  %mul106110770 = add i32 %add1060, %add105810769
  %add1062 = mul i32 %mul106110770, %add1058
  %add106210771 = add i32 %add1060, 1
  %add1064 = mul i32 %add1062, %add106210771
  %mul106510772 = add i32 %add1064, %add106210771
  %add1066 = mul i32 %mul106510772, %add1062
  %add106610773 = add i32 %add1064, 1
  %add1068 = mul i32 %add1066, %add106610773
  %mul106910774 = add i32 %add1068, %add106610773
  %add1070 = mul i32 %mul106910774, %add1066
  %add107010775 = add i32 %add1068, 1
  %add1072 = mul i32 %add1070, %add107010775
  %mul107310776 = add i32 %add1072, %add107010775
  %add1074 = mul i32 %mul107310776, %add1070
  %add107410777 = add i32 %add1072, 1
  %add1076 = mul i32 %add1074, %add107410777
  %mul107710778 = add i32 %add1076, %add107410777
  %add1078 = mul i32 %mul107710778, %add1074
  %add107810779 = add i32 %add1076, 1
  %add1080 = mul i32 %add1078, %add107810779
  %mul108110780 = add i32 %add1080, %add107810779
  %add1082 = mul i32 %mul108110780, %add1078
  %add108210781 = add i32 %add1080, 1
  %add1084 = mul i32 %add1082, %add108210781
  %mul108510782 = add i32 %add1084, %add108210781
  %add1086 = mul i32 %mul108510782, %add1082
  %add108610783 = add i32 %add1084, 1
  %add1088 = mul i32 %add1086, %add108610783
  %mul108910784 = add i32 %add1088, %add108610783
  %add1090 = mul i32 %mul108910784, %add1086
  %add109010785 = add i32 %add1088, 1
  %add1092 = mul i32 %add1090, %add109010785
  %mul109310786 = add i32 %add1092, %add109010785
  %add1094 = mul i32 %mul109310786, %add1090
  %add109410787 = add i32 %add1092, 1
  %add1096 = mul i32 %add1094, %add109410787
  %mul109710788 = add i32 %add1096, %add109410787
  %add1098 = mul i32 %mul109710788, %add1094
  %add109810789 = add i32 %add1096, 1
  %add1100 = mul i32 %add1098, %add109810789
  %mul110110790 = add i32 %add1100, %add109810789
  %add1102 = mul i32 %mul110110790, %add1098
  %add110210791 = add i32 %add1100, 1
  %add1104 = mul i32 %add1102, %add110210791
  %mul110510792 = add i32 %add1104, %add110210791
  %add1106 = mul i32 %mul110510792, %add1102
  %add110610793 = add i32 %add1104, 1
  %add1108 = mul i32 %add1106, %add110610793
  %mul110910794 = add i32 %add1108, %add110610793
  %add1110 = mul i32 %mul110910794, %add1106
  %add111010795 = add i32 %add1108, 1
  %add1112 = mul i32 %add1110, %add111010795
  %mul111310796 = add i32 %add1112, %add111010795
  %add1114 = mul i32 %mul111310796, %add1110
  %add111410797 = add i32 %add1112, 1
  %add1116 = mul i32 %add1114, %add111410797
  %mul111710798 = add i32 %add1116, %add111410797
  %add1118 = mul i32 %mul111710798, %add1114
  %add111810799 = add i32 %add1116, 1
  %add1120 = mul i32 %add1118, %add111810799
  %mul112110800 = add i32 %add1120, %add111810799
  %add1122 = mul i32 %mul112110800, %add1118
  %add112210801 = add i32 %add1120, 1
  %add1124 = mul i32 %add1122, %add112210801
  %mul112510802 = add i32 %add1124, %add112210801
  %add1126 = mul i32 %mul112510802, %add1122
  %add112610803 = add i32 %add1124, 1
  %add1128 = mul i32 %add1126, %add112610803
  %mul112910804 = add i32 %add1128, %add112610803
  %add1130 = mul i32 %mul112910804, %add1126
  %add113010805 = add i32 %add1128, 1
  %add1132 = mul i32 %add1130, %add113010805
  %mul113310806 = add i32 %add1132, %add113010805
  %add1134 = mul i32 %mul113310806, %add1130
  %add113410807 = add i32 %add1132, 1
  %add1136 = mul i32 %add1134, %add113410807
  %mul113710808 = add i32 %add1136, %add113410807
  %add1138 = mul i32 %mul113710808, %add1134
  %add113810809 = add i32 %add1136, 1
  %add1140 = mul i32 %add1138, %add113810809
  %mul114110810 = add i32 %add1140, %add113810809
  %add1142 = mul i32 %mul114110810, %add1138
  %add114210811 = add i32 %add1140, 1
  %add1144 = mul i32 %add1142, %add114210811
  %mul114510812 = add i32 %add1144, %add114210811
  %add1146 = mul i32 %mul114510812, %add1142
  %add114610813 = add i32 %add1144, 1
  %add1148 = mul i32 %add1146, %add114610813
  %mul114910814 = add i32 %add1148, %add114610813
  %add1150 = mul i32 %mul114910814, %add1146
  %add115010815 = add i32 %add1148, 1
  %add1152 = mul i32 %add1150, %add115010815
  %mul115310816 = add i32 %add1152, %add115010815
  %add1154 = mul i32 %mul115310816, %add1150
  %add115410817 = add i32 %add1152, 1
  %add1156 = mul i32 %add1154, %add115410817
  %mul115710818 = add i32 %add1156, %add115410817
  %add1158 = mul i32 %mul115710818, %add1154
  %add115810819 = add i32 %add1156, 1
  %add1160 = mul i32 %add1158, %add115810819
  %mul116110820 = add i32 %add1160, %add115810819
  %add1162 = mul i32 %mul116110820, %add1158
  %add116210821 = add i32 %add1160, 1
  %add1164 = mul i32 %add1162, %add116210821
  %mul116510822 = add i32 %add1164, %add116210821
  %add1166 = mul i32 %mul116510822, %add1162
  %add116610823 = add i32 %add1164, 1
  %add1168 = mul i32 %add1166, %add116610823
  %mul116910824 = add i32 %add1168, %add116610823
  %add1170 = mul i32 %mul116910824, %add1166
  %add117010825 = add i32 %add1168, 1
  %add1172 = mul i32 %add1170, %add117010825
  %mul117310826 = add i32 %add1172, %add117010825
  %add1174 = mul i32 %mul117310826, %add1170
  %add117410827 = add i32 %add1172, 1
  %add1176 = mul i32 %add1174, %add117410827
  %mul117710828 = add i32 %add1176, %add117410827
  %add1178 = mul i32 %mul117710828, %add1174
  %add117810829 = add i32 %add1176, 1
  %add1180 = mul i32 %add1178, %add117810829
  %mul118110830 = add i32 %add1180, %add117810829
  %add1182 = mul i32 %mul118110830, %add1178
  %add118210831 = add i32 %add1180, 1
  %add1184 = mul i32 %add1182, %add118210831
  %mul118510832 = add i32 %add1184, %add118210831
  %add1186 = mul i32 %mul118510832, %add1182
  %add118610833 = add i32 %add1184, 1
  %add1188 = mul i32 %add1186, %add118610833
  %mul118910834 = add i32 %add1188, %add118610833
  %add1190 = mul i32 %mul118910834, %add1186
  %add119010835 = add i32 %add1188, 1
  %add1192 = mul i32 %add1190, %add119010835
  %mul119310836 = add i32 %add1192, %add119010835
  %add1194 = mul i32 %mul119310836, %add1190
  %add119410837 = add i32 %add1192, 1
  %add1196 = mul i32 %add1194, %add119410837
  %mul119710838 = add i32 %add1196, %add119410837
  %add1198 = mul i32 %mul119710838, %add1194
  %add119810839 = add i32 %add1196, 1
  %add1200 = mul i32 %add1198, %add119810839
  %mul120110840 = add i32 %add1200, %add119810839
  %add1202 = mul i32 %mul120110840, %add1198
  %add120210841 = add i32 %add1200, 1
  %add1204 = mul i32 %add1202, %add120210841
  %mul120510842 = add i32 %add1204, %add120210841
  %add1206 = mul i32 %mul120510842, %add1202
  %add120610843 = add i32 %add1204, 1
  %add1208 = mul i32 %add1206, %add120610843
  %mul120910844 = add i32 %add1208, %add120610843
  %add1210 = mul i32 %mul120910844, %add1206
  %add121010845 = add i32 %add1208, 1
  %add1212 = mul i32 %add1210, %add121010845
  %mul121310846 = add i32 %add1212, %add121010845
  %add1214 = mul i32 %mul121310846, %add1210
  %add121410847 = add i32 %add1212, 1
  %add1216 = mul i32 %add1214, %add121410847
  %mul121710848 = add i32 %add1216, %add121410847
  %add1218 = mul i32 %mul121710848, %add1214
  %add121810849 = add i32 %add1216, 1
  %add1220 = mul i32 %add1218, %add121810849
  %mul122110850 = add i32 %add1220, %add121810849
  %add1222 = mul i32 %mul122110850, %add1218
  %add122210851 = add i32 %add1220, 1
  %add1224 = mul i32 %add1222, %add122210851
  %mul122510852 = add i32 %add1224, %add122210851
  %add1226 = mul i32 %mul122510852, %add1222
  %add122610853 = add i32 %add1224, 1
  %add1228 = mul i32 %add1226, %add122610853
  %mul122910854 = add i32 %add1228, %add122610853
  %add1230 = mul i32 %mul122910854, %add1226
  %add123010855 = add i32 %add1228, 1
  %add1232 = mul i32 %add1230, %add123010855
  %mul123310856 = add i32 %add1232, %add123010855
  %add1234 = mul i32 %mul123310856, %add1230
  %add123410857 = add i32 %add1232, 1
  %add1236 = mul i32 %add1234, %add123410857
  %mul123710858 = add i32 %add1236, %add123410857
  %add1238 = mul i32 %mul123710858, %add1234
  %add123810859 = add i32 %add1236, 1
  %add1240 = mul i32 %add1238, %add123810859
  %mul124110860 = add i32 %add1240, %add123810859
  %add1242 = mul i32 %mul124110860, %add1238
  %add124210861 = add i32 %add1240, 1
  %add1244 = mul i32 %add1242, %add124210861
  %mul124510862 = add i32 %add1244, %add124210861
  %add1246 = mul i32 %mul124510862, %add1242
  %add124610863 = add i32 %add1244, 1
  %add1248 = mul i32 %add1246, %add124610863
  %mul124910864 = add i32 %add1248, %add124610863
  %add1250 = mul i32 %mul124910864, %add1246
  %add125010865 = add i32 %add1248, 1
  %add1252 = mul i32 %add1250, %add125010865
  %mul125310866 = add i32 %add1252, %add125010865
  %add1254 = mul i32 %mul125310866, %add1250
  %add125410867 = add i32 %add1252, 1
  %add1256 = mul i32 %add1254, %add125410867
  %mul125710868 = add i32 %add1256, %add125410867
  %add1258 = mul i32 %mul125710868, %add1254
  %add125810869 = add i32 %add1256, 1
  %add1260 = mul i32 %add1258, %add125810869
  %mul126110870 = add i32 %add1260, %add125810869
  %add1262 = mul i32 %mul126110870, %add1258
  %add126210871 = add i32 %add1260, 1
  %add1264 = mul i32 %add1262, %add126210871
  %mul126510872 = add i32 %add1264, %add126210871
  %add1266 = mul i32 %mul126510872, %add1262
  %add126610873 = add i32 %add1264, 1
  %add1268 = mul i32 %add1266, %add126610873
  %mul126910874 = add i32 %add1268, %add126610873
  %add1270 = mul i32 %mul126910874, %add1266
  %add127010875 = add i32 %add1268, 1
  %add1272 = mul i32 %add1270, %add127010875
  %mul127310876 = add i32 %add1272, %add127010875
  %add1274 = mul i32 %mul127310876, %add1270
  %add127410877 = add i32 %add1272, 1
  %add1276 = mul i32 %add1274, %add127410877
  %mul127710878 = add i32 %add1276, %add127410877
  %add1278 = mul i32 %mul127710878, %add1274
  %add127810879 = add i32 %add1276, 1
  %add1280 = mul i32 %add1278, %add127810879
  %mul128110880 = add i32 %add1280, %add127810879
  %add1282 = mul i32 %mul128110880, %add1278
  %add128210881 = add i32 %add1280, 1
  %add1284 = mul i32 %add1282, %add128210881
  %mul128510882 = add i32 %add1284, %add128210881
  %add1286 = mul i32 %mul128510882, %add1282
  %add128610883 = add i32 %add1284, 1
  %add1288 = mul i32 %add1286, %add128610883
  %mul128910884 = add i32 %add1288, %add128610883
  %add1290 = mul i32 %mul128910884, %add1286
  %add129010885 = add i32 %add1288, 1
  %add1292 = mul i32 %add1290, %add129010885
  %mul129310886 = add i32 %add1292, %add129010885
  %add1294 = mul i32 %mul129310886, %add1290
  %add129410887 = add i32 %add1292, 1
  %add1296 = mul i32 %add1294, %add129410887
  %mul129710888 = add i32 %add1296, %add129410887
  %add1298 = mul i32 %mul129710888, %add1294
  %add129810889 = add i32 %add1296, 1
  %add1300 = mul i32 %add1298, %add129810889
  %mul130110890 = add i32 %add1300, %add129810889
  %add1302 = mul i32 %mul130110890, %add1298
  %add130210891 = add i32 %add1300, 1
  %add1304 = mul i32 %add1302, %add130210891
  %mul130510892 = add i32 %add1304, %add130210891
  %add1306 = mul i32 %mul130510892, %add1302
  %add130610893 = add i32 %add1304, 1
  %add1308 = mul i32 %add1306, %add130610893
  %mul130910894 = add i32 %add1308, %add130610893
  %add1310 = mul i32 %mul130910894, %add1306
  %add131010895 = add i32 %add1308, 1
  %add1312 = mul i32 %add1310, %add131010895
  %mul131310896 = add i32 %add1312, %add131010895
  %add1314 = mul i32 %mul131310896, %add1310
  %add131410897 = add i32 %add1312, 1
  %add1316 = mul i32 %add1314, %add131410897
  %mul131710898 = add i32 %add1316, %add131410897
  %add1318 = mul i32 %mul131710898, %add1314
  %add131810899 = add i32 %add1316, 1
  %add1320 = mul i32 %add1318, %add131810899
  %mul132110900 = add i32 %add1320, %add131810899
  %add1322 = mul i32 %mul132110900, %add1318
  %add132210901 = add i32 %add1320, 1
  %add1324 = mul i32 %add1322, %add132210901
  %mul132510902 = add i32 %add1324, %add132210901
  %add1326 = mul i32 %mul132510902, %add1322
  %add132610903 = add i32 %add1324, 1
  %add1328 = mul i32 %add1326, %add132610903
  %mul132910904 = add i32 %add1328, %add132610903
  %add1330 = mul i32 %mul132910904, %add1326
  %add133010905 = add i32 %add1328, 1
  %add1332 = mul i32 %add1330, %add133010905
  %mul133310906 = add i32 %add1332, %add133010905
  %add1334 = mul i32 %mul133310906, %add1330
  %add133410907 = add i32 %add1332, 1
  %add1336 = mul i32 %add1334, %add133410907
  %mul133710908 = add i32 %add1336, %add133410907
  %add1338 = mul i32 %mul133710908, %add1334
  %add133810909 = add i32 %add1336, 1
  %add1340 = mul i32 %add1338, %add133810909
  %mul134110910 = add i32 %add1340, %add133810909
  %add1342 = mul i32 %mul134110910, %add1338
  %add134210911 = add i32 %add1340, 1
  %add1344 = mul i32 %add1342, %add134210911
  %mul134510912 = add i32 %add1344, %add134210911
  %add1346 = mul i32 %mul134510912, %add1342
  %add134610913 = add i32 %add1344, 1
  %add1348 = mul i32 %add1346, %add134610913
  %mul134910914 = add i32 %add1348, %add134610913
  %add1350 = mul i32 %mul134910914, %add1346
  %add135010915 = add i32 %add1348, 1
  %add1352 = mul i32 %add1350, %add135010915
  %mul135310916 = add i32 %add1352, %add135010915
  %add1354 = mul i32 %mul135310916, %add1350
  %add135410917 = add i32 %add1352, 1
  %add1356 = mul i32 %add1354, %add135410917
  %mul135710918 = add i32 %add1356, %add135410917
  %add1358 = mul i32 %mul135710918, %add1354
  %add135810919 = add i32 %add1356, 1
  %add1360 = mul i32 %add1358, %add135810919
  %mul136110920 = add i32 %add1360, %add135810919
  %add1362 = mul i32 %mul136110920, %add1358
  %add136210921 = add i32 %add1360, 1
  %add1364 = mul i32 %add1362, %add136210921
  %mul136510922 = add i32 %add1364, %add136210921
  %add1366 = mul i32 %mul136510922, %add1362
  %add136610923 = add i32 %add1364, 1
  %add1368 = mul i32 %add1366, %add136610923
  %mul136910924 = add i32 %add1368, %add136610923
  %add1370 = mul i32 %mul136910924, %add1366
  %add137010925 = add i32 %add1368, 1
  %add1372 = mul i32 %add1370, %add137010925
  %mul137310926 = add i32 %add1372, %add137010925
  %add1374 = mul i32 %mul137310926, %add1370
  %add137410927 = add i32 %add1372, 1
  %add1376 = mul i32 %add1374, %add137410927
  %mul137710928 = add i32 %add1376, %add137410927
  %add1378 = mul i32 %mul137710928, %add1374
  %add137810929 = add i32 %add1376, 1
  %add1380 = mul i32 %add1378, %add137810929
  %mul138110930 = add i32 %add1380, %add137810929
  %add1382 = mul i32 %mul138110930, %add1378
  %add138210931 = add i32 %add1380, 1
  %add1384 = mul i32 %add1382, %add138210931
  %mul138510932 = add i32 %add1384, %add138210931
  %add1386 = mul i32 %mul138510932, %add1382
  %add138610933 = add i32 %add1384, 1
  %add1388 = mul i32 %add1386, %add138610933
  %mul138910934 = add i32 %add1388, %add138610933
  %add1390 = mul i32 %mul138910934, %add1386
  %add139010935 = add i32 %add1388, 1
  %add1392 = mul i32 %add1390, %add139010935
  %mul139310936 = add i32 %add1392, %add139010935
  %add1394 = mul i32 %mul139310936, %add1390
  %add139410937 = add i32 %add1392, 1
  %add1396 = mul i32 %add1394, %add139410937
  %mul139710938 = add i32 %add1396, %add139410937
  %add1398 = mul i32 %mul139710938, %add1394
  %add139810939 = add i32 %add1396, 1
  %add1400 = mul i32 %add1398, %add139810939
  %mul140110940 = add i32 %add1400, %add139810939
  %add1402 = mul i32 %mul140110940, %add1398
  %add140210941 = add i32 %add1400, 1
  %add1404 = mul i32 %add1402, %add140210941
  %mul140510942 = add i32 %add1404, %add140210941
  %add1406 = mul i32 %mul140510942, %add1402
  %add140610943 = add i32 %add1404, 1
  %add1408 = mul i32 %add1406, %add140610943
  %mul140910944 = add i32 %add1408, %add140610943
  %add1410 = mul i32 %mul140910944, %add1406
  %add141010945 = add i32 %add1408, 1
  %add1412 = mul i32 %add1410, %add141010945
  %mul141310946 = add i32 %add1412, %add141010945
  %add1414 = mul i32 %mul141310946, %add1410
  %add141410947 = add i32 %add1412, 1
  %add1416 = mul i32 %add1414, %add141410947
  %mul141710948 = add i32 %add1416, %add141410947
  %add1418 = mul i32 %mul141710948, %add1414
  %add141810949 = add i32 %add1416, 1
  %add1420 = mul i32 %add1418, %add141810949
  %mul142110950 = add i32 %add1420, %add141810949
  %add1422 = mul i32 %mul142110950, %add1418
  %add142210951 = add i32 %add1420, 1
  %add1424 = mul i32 %add1422, %add142210951
  %mul142510952 = add i32 %add1424, %add142210951
  %add1426 = mul i32 %mul142510952, %add1422
  %add142610953 = add i32 %add1424, 1
  %add1428 = mul i32 %add1426, %add142610953
  %mul142910954 = add i32 %add1428, %add142610953
  %add1430 = mul i32 %mul142910954, %add1426
  %add143010955 = add i32 %add1428, 1
  %add1432 = mul i32 %add1430, %add143010955
  %mul143310956 = add i32 %add1432, %add143010955
  %add1434 = mul i32 %mul143310956, %add1430
  %add143410957 = add i32 %add1432, 1
  %add1436 = mul i32 %add1434, %add143410957
  %mul143710958 = add i32 %add1436, %add143410957
  %add1438 = mul i32 %mul143710958, %add1434
  %add143810959 = add i32 %add1436, 1
  %add1440 = mul i32 %add1438, %add143810959
  %mul144110960 = add i32 %add1440, %add143810959
  %add1442 = mul i32 %mul144110960, %add1438
  %add144210961 = add i32 %add1440, 1
  %add1444 = mul i32 %add1442, %add144210961
  %mul144510962 = add i32 %add1444, %add144210961
  %add1446 = mul i32 %mul144510962, %add1442
  %add144610963 = add i32 %add1444, 1
  %add1448 = mul i32 %add1446, %add144610963
  %mul144910964 = add i32 %add1448, %add144610963
  %add1450 = mul i32 %mul144910964, %add1446
  %add145010965 = add i32 %add1448, 1
  %add1452 = mul i32 %add1450, %add145010965
  %mul145310966 = add i32 %add1452, %add145010965
  %add1454 = mul i32 %mul145310966, %add1450
  %add145410967 = add i32 %add1452, 1
  %add1456 = mul i32 %add1454, %add145410967
  %mul145710968 = add i32 %add1456, %add145410967
  %add1458 = mul i32 %mul145710968, %add1454
  %add145810969 = add i32 %add1456, 1
  %add1460 = mul i32 %add1458, %add145810969
  %mul146110970 = add i32 %add1460, %add145810969
  %add1462 = mul i32 %mul146110970, %add1458
  %add146210971 = add i32 %add1460, 1
  %add1464 = mul i32 %add1462, %add146210971
  %mul146510972 = add i32 %add1464, %add146210971
  %add1466 = mul i32 %mul146510972, %add1462
  %add146610973 = add i32 %add1464, 1
  %add1468 = mul i32 %add1466, %add146610973
  %mul146910974 = add i32 %add1468, %add146610973
  %add1470 = mul i32 %mul146910974, %add1466
  %add147010975 = add i32 %add1468, 1
  %add1472 = mul i32 %add1470, %add147010975
  %mul147310976 = add i32 %add1472, %add147010975
  %add1474 = mul i32 %mul147310976, %add1470
  %add147410977 = add i32 %add1472, 1
  %add1476 = mul i32 %add1474, %add147410977
  %mul147710978 = add i32 %add1476, %add147410977
  %add1478 = mul i32 %mul147710978, %add1474
  %add147810979 = add i32 %add1476, 1
  %add1480 = mul i32 %add1478, %add147810979
  %mul148110980 = add i32 %add1480, %add147810979
  %add1482 = mul i32 %mul148110980, %add1478
  %add148210981 = add i32 %add1480, 1
  %add1484 = mul i32 %add1482, %add148210981
  %mul148510982 = add i32 %add1484, %add148210981
  %add1486 = mul i32 %mul148510982, %add1482
  %add148610983 = add i32 %add1484, 1
  %add1488 = mul i32 %add1486, %add148610983
  %mul148910984 = add i32 %add1488, %add148610983
  %add1490 = mul i32 %mul148910984, %add1486
  %add149010985 = add i32 %add1488, 1
  %add1492 = mul i32 %add1490, %add149010985
  %mul149310986 = add i32 %add1492, %add149010985
  %add1494 = mul i32 %mul149310986, %add1490
  %add149410987 = add i32 %add1492, 1
  %add1496 = mul i32 %add1494, %add149410987
  %mul149710988 = add i32 %add1496, %add149410987
  %add1498 = mul i32 %mul149710988, %add1494
  %add149810989 = add i32 %add1496, 1
  %add1500 = mul i32 %add1498, %add149810989
  %mul150110990 = add i32 %add1500, %add149810989
  %add1502 = mul i32 %mul150110990, %add1498
  %add150210991 = add i32 %add1500, 1
  %add1504 = mul i32 %add1502, %add150210991
  %mul150510992 = add i32 %add1504, %add150210991
  %add1506 = mul i32 %mul150510992, %add1502
  %add150610993 = add i32 %add1504, 1
  %add1508 = mul i32 %add1506, %add150610993
  %mul150910994 = add i32 %add1508, %add150610993
  %add1510 = mul i32 %mul150910994, %add1506
  %add151010995 = add i32 %add1508, 1
  %add1512 = mul i32 %add1510, %add151010995
  %mul151310996 = add i32 %add1512, %add151010995
  %add1514 = mul i32 %mul151310996, %add1510
  %add151410997 = add i32 %add1512, 1
  %add1516 = mul i32 %add1514, %add151410997
  %mul151710998 = add i32 %add1516, %add151410997
  %add1518 = mul i32 %mul151710998, %add1514
  %add151810999 = add i32 %add1516, 1
  %add1520 = mul i32 %add1518, %add151810999
  %mul152111000 = add i32 %add1520, %add151810999
  %add1522 = mul i32 %mul152111000, %add1518
  %add152211001 = add i32 %add1520, 1
  %add1524 = mul i32 %add1522, %add152211001
  %mul152511002 = add i32 %add1524, %add152211001
  %add1526 = mul i32 %mul152511002, %add1522
  %add152611003 = add i32 %add1524, 1
  %add1528 = mul i32 %add1526, %add152611003
  %mul152911004 = add i32 %add1528, %add152611003
  %add1530 = mul i32 %mul152911004, %add1526
  %add153011005 = add i32 %add1528, 1
  %add1532 = mul i32 %add1530, %add153011005
  %mul153311006 = add i32 %add1532, %add153011005
  %add1534 = mul i32 %mul153311006, %add1530
  %add153411007 = add i32 %add1532, 1
  %add1536 = mul i32 %add1534, %add153411007
  %mul153711008 = add i32 %add1536, %add153411007
  %add1538 = mul i32 %mul153711008, %add1534
  %add153811009 = add i32 %add1536, 1
  %add1540 = mul i32 %add1538, %add153811009
  %mul154111010 = add i32 %add1540, %add153811009
  %add1542 = mul i32 %mul154111010, %add1538
  %add154211011 = add i32 %add1540, 1
  %add1544 = mul i32 %add1542, %add154211011
  %mul154511012 = add i32 %add1544, %add154211011
  %add1546 = mul i32 %mul154511012, %add1542
  %add154611013 = add i32 %add1544, 1
  %add1548 = mul i32 %add1546, %add154611013
  %mul154911014 = add i32 %add1548, %add154611013
  %add1550 = mul i32 %mul154911014, %add1546
  %add155011015 = add i32 %add1548, 1
  %add1552 = mul i32 %add1550, %add155011015
  %mul155311016 = add i32 %add1552, %add155011015
  %add1554 = mul i32 %mul155311016, %add1550
  %add155411017 = add i32 %add1552, 1
  %add1556 = mul i32 %add1554, %add155411017
  %mul155711018 = add i32 %add1556, %add155411017
  %add1558 = mul i32 %mul155711018, %add1554
  %add155811019 = add i32 %add1556, 1
  %add1560 = mul i32 %add1558, %add155811019
  %mul156111020 = add i32 %add1560, %add155811019
  %add1562 = mul i32 %mul156111020, %add1558
  %add156211021 = add i32 %add1560, 1
  %add1564 = mul i32 %add1562, %add156211021
  %mul156511022 = add i32 %add1564, %add156211021
  %add1566 = mul i32 %mul156511022, %add1562
  %add156611023 = add i32 %add1564, 1
  %add1568 = mul i32 %add1566, %add156611023
  %mul156911024 = add i32 %add1568, %add156611023
  %add1570 = mul i32 %mul156911024, %add1566
  %add157011025 = add i32 %add1568, 1
  %add1572 = mul i32 %add1570, %add157011025
  %mul157311026 = add i32 %add1572, %add157011025
  %add1574 = mul i32 %mul157311026, %add1570
  %add157411027 = add i32 %add1572, 1
  %add1576 = mul i32 %add1574, %add157411027
  %mul157711028 = add i32 %add1576, %add157411027
  %add1578 = mul i32 %mul157711028, %add1574
  %add157811029 = add i32 %add1576, 1
  %add1580 = mul i32 %add1578, %add157811029
  %mul158111030 = add i32 %add1580, %add157811029
  %add1582 = mul i32 %mul158111030, %add1578
  %add158211031 = add i32 %add1580, 1
  %add1584 = mul i32 %add1582, %add158211031
  %mul158511032 = add i32 %add1584, %add158211031
  %add1586 = mul i32 %mul158511032, %add1582
  %add158611033 = add i32 %add1584, 1
  %add1588 = mul i32 %add1586, %add158611033
  %mul158911034 = add i32 %add1588, %add158611033
  %add1590 = mul i32 %mul158911034, %add1586
  %add159011035 = add i32 %add1588, 1
  %add1592 = mul i32 %add1590, %add159011035
  %mul159311036 = add i32 %add1592, %add159011035
  %add1594 = mul i32 %mul159311036, %add1590
  %add159411037 = add i32 %add1592, 1
  %add1596 = mul i32 %add1594, %add159411037
  %mul159711038 = add i32 %add1596, %add159411037
  %add1598 = mul i32 %mul159711038, %add1594
  %add159811039 = add i32 %add1596, 1
  %add1600 = mul i32 %add1598, %add159811039
  %mul160111040 = add i32 %add1600, %add159811039
  %add1602 = mul i32 %mul160111040, %add1598
  %add160211041 = add i32 %add1600, 1
  %add1604 = mul i32 %add1602, %add160211041
  %mul160511042 = add i32 %add1604, %add160211041
  %add1606 = mul i32 %mul160511042, %add1602
  %add160611043 = add i32 %add1604, 1
  %add1608 = mul i32 %add1606, %add160611043
  %mul160911044 = add i32 %add1608, %add160611043
  %add1610 = mul i32 %mul160911044, %add1606
  %add161011045 = add i32 %add1608, 1
  %add1612 = mul i32 %add1610, %add161011045
  %mul161311046 = add i32 %add1612, %add161011045
  %add1614 = mul i32 %mul161311046, %add1610
  %add161411047 = add i32 %add1612, 1
  %add1616 = mul i32 %add1614, %add161411047
  %mul161711048 = add i32 %add1616, %add161411047
  %add1618 = mul i32 %mul161711048, %add1614
  %add161811049 = add i32 %add1616, 1
  %add1620 = mul i32 %add1618, %add161811049
  %mul162111050 = add i32 %add1620, %add161811049
  %add1622 = mul i32 %mul162111050, %add1618
  %add162211051 = add i32 %add1620, 1
  %add1624 = mul i32 %add1622, %add162211051
  %mul162511052 = add i32 %add1624, %add162211051
  %add1626 = mul i32 %mul162511052, %add1622
  %add162611053 = add i32 %add1624, 1
  %add1628 = mul i32 %add1626, %add162611053
  %mul162911054 = add i32 %add1628, %add162611053
  %add1630 = mul i32 %mul162911054, %add1626
  %add163011055 = add i32 %add1628, 1
  %add1632 = mul i32 %add1630, %add163011055
  %mul163311056 = add i32 %add1632, %add163011055
  %add1634 = mul i32 %mul163311056, %add1630
  %add163411057 = add i32 %add1632, 1
  %add1636 = mul i32 %add1634, %add163411057
  %mul163711058 = add i32 %add1636, %add163411057
  %add1638 = mul i32 %mul163711058, %add1634
  %add163811059 = add i32 %add1636, 1
  %add1640 = mul i32 %add1638, %add163811059
  %mul164111060 = add i32 %add1640, %add163811059
  %add1642 = mul i32 %mul164111060, %add1638
  %add164211061 = add i32 %add1640, 1
  %add1644 = mul i32 %add1642, %add164211061
  %mul164511062 = add i32 %add1644, %add164211061
  %add1646 = mul i32 %mul164511062, %add1642
  %add164611063 = add i32 %add1644, 1
  %add1648 = mul i32 %add1646, %add164611063
  %mul164911064 = add i32 %add1648, %add164611063
  %add1650 = mul i32 %mul164911064, %add1646
  %add165011065 = add i32 %add1648, 1
  %add1652 = mul i32 %add1650, %add165011065
  %mul165311066 = add i32 %add1652, %add165011065
  %add1654 = mul i32 %mul165311066, %add1650
  %add165411067 = add i32 %add1652, 1
  %add1656 = mul i32 %add1654, %add165411067
  %mul165711068 = add i32 %add1656, %add165411067
  %add1658 = mul i32 %mul165711068, %add1654
  %add165811069 = add i32 %add1656, 1
  %add1660 = mul i32 %add1658, %add165811069
  %mul166111070 = add i32 %add1660, %add165811069
  %add1662 = mul i32 %mul166111070, %add1658
  %add166211071 = add i32 %add1660, 1
  %add1664 = mul i32 %add1662, %add166211071
  %mul166511072 = add i32 %add1664, %add166211071
  %add1666 = mul i32 %mul166511072, %add1662
  %add166611073 = add i32 %add1664, 1
  %add1668 = mul i32 %add1666, %add166611073
  %mul166911074 = add i32 %add1668, %add166611073
  %add1670 = mul i32 %mul166911074, %add1666
  %add167011075 = add i32 %add1668, 1
  %add1672 = mul i32 %add1670, %add167011075
  %mul167311076 = add i32 %add1672, %add167011075
  %add1674 = mul i32 %mul167311076, %add1670
  %add167411077 = add i32 %add1672, 1
  %add1676 = mul i32 %add1674, %add167411077
  %mul167711078 = add i32 %add1676, %add167411077
  %add1678 = mul i32 %mul167711078, %add1674
  %add167811079 = add i32 %add1676, 1
  %add1680 = mul i32 %add1678, %add167811079
  %mul168111080 = add i32 %add1680, %add167811079
  %add1682 = mul i32 %mul168111080, %add1678
  %add168211081 = add i32 %add1680, 1
  %add1684 = mul i32 %add1682, %add168211081
  %mul168511082 = add i32 %add1684, %add168211081
  %add1686 = mul i32 %mul168511082, %add1682
  %add168611083 = add i32 %add1684, 1
  %add1688 = mul i32 %add1686, %add168611083
  %mul168911084 = add i32 %add1688, %add168611083
  %add1690 = mul i32 %mul168911084, %add1686
  %add169011085 = add i32 %add1688, 1
  %add1692 = mul i32 %add1690, %add169011085
  %mul169311086 = add i32 %add1692, %add169011085
  %add1694 = mul i32 %mul169311086, %add1690
  %add169411087 = add i32 %add1692, 1
  %add1696 = mul i32 %add1694, %add169411087
  %mul169711088 = add i32 %add1696, %add169411087
  %add1698 = mul i32 %mul169711088, %add1694
  %add169811089 = add i32 %add1696, 1
  %add1700 = mul i32 %add1698, %add169811089
  %mul170111090 = add i32 %add1700, %add169811089
  %add1702 = mul i32 %mul170111090, %add1698
  %add170211091 = add i32 %add1700, 1
  %add1704 = mul i32 %add1702, %add170211091
  %mul170511092 = add i32 %add1704, %add170211091
  %add1706 = mul i32 %mul170511092, %add1702
  %add170611093 = add i32 %add1704, 1
  %add1708 = mul i32 %add1706, %add170611093
  %mul170911094 = add i32 %add1708, %add170611093
  %add1710 = mul i32 %mul170911094, %add1706
  %add171011095 = add i32 %add1708, 1
  %add1712 = mul i32 %add1710, %add171011095
  %mul171311096 = add i32 %add1712, %add171011095
  %add1714 = mul i32 %mul171311096, %add1710
  %add171411097 = add i32 %add1712, 1
  %add1716 = mul i32 %add1714, %add171411097
  %mul171711098 = add i32 %add1716, %add171411097
  %add1718 = mul i32 %mul171711098, %add1714
  %add171811099 = add i32 %add1716, 1
  %add1720 = mul i32 %add1718, %add171811099
  %mul172111100 = add i32 %add1720, %add171811099
  %add1722 = mul i32 %mul172111100, %add1718
  %add172211101 = add i32 %add1720, 1
  %add1724 = mul i32 %add1722, %add172211101
  %mul172511102 = add i32 %add1724, %add172211101
  %add1726 = mul i32 %mul172511102, %add1722
  %add172611103 = add i32 %add1724, 1
  %add1728 = mul i32 %add1726, %add172611103
  %mul172911104 = add i32 %add1728, %add172611103
  %add1730 = mul i32 %mul172911104, %add1726
  %add173011105 = add i32 %add1728, 1
  %add1732 = mul i32 %add1730, %add173011105
  %mul173311106 = add i32 %add1732, %add173011105
  %add1734 = mul i32 %mul173311106, %add1730
  %add173411107 = add i32 %add1732, 1
  %add1736 = mul i32 %add1734, %add173411107
  %mul173711108 = add i32 %add1736, %add173411107
  %add1738 = mul i32 %mul173711108, %add1734
  %add173811109 = add i32 %add1736, 1
  %add1740 = mul i32 %add1738, %add173811109
  %mul174111110 = add i32 %add1740, %add173811109
  %add1742 = mul i32 %mul174111110, %add1738
  %add174211111 = add i32 %add1740, 1
  %add1744 = mul i32 %add1742, %add174211111
  %mul174511112 = add i32 %add1744, %add174211111
  %add1746 = mul i32 %mul174511112, %add1742
  %add174611113 = add i32 %add1744, 1
  %add1748 = mul i32 %add1746, %add174611113
  %mul174911114 = add i32 %add1748, %add174611113
  %add1750 = mul i32 %mul174911114, %add1746
  %add175011115 = add i32 %add1748, 1
  %add1752 = mul i32 %add1750, %add175011115
  %mul175311116 = add i32 %add1752, %add175011115
  %add1754 = mul i32 %mul175311116, %add1750
  %add175411117 = add i32 %add1752, 1
  %add1756 = mul i32 %add1754, %add175411117
  %mul175711118 = add i32 %add1756, %add175411117
  %add1758 = mul i32 %mul175711118, %add1754
  %add175811119 = add i32 %add1756, 1
  %add1760 = mul i32 %add1758, %add175811119
  %mul176111120 = add i32 %add1760, %add175811119
  %add1762 = mul i32 %mul176111120, %add1758
  %add176211121 = add i32 %add1760, 1
  %add1764 = mul i32 %add1762, %add176211121
  %mul176511122 = add i32 %add1764, %add176211121
  %add1766 = mul i32 %mul176511122, %add1762
  %add176611123 = add i32 %add1764, 1
  %add1768 = mul i32 %add1766, %add176611123
  %mul176911124 = add i32 %add1768, %add176611123
  %add1770 = mul i32 %mul176911124, %add1766
  %add177011125 = add i32 %add1768, 1
  %add1772 = mul i32 %add1770, %add177011125
  %mul177311126 = add i32 %add1772, %add177011125
  %add1774 = mul i32 %mul177311126, %add1770
  %add177411127 = add i32 %add1772, 1
  %add1776 = mul i32 %add1774, %add177411127
  %mul177711128 = add i32 %add1776, %add177411127
  %add1778 = mul i32 %mul177711128, %add1774
  %add177811129 = add i32 %add1776, 1
  %add1780 = mul i32 %add1778, %add177811129
  %mul178111130 = add i32 %add1780, %add177811129
  %add1782 = mul i32 %mul178111130, %add1778
  %add178211131 = add i32 %add1780, 1
  %add1784 = mul i32 %add1782, %add178211131
  %mul178511132 = add i32 %add1784, %add178211131
  %add1786 = mul i32 %mul178511132, %add1782
  %add178611133 = add i32 %add1784, 1
  %add1788 = mul i32 %add1786, %add178611133
  %mul178911134 = add i32 %add1788, %add178611133
  %add1790 = mul i32 %mul178911134, %add1786
  %add179011135 = add i32 %add1788, 1
  %add1792 = mul i32 %add1790, %add179011135
  %mul179311136 = add i32 %add1792, %add179011135
  %add1794 = mul i32 %mul179311136, %add1790
  %add179411137 = add i32 %add1792, 1
  %add1796 = mul i32 %add1794, %add179411137
  %mul179711138 = add i32 %add1796, %add179411137
  %add1798 = mul i32 %mul179711138, %add1794
  %add179811139 = add i32 %add1796, 1
  %add1800 = mul i32 %add1798, %add179811139
  %mul180111140 = add i32 %add1800, %add179811139
  %add1802 = mul i32 %mul180111140, %add1798
  %add180211141 = add i32 %add1800, 1
  %add1804 = mul i32 %add1802, %add180211141
  %mul180511142 = add i32 %add1804, %add180211141
  %add1806 = mul i32 %mul180511142, %add1802
  %add180611143 = add i32 %add1804, 1
  %add1808 = mul i32 %add1806, %add180611143
  %mul180911144 = add i32 %add1808, %add180611143
  %add1810 = mul i32 %mul180911144, %add1806
  %add181011145 = add i32 %add1808, 1
  %add1812 = mul i32 %add1810, %add181011145
  %mul181311146 = add i32 %add1812, %add181011145
  %add1814 = mul i32 %mul181311146, %add1810
  %add181411147 = add i32 %add1812, 1
  %add1816 = mul i32 %add1814, %add181411147
  %mul181711148 = add i32 %add1816, %add181411147
  %add1818 = mul i32 %mul181711148, %add1814
  %add181811149 = add i32 %add1816, 1
  %add1820 = mul i32 %add1818, %add181811149
  %mul182111150 = add i32 %add1820, %add181811149
  %add1822 = mul i32 %mul182111150, %add1818
  %add182211151 = add i32 %add1820, 1
  %add1824 = mul i32 %add1822, %add182211151
  %mul182511152 = add i32 %add1824, %add182211151
  %add1826 = mul i32 %mul182511152, %add1822
  %add182611153 = add i32 %add1824, 1
  %add1828 = mul i32 %add1826, %add182611153
  %mul182911154 = add i32 %add1828, %add182611153
  %add1830 = mul i32 %mul182911154, %add1826
  %add183011155 = add i32 %add1828, 1
  %add1832 = mul i32 %add1830, %add183011155
  %mul183311156 = add i32 %add1832, %add183011155
  %add1834 = mul i32 %mul183311156, %add1830
  %add183411157 = add i32 %add1832, 1
  %add1836 = mul i32 %add1834, %add183411157
  %mul183711158 = add i32 %add1836, %add183411157
  %add1838 = mul i32 %mul183711158, %add1834
  %add183811159 = add i32 %add1836, 1
  %add1840 = mul i32 %add1838, %add183811159
  %mul184111160 = add i32 %add1840, %add183811159
  %add1842 = mul i32 %mul184111160, %add1838
  %add184211161 = add i32 %add1840, 1
  %add1844 = mul i32 %add1842, %add184211161
  %mul184511162 = add i32 %add1844, %add184211161
  %add1846 = mul i32 %mul184511162, %add1842
  %add184611163 = add i32 %add1844, 1
  %add1848 = mul i32 %add1846, %add184611163
  %mul184911164 = add i32 %add1848, %add184611163
  %add1850 = mul i32 %mul184911164, %add1846
  %add185011165 = add i32 %add1848, 1
  %add1852 = mul i32 %add1850, %add185011165
  %mul185311166 = add i32 %add1852, %add185011165
  %add1854 = mul i32 %mul185311166, %add1850
  %add185411167 = add i32 %add1852, 1
  %add1856 = mul i32 %add1854, %add185411167
  %mul185711168 = add i32 %add1856, %add185411167
  %add1858 = mul i32 %mul185711168, %add1854
  %add185811169 = add i32 %add1856, 1
  %add1860 = mul i32 %add1858, %add185811169
  %mul186111170 = add i32 %add1860, %add185811169
  %add1862 = mul i32 %mul186111170, %add1858
  %add186211171 = add i32 %add1860, 1
  %add1864 = mul i32 %add1862, %add186211171
  %mul186511172 = add i32 %add1864, %add186211171
  %add1866 = mul i32 %mul186511172, %add1862
  %add186611173 = add i32 %add1864, 1
  %add1868 = mul i32 %add1866, %add186611173
  %mul186911174 = add i32 %add1868, %add186611173
  %add1870 = mul i32 %mul186911174, %add1866
  %add187011175 = add i32 %add1868, 1
  %add1872 = mul i32 %add1870, %add187011175
  %mul187311176 = add i32 %add1872, %add187011175
  %add1874 = mul i32 %mul187311176, %add1870
  %add187411177 = add i32 %add1872, 1
  %add1876 = mul i32 %add1874, %add187411177
  %mul187711178 = add i32 %add1876, %add187411177
  %add1878 = mul i32 %mul187711178, %add1874
  %add187811179 = add i32 %add1876, 1
  %add1880 = mul i32 %add1878, %add187811179
  %mul188111180 = add i32 %add1880, %add187811179
  %add1882 = mul i32 %mul188111180, %add1878
  %add188211181 = add i32 %add1880, 1
  %add1884 = mul i32 %add1882, %add188211181
  %mul188511182 = add i32 %add1884, %add188211181
  %add1886 = mul i32 %mul188511182, %add1882
  %add188611183 = add i32 %add1884, 1
  %add1888 = mul i32 %add1886, %add188611183
  %mul188911184 = add i32 %add1888, %add188611183
  %add1890 = mul i32 %mul188911184, %add1886
  %add189011185 = add i32 %add1888, 1
  %add1892 = mul i32 %add1890, %add189011185
  %mul189311186 = add i32 %add1892, %add189011185
  %add1894 = mul i32 %mul189311186, %add1890
  %add189411187 = add i32 %add1892, 1
  %add1896 = mul i32 %add1894, %add189411187
  %mul189711188 = add i32 %add1896, %add189411187
  %add1898 = mul i32 %mul189711188, %add1894
  %add189811189 = add i32 %add1896, 1
  %add1900 = mul i32 %add1898, %add189811189
  %mul190111190 = add i32 %add1900, %add189811189
  %add1902 = mul i32 %mul190111190, %add1898
  %add190211191 = add i32 %add1900, 1
  %add1904 = mul i32 %add1902, %add190211191
  %mul190511192 = add i32 %add1904, %add190211191
  %add1906 = mul i32 %mul190511192, %add1902
  %add190611193 = add i32 %add1904, 1
  %add1908 = mul i32 %add1906, %add190611193
  %mul190911194 = add i32 %add1908, %add190611193
  %add1910 = mul i32 %mul190911194, %add1906
  %add191011195 = add i32 %add1908, 1
  %add1912 = mul i32 %add1910, %add191011195
  %mul191311196 = add i32 %add1912, %add191011195
  %add1914 = mul i32 %mul191311196, %add1910
  %add191411197 = add i32 %add1912, 1
  %add1916 = mul i32 %add1914, %add191411197
  %mul191711198 = add i32 %add1916, %add191411197
  %add1918 = mul i32 %mul191711198, %add1914
  %add191811199 = add i32 %add1916, 1
  %add1920 = mul i32 %add1918, %add191811199
  %mul192111200 = add i32 %add1920, %add191811199
  %add1922 = mul i32 %mul192111200, %add1918
  %add192211201 = add i32 %add1920, 1
  %add1924 = mul i32 %add1922, %add192211201
  %mul192511202 = add i32 %add1924, %add192211201
  %add1926 = mul i32 %mul192511202, %add1922
  %add192611203 = add i32 %add1924, 1
  %add1928 = mul i32 %add1926, %add192611203
  %mul192911204 = add i32 %add1928, %add192611203
  %add1930 = mul i32 %mul192911204, %add1926
  %add193011205 = add i32 %add1928, 1
  %add1932 = mul i32 %add1930, %add193011205
  %mul193311206 = add i32 %add1932, %add193011205
  %add1934 = mul i32 %mul193311206, %add1930
  %add193411207 = add i32 %add1932, 1
  %add1936 = mul i32 %add1934, %add193411207
  %mul193711208 = add i32 %add1936, %add193411207
  %add1938 = mul i32 %mul193711208, %add1934
  %add193811209 = add i32 %add1936, 1
  %add1940 = mul i32 %add1938, %add193811209
  %mul194111210 = add i32 %add1940, %add193811209
  %add1942 = mul i32 %mul194111210, %add1938
  %add194211211 = add i32 %add1940, 1
  %add1944 = mul i32 %add1942, %add194211211
  %mul194511212 = add i32 %add1944, %add194211211
  %add1946 = mul i32 %mul194511212, %add1942
  %add194611213 = add i32 %add1944, 1
  %add1948 = mul i32 %add1946, %add194611213
  %mul194911214 = add i32 %add1948, %add194611213
  %add1950 = mul i32 %mul194911214, %add1946
  %add195011215 = add i32 %add1948, 1
  %add1952 = mul i32 %add1950, %add195011215
  %mul195311216 = add i32 %add1952, %add195011215
  %add1954 = mul i32 %mul195311216, %add1950
  %add195411217 = add i32 %add1952, 1
  %add1956 = mul i32 %add1954, %add195411217
  %mul195711218 = add i32 %add1956, %add195411217
  %add1958 = mul i32 %mul195711218, %add1954
  %add195811219 = add i32 %add1956, 1
  %add1960 = mul i32 %add1958, %add195811219
  %mul196111220 = add i32 %add1960, %add195811219
  %add1962 = mul i32 %mul196111220, %add1958
  %add196211221 = add i32 %add1960, 1
  %add1964 = mul i32 %add1962, %add196211221
  %mul196511222 = add i32 %add1964, %add196211221
  %add1966 = mul i32 %mul196511222, %add1962
  %add196611223 = add i32 %add1964, 1
  %add1968 = mul i32 %add1966, %add196611223
  %mul196911224 = add i32 %add1968, %add196611223
  %add1970 = mul i32 %mul196911224, %add1966
  %add197011225 = add i32 %add1968, 1
  %add1972 = mul i32 %add1970, %add197011225
  %mul197311226 = add i32 %add1972, %add197011225
  %add1974 = mul i32 %mul197311226, %add1970
  %add197411227 = add i32 %add1972, 1
  %add1976 = mul i32 %add1974, %add197411227
  %mul197711228 = add i32 %add1976, %add197411227
  %add1978 = mul i32 %mul197711228, %add1974
  %add197811229 = add i32 %add1976, 1
  %add1980 = mul i32 %add1978, %add197811229
  %mul198111230 = add i32 %add1980, %add197811229
  %add1982 = mul i32 %mul198111230, %add1978
  %add198211231 = add i32 %add1980, 1
  %add1984 = mul i32 %add1982, %add198211231
  %mul198511232 = add i32 %add1984, %add198211231
  %add1986 = mul i32 %mul198511232, %add1982
  %add198611233 = add i32 %add1984, 1
  %add1988 = mul i32 %add1986, %add198611233
  %mul198911234 = add i32 %add1988, %add198611233
  %add1990 = mul i32 %mul198911234, %add1986
  %add199011235 = add i32 %add1988, 1
  %add1992 = mul i32 %add1990, %add199011235
  %mul199311236 = add i32 %add1992, %add199011235
  %add1994 = mul i32 %mul199311236, %add1990
  %add199411237 = add i32 %add1992, 1
  %add1996 = mul i32 %add1994, %add199411237
  %mul199711238 = add i32 %add1996, %add199411237
  %add1998 = mul i32 %mul199711238, %add1994
  %add199811239 = add i32 %add1996, 1
  %add2000 = mul i32 %add1998, %add199811239
  %mul200111240 = add i32 %add2000, %add199811239
  %add2002 = mul i32 %mul200111240, %add1998
  %add200211241 = add i32 %add2000, 1
  %add2004 = mul i32 %add2002, %add200211241
  %mul200511242 = add i32 %add2004, %add200211241
  %add2006 = mul i32 %mul200511242, %add2002
  %add200611243 = add i32 %add2004, 1
  %add2008 = mul i32 %add2006, %add200611243
  %mul200911244 = add i32 %add2008, %add200611243
  %add2010 = mul i32 %mul200911244, %add2006
  %add201011245 = add i32 %add2008, 1
  %add2012 = mul i32 %add2010, %add201011245
  %mul201311246 = add i32 %add2012, %add201011245
  %add2014 = mul i32 %mul201311246, %add2010
  %add201411247 = add i32 %add2012, 1
  %add2016 = mul i32 %add2014, %add201411247
  %mul201711248 = add i32 %add2016, %add201411247
  %add2018 = mul i32 %mul201711248, %add2014
  %add201811249 = add i32 %add2016, 1
  %add2020 = mul i32 %add2018, %add201811249
  %mul202111250 = add i32 %add2020, %add201811249
  %add2022 = mul i32 %mul202111250, %add2018
  %add202211251 = add i32 %add2020, 1
  %add2024 = mul i32 %add2022, %add202211251
  %mul202511252 = add i32 %add2024, %add202211251
  %add2026 = mul i32 %mul202511252, %add2022
  %add202611253 = add i32 %add2024, 1
  %add2028 = mul i32 %add2026, %add202611253
  %mul202911254 = add i32 %add2028, %add202611253
  %add2030 = mul i32 %mul202911254, %add2026
  %add203011255 = add i32 %add2028, 1
  %add2032 = mul i32 %add2030, %add203011255
  %mul203311256 = add i32 %add2032, %add203011255
  %add2034 = mul i32 %mul203311256, %add2030
  %add203411257 = add i32 %add2032, 1
  %add2036 = mul i32 %add2034, %add203411257
  %mul203711258 = add i32 %add2036, %add203411257
  %add2038 = mul i32 %mul203711258, %add2034
  %add203811259 = add i32 %add2036, 1
  %add2040 = mul i32 %add2038, %add203811259
  %mul204111260 = add i32 %add2040, %add203811259
  %add2042 = mul i32 %mul204111260, %add2038
  %add204211261 = add i32 %add2040, 1
  %add2044 = mul i32 %add2042, %add204211261
  %mul204511262 = add i32 %add2044, %add204211261
  %add2046 = mul i32 %mul204511262, %add2042
  %add204611263 = add i32 %add2044, 1
  %add2048 = mul i32 %add2046, %add204611263
  %mul204911264 = add i32 %add2048, %add204611263
  %add2050 = mul i32 %mul204911264, %add2046
  %add205011265 = add i32 %add2048, 1
  %add2052 = mul i32 %add2050, %add205011265
  %mul205311266 = add i32 %add2052, %add205011265
  %add2054 = mul i32 %mul205311266, %add2050
  %add205411267 = add i32 %add2052, 1
  %add2056 = mul i32 %add2054, %add205411267
  %mul205711268 = add i32 %add2056, %add205411267
  %add2058 = mul i32 %mul205711268, %add2054
  %add205811269 = add i32 %add2056, 1
  %add2060 = mul i32 %add2058, %add205811269
  %mul206111270 = add i32 %add2060, %add205811269
  %add2062 = mul i32 %mul206111270, %add2058
  %add206211271 = add i32 %add2060, 1
  %add2064 = mul i32 %add2062, %add206211271
  %mul206511272 = add i32 %add2064, %add206211271
  %add2066 = mul i32 %mul206511272, %add2062
  %add206611273 = add i32 %add2064, 1
  %add2068 = mul i32 %add2066, %add206611273
  %mul206911274 = add i32 %add2068, %add206611273
  %add2070 = mul i32 %mul206911274, %add2066
  %add207011275 = add i32 %add2068, 1
  %add2072 = mul i32 %add2070, %add207011275
  %mul207311276 = add i32 %add2072, %add207011275
  %add2074 = mul i32 %mul207311276, %add2070
  %add207411277 = add i32 %add2072, 1
  %add2076 = mul i32 %add2074, %add207411277
  %mul207711278 = add i32 %add2076, %add207411277
  %add2078 = mul i32 %mul207711278, %add2074
  %add207811279 = add i32 %add2076, 1
  %add2080 = mul i32 %add2078, %add207811279
  %mul208111280 = add i32 %add2080, %add207811279
  %add2082 = mul i32 %mul208111280, %add2078
  %add208211281 = add i32 %add2080, 1
  %add2084 = mul i32 %add2082, %add208211281
  %mul208511282 = add i32 %add2084, %add208211281
  %add2086 = mul i32 %mul208511282, %add2082
  %add208611283 = add i32 %add2084, 1
  %add2088 = mul i32 %add2086, %add208611283
  %mul208911284 = add i32 %add2088, %add208611283
  %add2090 = mul i32 %mul208911284, %add2086
  %add209011285 = add i32 %add2088, 1
  %add2092 = mul i32 %add2090, %add209011285
  %mul209311286 = add i32 %add2092, %add209011285
  %add2094 = mul i32 %mul209311286, %add2090
  %add209411287 = add i32 %add2092, 1
  %add2096 = mul i32 %add2094, %add209411287
  %mul209711288 = add i32 %add2096, %add209411287
  %add2098 = mul i32 %mul209711288, %add2094
  %add209811289 = add i32 %add2096, 1
  %add2100 = mul i32 %add2098, %add209811289
  %mul210111290 = add i32 %add2100, %add209811289
  %add2102 = mul i32 %mul210111290, %add2098
  %add210211291 = add i32 %add2100, 1
  %add2104 = mul i32 %add2102, %add210211291
  %mul210511292 = add i32 %add2104, %add210211291
  %add2106 = mul i32 %mul210511292, %add2102
  %add210611293 = add i32 %add2104, 1
  %add2108 = mul i32 %add2106, %add210611293
  %mul210911294 = add i32 %add2108, %add210611293
  %add2110 = mul i32 %mul210911294, %add2106
  %add211011295 = add i32 %add2108, 1
  %add2112 = mul i32 %add2110, %add211011295
  %mul211311296 = add i32 %add2112, %add211011295
  %add2114 = mul i32 %mul211311296, %add2110
  %add211411297 = add i32 %add2112, 1
  %add2116 = mul i32 %add2114, %add211411297
  %mul211711298 = add i32 %add2116, %add211411297
  %add2118 = mul i32 %mul211711298, %add2114
  %add211811299 = add i32 %add2116, 1
  %add2120 = mul i32 %add2118, %add211811299
  %mul212111300 = add i32 %add2120, %add211811299
  %add2122 = mul i32 %mul212111300, %add2118
  %add212211301 = add i32 %add2120, 1
  %add2124 = mul i32 %add2122, %add212211301
  %mul212511302 = add i32 %add2124, %add212211301
  %add2126 = mul i32 %mul212511302, %add2122
  %add212611303 = add i32 %add2124, 1
  %add2128 = mul i32 %add2126, %add212611303
  %mul212911304 = add i32 %add2128, %add212611303
  %add2130 = mul i32 %mul212911304, %add2126
  %add213011305 = add i32 %add2128, 1
  %add2132 = mul i32 %add2130, %add213011305
  %mul213311306 = add i32 %add2132, %add213011305
  %add2134 = mul i32 %mul213311306, %add2130
  %add213411307 = add i32 %add2132, 1
  %add2136 = mul i32 %add2134, %add213411307
  %mul213711308 = add i32 %add2136, %add213411307
  %add2138 = mul i32 %mul213711308, %add2134
  %add213811309 = add i32 %add2136, 1
  %add2140 = mul i32 %add2138, %add213811309
  %mul214111310 = add i32 %add2140, %add213811309
  %add2142 = mul i32 %mul214111310, %add2138
  %add214211311 = add i32 %add2140, 1
  %add2144 = mul i32 %add2142, %add214211311
  %mul214511312 = add i32 %add2144, %add214211311
  %add2146 = mul i32 %mul214511312, %add2142
  %add214611313 = add i32 %add2144, 1
  %add2148 = mul i32 %add2146, %add214611313
  %mul214911314 = add i32 %add2148, %add214611313
  %add2150 = mul i32 %mul214911314, %add2146
  %add215011315 = add i32 %add2148, 1
  %add2152 = mul i32 %add2150, %add215011315
  %mul215311316 = add i32 %add2152, %add215011315
  %add2154 = mul i32 %mul215311316, %add2150
  %add215411317 = add i32 %add2152, 1
  %add2156 = mul i32 %add2154, %add215411317
  %mul215711318 = add i32 %add2156, %add215411317
  %add2158 = mul i32 %mul215711318, %add2154
  %add215811319 = add i32 %add2156, 1
  %add2160 = mul i32 %add2158, %add215811319
  %mul216111320 = add i32 %add2160, %add215811319
  %add2162 = mul i32 %mul216111320, %add2158
  %add216211321 = add i32 %add2160, 1
  %add2164 = mul i32 %add2162, %add216211321
  %mul216511322 = add i32 %add2164, %add216211321
  %add2166 = mul i32 %mul216511322, %add2162
  %add216611323 = add i32 %add2164, 1
  %add2168 = mul i32 %add2166, %add216611323
  %mul216911324 = add i32 %add2168, %add216611323
  %add2170 = mul i32 %mul216911324, %add2166
  %add217011325 = add i32 %add2168, 1
  %add2172 = mul i32 %add2170, %add217011325
  %mul217311326 = add i32 %add2172, %add217011325
  %add2174 = mul i32 %mul217311326, %add2170
  %add217411327 = add i32 %add2172, 1
  %add2176 = mul i32 %add2174, %add217411327
  %mul217711328 = add i32 %add2176, %add217411327
  %add2178 = mul i32 %mul217711328, %add2174
  %add217811329 = add i32 %add2176, 1
  %add2180 = mul i32 %add2178, %add217811329
  %mul218111330 = add i32 %add2180, %add217811329
  %add2182 = mul i32 %mul218111330, %add2178
  %add218211331 = add i32 %add2180, 1
  %add2184 = mul i32 %add2182, %add218211331
  %mul218511332 = add i32 %add2184, %add218211331
  %add2186 = mul i32 %mul218511332, %add2182
  %add218611333 = add i32 %add2184, 1
  %add2188 = mul i32 %add2186, %add218611333
  %mul218911334 = add i32 %add2188, %add218611333
  %add2190 = mul i32 %mul218911334, %add2186
  %add219011335 = add i32 %add2188, 1
  %add2192 = mul i32 %add2190, %add219011335
  %mul219311336 = add i32 %add2192, %add219011335
  %add2194 = mul i32 %mul219311336, %add2190
  %add219411337 = add i32 %add2192, 1
  %add2196 = mul i32 %add2194, %add219411337
  %mul219711338 = add i32 %add2196, %add219411337
  %add2198 = mul i32 %mul219711338, %add2194
  %add219811339 = add i32 %add2196, 1
  %add2200 = mul i32 %add2198, %add219811339
  %mul220111340 = add i32 %add2200, %add219811339
  %add2202 = mul i32 %mul220111340, %add2198
  %add220211341 = add i32 %add2200, 1
  %add2204 = mul i32 %add2202, %add220211341
  %mul220511342 = add i32 %add2204, %add220211341
  %add2206 = mul i32 %mul220511342, %add2202
  %add220611343 = add i32 %add2204, 1
  %add2208 = mul i32 %add2206, %add220611343
  %mul220911344 = add i32 %add2208, %add220611343
  %add2210 = mul i32 %mul220911344, %add2206
  %add221011345 = add i32 %add2208, 1
  %add2212 = mul i32 %add2210, %add221011345
  %mul221311346 = add i32 %add2212, %add221011345
  %add2214 = mul i32 %mul221311346, %add2210
  %add221411347 = add i32 %add2212, 1
  %add2216 = mul i32 %add2214, %add221411347
  %mul221711348 = add i32 %add2216, %add221411347
  %add2218 = mul i32 %mul221711348, %add2214
  %add221811349 = add i32 %add2216, 1
  %add2220 = mul i32 %add2218, %add221811349
  %mul222111350 = add i32 %add2220, %add221811349
  %add2222 = mul i32 %mul222111350, %add2218
  %add222211351 = add i32 %add2220, 1
  %add2224 = mul i32 %add2222, %add222211351
  %mul222511352 = add i32 %add2224, %add222211351
  %add2226 = mul i32 %mul222511352, %add2222
  %add222611353 = add i32 %add2224, 1
  %add2228 = mul i32 %add2226, %add222611353
  %mul222911354 = add i32 %add2228, %add222611353
  %add2230 = mul i32 %mul222911354, %add2226
  %add223011355 = add i32 %add2228, 1
  %add2232 = mul i32 %add2230, %add223011355
  %mul223311356 = add i32 %add2232, %add223011355
  %add2234 = mul i32 %mul223311356, %add2230
  %add223411357 = add i32 %add2232, 1
  %add2236 = mul i32 %add2234, %add223411357
  %mul223711358 = add i32 %add2236, %add223411357
  %add2238 = mul i32 %mul223711358, %add2234
  %add223811359 = add i32 %add2236, 1
  %add2240 = mul i32 %add2238, %add223811359
  %mul224111360 = add i32 %add2240, %add223811359
  %add2242 = mul i32 %mul224111360, %add2238
  %add224211361 = add i32 %add2240, 1
  %add2244 = mul i32 %add2242, %add224211361
  %mul224511362 = add i32 %add2244, %add224211361
  %add2246 = mul i32 %mul224511362, %add2242
  %add224611363 = add i32 %add2244, 1
  %add2248 = mul i32 %add2246, %add224611363
  %mul224911364 = add i32 %add2248, %add224611363
  %add2250 = mul i32 %mul224911364, %add2246
  %add225011365 = add i32 %add2248, 1
  %add2252 = mul i32 %add2250, %add225011365
  %mul225311366 = add i32 %add2252, %add225011365
  %add2254 = mul i32 %mul225311366, %add2250
  %add225411367 = add i32 %add2252, 1
  %add2256 = mul i32 %add2254, %add225411367
  %mul225711368 = add i32 %add2256, %add225411367
  %add2258 = mul i32 %mul225711368, %add2254
  %add225811369 = add i32 %add2256, 1
  %add2260 = mul i32 %add2258, %add225811369
  %mul226111370 = add i32 %add2260, %add225811369
  %add2262 = mul i32 %mul226111370, %add2258
  %add226211371 = add i32 %add2260, 1
  %add2264 = mul i32 %add2262, %add226211371
  %mul226511372 = add i32 %add2264, %add226211371
  %add2266 = mul i32 %mul226511372, %add2262
  %add226611373 = add i32 %add2264, 1
  %add2268 = mul i32 %add2266, %add226611373
  %mul226911374 = add i32 %add2268, %add226611373
  %add2270 = mul i32 %mul226911374, %add2266
  %add227011375 = add i32 %add2268, 1
  %add2272 = mul i32 %add2270, %add227011375
  %mul227311376 = add i32 %add2272, %add227011375
  %add2274 = mul i32 %mul227311376, %add2270
  %add227411377 = add i32 %add2272, 1
  %add2276 = mul i32 %add2274, %add227411377
  %mul227711378 = add i32 %add2276, %add227411377
  %add2278 = mul i32 %mul227711378, %add2274
  %add227811379 = add i32 %add2276, 1
  %add2280 = mul i32 %add2278, %add227811379
  %mul228111380 = add i32 %add2280, %add227811379
  %add2282 = mul i32 %mul228111380, %add2278
  %add228211381 = add i32 %add2280, 1
  %add2284 = mul i32 %add2282, %add228211381
  %mul228511382 = add i32 %add2284, %add228211381
  %add2286 = mul i32 %mul228511382, %add2282
  %add228611383 = add i32 %add2284, 1
  %add2288 = mul i32 %add2286, %add228611383
  %mul228911384 = add i32 %add2288, %add228611383
  %add2290 = mul i32 %mul228911384, %add2286
  %add229011385 = add i32 %add2288, 1
  %add2292 = mul i32 %add2290, %add229011385
  %mul229311386 = add i32 %add2292, %add229011385
  %add2294 = mul i32 %mul229311386, %add2290
  %add229411387 = add i32 %add2292, 1
  %add2296 = mul i32 %add2294, %add229411387
  %mul229711388 = add i32 %add2296, %add229411387
  %add2298 = mul i32 %mul229711388, %add2294
  %add229811389 = add i32 %add2296, 1
  %add2300 = mul i32 %add2298, %add229811389
  %mul230111390 = add i32 %add2300, %add229811389
  %add2302 = mul i32 %mul230111390, %add2298
  %add230211391 = add i32 %add2300, 1
  %add2304 = mul i32 %add2302, %add230211391
  %mul230511392 = add i32 %add2304, %add230211391
  %add2306 = mul i32 %mul230511392, %add2302
  %add230611393 = add i32 %add2304, 1
  %add2308 = mul i32 %add2306, %add230611393
  %mul230911394 = add i32 %add2308, %add230611393
  %add2310 = mul i32 %mul230911394, %add2306
  %add231011395 = add i32 %add2308, 1
  %add2312 = mul i32 %add2310, %add231011395
  %mul231311396 = add i32 %add2312, %add231011395
  %add2314 = mul i32 %mul231311396, %add2310
  %add231411397 = add i32 %add2312, 1
  %add2316 = mul i32 %add2314, %add231411397
  %mul231711398 = add i32 %add2316, %add231411397
  %add2318 = mul i32 %mul231711398, %add2314
  %add231811399 = add i32 %add2316, 1
  %add2320 = mul i32 %add2318, %add231811399
  %mul232111400 = add i32 %add2320, %add231811399
  %add2322 = mul i32 %mul232111400, %add2318
  %add232211401 = add i32 %add2320, 1
  %add2324 = mul i32 %add2322, %add232211401
  %mul232511402 = add i32 %add2324, %add232211401
  %add2326 = mul i32 %mul232511402, %add2322
  %add232611403 = add i32 %add2324, 1
  %add2328 = mul i32 %add2326, %add232611403
  %mul232911404 = add i32 %add2328, %add232611403
  %add2330 = mul i32 %mul232911404, %add2326
  %add233011405 = add i32 %add2328, 1
  %add2332 = mul i32 %add2330, %add233011405
  %mul233311406 = add i32 %add2332, %add233011405
  %add2334 = mul i32 %mul233311406, %add2330
  %add233411407 = add i32 %add2332, 1
  %add2336 = mul i32 %add2334, %add233411407
  %mul233711408 = add i32 %add2336, %add233411407
  %add2338 = mul i32 %mul233711408, %add2334
  %add233811409 = add i32 %add2336, 1
  %add2340 = mul i32 %add2338, %add233811409
  %mul234111410 = add i32 %add2340, %add233811409
  %add2342 = mul i32 %mul234111410, %add2338
  %add234211411 = add i32 %add2340, 1
  %add2344 = mul i32 %add2342, %add234211411
  %mul234511412 = add i32 %add2344, %add234211411
  %add2346 = mul i32 %mul234511412, %add2342
  %add234611413 = add i32 %add2344, 1
  %add2348 = mul i32 %add2346, %add234611413
  %mul234911414 = add i32 %add2348, %add234611413
  %add2350 = mul i32 %mul234911414, %add2346
  %add235011415 = add i32 %add2348, 1
  %add2352 = mul i32 %add2350, %add235011415
  %mul235311416 = add i32 %add2352, %add235011415
  %add2354 = mul i32 %mul235311416, %add2350
  %add235411417 = add i32 %add2352, 1
  %add2356 = mul i32 %add2354, %add235411417
  %mul235711418 = add i32 %add2356, %add235411417
  %add2358 = mul i32 %mul235711418, %add2354
  %add235811419 = add i32 %add2356, 1
  %add2360 = mul i32 %add2358, %add235811419
  %mul236111420 = add i32 %add2360, %add235811419
  %add2362 = mul i32 %mul236111420, %add2358
  %add236211421 = add i32 %add2360, 1
  %add2364 = mul i32 %add2362, %add236211421
  %mul236511422 = add i32 %add2364, %add236211421
  %add2366 = mul i32 %mul236511422, %add2362
  %add236611423 = add i32 %add2364, 1
  %add2368 = mul i32 %add2366, %add236611423
  %mul236911424 = add i32 %add2368, %add236611423
  %add2370 = mul i32 %mul236911424, %add2366
  %add237011425 = add i32 %add2368, 1
  %add2372 = mul i32 %add2370, %add237011425
  %mul237311426 = add i32 %add2372, %add237011425
  %add2374 = mul i32 %mul237311426, %add2370
  %add237411427 = add i32 %add2372, 1
  %add2376 = mul i32 %add2374, %add237411427
  %mul237711428 = add i32 %add2376, %add237411427
  %add2378 = mul i32 %mul237711428, %add2374
  %add237811429 = add i32 %add2376, 1
  %add2380 = mul i32 %add2378, %add237811429
  %mul238111430 = add i32 %add2380, %add237811429
  %add2382 = mul i32 %mul238111430, %add2378
  %add238211431 = add i32 %add2380, 1
  %add2384 = mul i32 %add2382, %add238211431
  %mul238511432 = add i32 %add2384, %add238211431
  %add2386 = mul i32 %mul238511432, %add2382
  %add238611433 = add i32 %add2384, 1
  %add2388 = mul i32 %add2386, %add238611433
  %mul238911434 = add i32 %add2388, %add238611433
  %add2390 = mul i32 %mul238911434, %add2386
  %add239011435 = add i32 %add2388, 1
  %add2392 = mul i32 %add2390, %add239011435
  %mul239311436 = add i32 %add2392, %add239011435
  %add2394 = mul i32 %mul239311436, %add2390
  %add239411437 = add i32 %add2392, 1
  %add2396 = mul i32 %add2394, %add239411437
  %mul239711438 = add i32 %add2396, %add239411437
  %add2398 = mul i32 %mul239711438, %add2394
  %add239811439 = add i32 %add2396, 1
  %add2400 = mul i32 %add2398, %add239811439
  %mul240111440 = add i32 %add2400, %add239811439
  %add2402 = mul i32 %mul240111440, %add2398
  %add240211441 = add i32 %add2400, 1
  %add2404 = mul i32 %add2402, %add240211441
  %mul240511442 = add i32 %add2404, %add240211441
  %add2406 = mul i32 %mul240511442, %add2402
  %add240611443 = add i32 %add2404, 1
  %add2408 = mul i32 %add2406, %add240611443
  %mul240911444 = add i32 %add2408, %add240611443
  %add2410 = mul i32 %mul240911444, %add2406
  %add241011445 = add i32 %add2408, 1
  %add2412 = mul i32 %add2410, %add241011445
  %mul241311446 = add i32 %add2412, %add241011445
  %add2414 = mul i32 %mul241311446, %add2410
  %add241411447 = add i32 %add2412, 1
  %add2416 = mul i32 %add2414, %add241411447
  %mul241711448 = add i32 %add2416, %add241411447
  %add2418 = mul i32 %mul241711448, %add2414
  %add241811449 = add i32 %add2416, 1
  %add2420 = mul i32 %add2418, %add241811449
  %mul242111450 = add i32 %add2420, %add241811449
  %add2422 = mul i32 %mul242111450, %add2418
  %add242211451 = add i32 %add2420, 1
  %add2424 = mul i32 %add2422, %add242211451
  %mul242511452 = add i32 %add2424, %add242211451
  %add2426 = mul i32 %mul242511452, %add2422
  %add242611453 = add i32 %add2424, 1
  %add2428 = mul i32 %add2426, %add242611453
  %mul242911454 = add i32 %add2428, %add242611453
  %add2430 = mul i32 %mul242911454, %add2426
  %add243011455 = add i32 %add2428, 1
  %add2432 = mul i32 %add2430, %add243011455
  %mul243311456 = add i32 %add2432, %add243011455
  %add2434 = mul i32 %mul243311456, %add2430
  %add243411457 = add i32 %add2432, 1
  %add2436 = mul i32 %add2434, %add243411457
  %mul243711458 = add i32 %add2436, %add243411457
  %add2438 = mul i32 %mul243711458, %add2434
  %add243811459 = add i32 %add2436, 1
  %add2440 = mul i32 %add2438, %add243811459
  %mul244111460 = add i32 %add2440, %add243811459
  %add2442 = mul i32 %mul244111460, %add2438
  %add244211461 = add i32 %add2440, 1
  %add2444 = mul i32 %add2442, %add244211461
  %mul244511462 = add i32 %add2444, %add244211461
  %add2446 = mul i32 %mul244511462, %add2442
  %add244611463 = add i32 %add2444, 1
  %add2448 = mul i32 %add2446, %add244611463
  %mul244911464 = add i32 %add2448, %add244611463
  %add2450 = mul i32 %mul244911464, %add2446
  %add245011465 = add i32 %add2448, 1
  %add2452 = mul i32 %add2450, %add245011465
  %mul245311466 = add i32 %add2452, %add245011465
  %add2454 = mul i32 %mul245311466, %add2450
  %add245411467 = add i32 %add2452, 1
  %add2456 = mul i32 %add2454, %add245411467
  %mul245711468 = add i32 %add2456, %add245411467
  %add2458 = mul i32 %mul245711468, %add2454
  %add245811469 = add i32 %add2456, 1
  %add2460 = mul i32 %add2458, %add245811469
  %mul246111470 = add i32 %add2460, %add245811469
  %add2462 = mul i32 %mul246111470, %add2458
  %add246211471 = add i32 %add2460, 1
  %add2464 = mul i32 %add2462, %add246211471
  %mul246511472 = add i32 %add2464, %add246211471
  %add2466 = mul i32 %mul246511472, %add2462
  %add246611473 = add i32 %add2464, 1
  %add2468 = mul i32 %add2466, %add246611473
  %mul246911474 = add i32 %add2468, %add246611473
  %add2470 = mul i32 %mul246911474, %add2466
  %add247011475 = add i32 %add2468, 1
  %add2472 = mul i32 %add2470, %add247011475
  %mul247311476 = add i32 %add2472, %add247011475
  %add2474 = mul i32 %mul247311476, %add2470
  %add247411477 = add i32 %add2472, 1
  %add2476 = mul i32 %add2474, %add247411477
  %mul247711478 = add i32 %add2476, %add247411477
  %add2478 = mul i32 %mul247711478, %add2474
  %add247811479 = add i32 %add2476, 1
  %add2480 = mul i32 %add2478, %add247811479
  %mul248111480 = add i32 %add2480, %add247811479
  %add2482 = mul i32 %mul248111480, %add2478
  %add248211481 = add i32 %add2480, 1
  %add2484 = mul i32 %add2482, %add248211481
  %mul248511482 = add i32 %add2484, %add248211481
  %add2486 = mul i32 %mul248511482, %add2482
  %add248611483 = add i32 %add2484, 1
  %add2488 = mul i32 %add2486, %add248611483
  %mul248911484 = add i32 %add2488, %add248611483
  %add2490 = mul i32 %mul248911484, %add2486
  %add249011485 = add i32 %add2488, 1
  %add2492 = mul i32 %add2490, %add249011485
  %mul249311486 = add i32 %add2492, %add249011485
  %add2494 = mul i32 %mul249311486, %add2490
  %add249411487 = add i32 %add2492, 1
  %add2496 = mul i32 %add2494, %add249411487
  %mul249711488 = add i32 %add2496, %add249411487
  %add2498 = mul i32 %mul249711488, %add2494
  %add249811489 = add i32 %add2496, 1
  %add2500 = mul i32 %add2498, %add249811489
  %mul250111490 = add i32 %add2500, %add249811489
  %add2502 = mul i32 %mul250111490, %add2498
  %add250211491 = add i32 %add2500, 1
  %add2504 = mul i32 %add2502, %add250211491
  %mul250511492 = add i32 %add2504, %add250211491
  %add2506 = mul i32 %mul250511492, %add2502
  %add250611493 = add i32 %add2504, 1
  %add2508 = mul i32 %add2506, %add250611493
  %mul250911494 = add i32 %add2508, %add250611493
  %add2510 = mul i32 %mul250911494, %add2506
  %add251011495 = add i32 %add2508, 1
  %add2512 = mul i32 %add2510, %add251011495
  %mul251311496 = add i32 %add2512, %add251011495
  %add2514 = mul i32 %mul251311496, %add2510
  %add251411497 = add i32 %add2512, 1
  %add2516 = mul i32 %add2514, %add251411497
  %mul251711498 = add i32 %add2516, %add251411497
  %add2518 = mul i32 %mul251711498, %add2514
  %add251811499 = add i32 %add2516, 1
  %add2520 = mul i32 %add2518, %add251811499
  %mul252111500 = add i32 %add2520, %add251811499
  %add2522 = mul i32 %mul252111500, %add2518
  %add252211501 = add i32 %add2520, 1
  %add2524 = mul i32 %add2522, %add252211501
  %mul252511502 = add i32 %add2524, %add252211501
  %add2526 = mul i32 %mul252511502, %add2522
  %add252611503 = add i32 %add2524, 1
  %add2528 = mul i32 %add2526, %add252611503
  %mul252911504 = add i32 %add2528, %add252611503
  %add2530 = mul i32 %mul252911504, %add2526
  %add253011505 = add i32 %add2528, 1
  %add2532 = mul i32 %add2530, %add253011505
  %mul253311506 = add i32 %add2532, %add253011505
  %add2534 = mul i32 %mul253311506, %add2530
  %add253411507 = add i32 %add2532, 1
  %add2536 = mul i32 %add2534, %add253411507
  %mul253711508 = add i32 %add2536, %add253411507
  %add2538 = mul i32 %mul253711508, %add2534
  %add253811509 = add i32 %add2536, 1
  %add2540 = mul i32 %add2538, %add253811509
  %mul254111510 = add i32 %add2540, %add253811509
  %add2542 = mul i32 %mul254111510, %add2538
  %add254211511 = add i32 %add2540, 1
  %add2544 = mul i32 %add2542, %add254211511
  %mul254511512 = add i32 %add2544, %add254211511
  %add2546 = mul i32 %mul254511512, %add2542
  %add254611513 = add i32 %add2544, 1
  %add2548 = mul i32 %add2546, %add254611513
  %mul254911514 = add i32 %add2548, %add254611513
  %add2550 = mul i32 %mul254911514, %add2546
  %add255011515 = add i32 %add2548, 1
  %add2552 = mul i32 %add2550, %add255011515
  %mul255311516 = add i32 %add2552, %add255011515
  %add2554 = mul i32 %mul255311516, %add2550
  %add255411517 = add i32 %add2552, 1
  %add2556 = mul i32 %add2554, %add255411517
  %mul255711518 = add i32 %add2556, %add255411517
  %add2558 = mul i32 %mul255711518, %add2554
  %add255811519 = add i32 %add2556, 1
  %add2560 = mul i32 %add2558, %add255811519
  %mul256111520 = add i32 %add2560, %add255811519
  %add2562 = mul i32 %mul256111520, %add2558
  %add256211521 = add i32 %add2560, 1
  %add2564 = mul i32 %add2562, %add256211521
  %mul256511522 = add i32 %add2564, %add256211521
  %add2566 = mul i32 %mul256511522, %add2562
  %add256611523 = add i32 %add2564, 1
  %add2568 = mul i32 %add2566, %add256611523
  %mul256911524 = add i32 %add2568, %add256611523
  %add2570 = mul i32 %mul256911524, %add2566
  %add257011525 = add i32 %add2568, 1
  %add2572 = mul i32 %add2570, %add257011525
  %mul257311526 = add i32 %add2572, %add257011525
  %add2574 = mul i32 %mul257311526, %add2570
  %add257411527 = add i32 %add2572, 1
  %add2576 = mul i32 %add2574, %add257411527
  %mul257711528 = add i32 %add2576, %add257411527
  %add2578 = mul i32 %mul257711528, %add2574
  %add257811529 = add i32 %add2576, 1
  %add2580 = mul i32 %add2578, %add257811529
  %mul258111530 = add i32 %add2580, %add257811529
  %add2582 = mul i32 %mul258111530, %add2578
  %add258211531 = add i32 %add2580, 1
  %add2584 = mul i32 %add2582, %add258211531
  %mul258511532 = add i32 %add2584, %add258211531
  %add2586 = mul i32 %mul258511532, %add2582
  %add258611533 = add i32 %add2584, 1
  %add2588 = mul i32 %add2586, %add258611533
  %mul258911534 = add i32 %add2588, %add258611533
  %add2590 = mul i32 %mul258911534, %add2586
  %add259011535 = add i32 %add2588, 1
  %add2592 = mul i32 %add2590, %add259011535
  %mul259311536 = add i32 %add2592, %add259011535
  %add2594 = mul i32 %mul259311536, %add2590
  %add259411537 = add i32 %add2592, 1
  %add2596 = mul i32 %add2594, %add259411537
  %mul259711538 = add i32 %add2596, %add259411537
  %add2598 = mul i32 %mul259711538, %add2594
  %add259811539 = add i32 %add2596, 1
  %add2600 = mul i32 %add2598, %add259811539
  %mul260111540 = add i32 %add2600, %add259811539
  %add2602 = mul i32 %mul260111540, %add2598
  %add260211541 = add i32 %add2600, 1
  %add2604 = mul i32 %add2602, %add260211541
  %mul260511542 = add i32 %add2604, %add260211541
  %add2606 = mul i32 %mul260511542, %add2602
  %add260611543 = add i32 %add2604, 1
  %add2608 = mul i32 %add2606, %add260611543
  %mul260911544 = add i32 %add2608, %add260611543
  %add2610 = mul i32 %mul260911544, %add2606
  %add261011545 = add i32 %add2608, 1
  %add2612 = mul i32 %add2610, %add261011545
  %mul261311546 = add i32 %add2612, %add261011545
  %add2614 = mul i32 %mul261311546, %add2610
  %add261411547 = add i32 %add2612, 1
  %add2616 = mul i32 %add2614, %add261411547
  %mul261711548 = add i32 %add2616, %add261411547
  %add2618 = mul i32 %mul261711548, %add2614
  %add261811549 = add i32 %add2616, 1
  %add2620 = mul i32 %add2618, %add261811549
  %mul262111550 = add i32 %add2620, %add261811549
  %add2622 = mul i32 %mul262111550, %add2618
  %add262211551 = add i32 %add2620, 1
  %add2624 = mul i32 %add2622, %add262211551
  %mul262511552 = add i32 %add2624, %add262211551
  %add2626 = mul i32 %mul262511552, %add2622
  %add262611553 = add i32 %add2624, 1
  %add2628 = mul i32 %add2626, %add262611553
  %mul262911554 = add i32 %add2628, %add262611553
  %add2630 = mul i32 %mul262911554, %add2626
  %add263011555 = add i32 %add2628, 1
  %add2632 = mul i32 %add2630, %add263011555
  %mul263311556 = add i32 %add2632, %add263011555
  %add2634 = mul i32 %mul263311556, %add2630
  %add263411557 = add i32 %add2632, 1
  %add2636 = mul i32 %add2634, %add263411557
  %mul263711558 = add i32 %add2636, %add263411557
  %add2638 = mul i32 %mul263711558, %add2634
  %add263811559 = add i32 %add2636, 1
  %add2640 = mul i32 %add2638, %add263811559
  %mul264111560 = add i32 %add2640, %add263811559
  %add2642 = mul i32 %mul264111560, %add2638
  %add264211561 = add i32 %add2640, 1
  %add2644 = mul i32 %add2642, %add264211561
  %mul264511562 = add i32 %add2644, %add264211561
  %add2646 = mul i32 %mul264511562, %add2642
  %add264611563 = add i32 %add2644, 1
  %add2648 = mul i32 %add2646, %add264611563
  %mul264911564 = add i32 %add2648, %add264611563
  %add2650 = mul i32 %mul264911564, %add2646
  %add265011565 = add i32 %add2648, 1
  %add2652 = mul i32 %add2650, %add265011565
  %mul265311566 = add i32 %add2652, %add265011565
  %add2654 = mul i32 %mul265311566, %add2650
  %add265411567 = add i32 %add2652, 1
  %add2656 = mul i32 %add2654, %add265411567
  %mul265711568 = add i32 %add2656, %add265411567
  %add2658 = mul i32 %mul265711568, %add2654
  %add265811569 = add i32 %add2656, 1
  %add2660 = mul i32 %add2658, %add265811569
  %mul266111570 = add i32 %add2660, %add265811569
  %add2662 = mul i32 %mul266111570, %add2658
  %add266211571 = add i32 %add2660, 1
  %add2664 = mul i32 %add2662, %add266211571
  %mul266511572 = add i32 %add2664, %add266211571
  %add2666 = mul i32 %mul266511572, %add2662
  %add266611573 = add i32 %add2664, 1
  %add2668 = mul i32 %add2666, %add266611573
  %mul266911574 = add i32 %add2668, %add266611573
  %add2670 = mul i32 %mul266911574, %add2666
  %add267011575 = add i32 %add2668, 1
  %add2672 = mul i32 %add2670, %add267011575
  %mul267311576 = add i32 %add2672, %add267011575
  %add2674 = mul i32 %mul267311576, %add2670
  %add267411577 = add i32 %add2672, 1
  %add2676 = mul i32 %add2674, %add267411577
  %mul267711578 = add i32 %add2676, %add267411577
  %add2678 = mul i32 %mul267711578, %add2674
  %add267811579 = add i32 %add2676, 1
  %add2680 = mul i32 %add2678, %add267811579
  %mul268111580 = add i32 %add2680, %add267811579
  %add2682 = mul i32 %mul268111580, %add2678
  %add268211581 = add i32 %add2680, 1
  %add2684 = mul i32 %add2682, %add268211581
  %mul268511582 = add i32 %add2684, %add268211581
  %add2686 = mul i32 %mul268511582, %add2682
  %add268611583 = add i32 %add2684, 1
  %add2688 = mul i32 %add2686, %add268611583
  %mul268911584 = add i32 %add2688, %add268611583
  %add2690 = mul i32 %mul268911584, %add2686
  %add269011585 = add i32 %add2688, 1
  %add2692 = mul i32 %add2690, %add269011585
  %mul269311586 = add i32 %add2692, %add269011585
  %add2694 = mul i32 %mul269311586, %add2690
  %add269411587 = add i32 %add2692, 1
  %add2696 = mul i32 %add2694, %add269411587
  %mul269711588 = add i32 %add2696, %add269411587
  %add2698 = mul i32 %mul269711588, %add2694
  %add269811589 = add i32 %add2696, 1
  %add2700 = mul i32 %add2698, %add269811589
  %mul270111590 = add i32 %add2700, %add269811589
  %add2702 = mul i32 %mul270111590, %add2698
  %add270211591 = add i32 %add2700, 1
  %add2704 = mul i32 %add2702, %add270211591
  %mul270511592 = add i32 %add2704, %add270211591
  %add2706 = mul i32 %mul270511592, %add2702
  %add270611593 = add i32 %add2704, 1
  %add2708 = mul i32 %add2706, %add270611593
  %mul270911594 = add i32 %add2708, %add270611593
  %add2710 = mul i32 %mul270911594, %add2706
  %add271011595 = add i32 %add2708, 1
  %add2712 = mul i32 %add2710, %add271011595
  %mul271311596 = add i32 %add2712, %add271011595
  %add2714 = mul i32 %mul271311596, %add2710
  %add271411597 = add i32 %add2712, 1
  %add2716 = mul i32 %add2714, %add271411597
  %mul271711598 = add i32 %add2716, %add271411597
  %add2718 = mul i32 %mul271711598, %add2714
  %add271811599 = add i32 %add2716, 1
  %add2720 = mul i32 %add2718, %add271811599
  %mul272111600 = add i32 %add2720, %add271811599
  %add2722 = mul i32 %mul272111600, %add2718
  %add272211601 = add i32 %add2720, 1
  %add2724 = mul i32 %add2722, %add272211601
  %mul272511602 = add i32 %add2724, %add272211601
  %add2726 = mul i32 %mul272511602, %add2722
  %add272611603 = add i32 %add2724, 1
  %add2728 = mul i32 %add2726, %add272611603
  %mul272911604 = add i32 %add2728, %add272611603
  %add2730 = mul i32 %mul272911604, %add2726
  %add273011605 = add i32 %add2728, 1
  %add2732 = mul i32 %add2730, %add273011605
  %mul273311606 = add i32 %add2732, %add273011605
  %add2734 = mul i32 %mul273311606, %add2730
  %add273411607 = add i32 %add2732, 1
  %add2736 = mul i32 %add2734, %add273411607
  %mul273711608 = add i32 %add2736, %add273411607
  %add2738 = mul i32 %mul273711608, %add2734
  %add273811609 = add i32 %add2736, 1
  %add2740 = mul i32 %add2738, %add273811609
  %mul274111610 = add i32 %add2740, %add273811609
  %add2742 = mul i32 %mul274111610, %add2738
  %add274211611 = add i32 %add2740, 1
  %add2744 = mul i32 %add2742, %add274211611
  %mul274511612 = add i32 %add2744, %add274211611
  %add2746 = mul i32 %mul274511612, %add2742
  %add274611613 = add i32 %add2744, 1
  %add2748 = mul i32 %add2746, %add274611613
  %mul274911614 = add i32 %add2748, %add274611613
  %add2750 = mul i32 %mul274911614, %add2746
  %add275011615 = add i32 %add2748, 1
  %add2752 = mul i32 %add2750, %add275011615
  %mul275311616 = add i32 %add2752, %add275011615
  %add2754 = mul i32 %mul275311616, %add2750
  %add275411617 = add i32 %add2752, 1
  %add2756 = mul i32 %add2754, %add275411617
  %mul275711618 = add i32 %add2756, %add275411617
  %add2758 = mul i32 %mul275711618, %add2754
  %add275811619 = add i32 %add2756, 1
  %add2760 = mul i32 %add2758, %add275811619
  %mul276111620 = add i32 %add2760, %add275811619
  %add2762 = mul i32 %mul276111620, %add2758
  %add276211621 = add i32 %add2760, 1
  %add2764 = mul i32 %add2762, %add276211621
  %mul276511622 = add i32 %add2764, %add276211621
  %add2766 = mul i32 %mul276511622, %add2762
  %add276611623 = add i32 %add2764, 1
  %add2768 = mul i32 %add2766, %add276611623
  %mul276911624 = add i32 %add2768, %add276611623
  %add2770 = mul i32 %mul276911624, %add2766
  %add277011625 = add i32 %add2768, 1
  %add2772 = mul i32 %add2770, %add277011625
  %mul277311626 = add i32 %add2772, %add277011625
  %add2774 = mul i32 %mul277311626, %add2770
  %add277411627 = add i32 %add2772, 1
  %add2776 = mul i32 %add2774, %add277411627
  %mul277711628 = add i32 %add2776, %add277411627
  %add2778 = mul i32 %mul277711628, %add2774
  %add277811629 = add i32 %add2776, 1
  %add2780 = mul i32 %add2778, %add277811629
  %mul278111630 = add i32 %add2780, %add277811629
  %add2782 = mul i32 %mul278111630, %add2778
  %add278211631 = add i32 %add2780, 1
  %add2784 = mul i32 %add2782, %add278211631
  %mul278511632 = add i32 %add2784, %add278211631
  %add2786 = mul i32 %mul278511632, %add2782
  %add278611633 = add i32 %add2784, 1
  %add2788 = mul i32 %add2786, %add278611633
  %mul278911634 = add i32 %add2788, %add278611633
  %add2790 = mul i32 %mul278911634, %add2786
  %add279011635 = add i32 %add2788, 1
  %add2792 = mul i32 %add2790, %add279011635
  %mul279311636 = add i32 %add2792, %add279011635
  %add2794 = mul i32 %mul279311636, %add2790
  %add279411637 = add i32 %add2792, 1
  %add2796 = mul i32 %add2794, %add279411637
  %mul279711638 = add i32 %add2796, %add279411637
  %add2798 = mul i32 %mul279711638, %add2794
  %add279811639 = add i32 %add2796, 1
  %add2800 = mul i32 %add2798, %add279811639
  %mul280111640 = add i32 %add2800, %add279811639
  %add2802 = mul i32 %mul280111640, %add2798
  %add280211641 = add i32 %add2800, 1
  %add2804 = mul i32 %add2802, %add280211641
  %mul280511642 = add i32 %add2804, %add280211641
  %add2806 = mul i32 %mul280511642, %add2802
  %add280611643 = add i32 %add2804, 1
  %add2808 = mul i32 %add2806, %add280611643
  %mul280911644 = add i32 %add2808, %add280611643
  %add2810 = mul i32 %mul280911644, %add2806
  %add281011645 = add i32 %add2808, 1
  %add2812 = mul i32 %add2810, %add281011645
  %mul281311646 = add i32 %add2812, %add281011645
  %add2814 = mul i32 %mul281311646, %add2810
  %add281411647 = add i32 %add2812, 1
  %add2816 = mul i32 %add2814, %add281411647
  %mul281711648 = add i32 %add2816, %add281411647
  %add2818 = mul i32 %mul281711648, %add2814
  %add281811649 = add i32 %add2816, 1
  %add2820 = mul i32 %add2818, %add281811649
  %mul282111650 = add i32 %add2820, %add281811649
  %add2822 = mul i32 %mul282111650, %add2818
  %add282211651 = add i32 %add2820, 1
  %add2824 = mul i32 %add2822, %add282211651
  %mul282511652 = add i32 %add2824, %add282211651
  %add2826 = mul i32 %mul282511652, %add2822
  %add282611653 = add i32 %add2824, 1
  %add2828 = mul i32 %add2826, %add282611653
  %mul282911654 = add i32 %add2828, %add282611653
  %add2830 = mul i32 %mul282911654, %add2826
  %add283011655 = add i32 %add2828, 1
  %add2832 = mul i32 %add2830, %add283011655
  %mul283311656 = add i32 %add2832, %add283011655
  %add2834 = mul i32 %mul283311656, %add2830
  %add283411657 = add i32 %add2832, 1
  %add2836 = mul i32 %add2834, %add283411657
  %mul283711658 = add i32 %add2836, %add283411657
  %add2838 = mul i32 %mul283711658, %add2834
  %add283811659 = add i32 %add2836, 1
  %add2840 = mul i32 %add2838, %add283811659
  %mul284111660 = add i32 %add2840, %add283811659
  %add2842 = mul i32 %mul284111660, %add2838
  %add284211661 = add i32 %add2840, 1
  %add2844 = mul i32 %add2842, %add284211661
  %mul284511662 = add i32 %add2844, %add284211661
  %add2846 = mul i32 %mul284511662, %add2842
  %add284611663 = add i32 %add2844, 1
  %add2848 = mul i32 %add2846, %add284611663
  %mul284911664 = add i32 %add2848, %add284611663
  %add2850 = mul i32 %mul284911664, %add2846
  %add285011665 = add i32 %add2848, 1
  %add2852 = mul i32 %add2850, %add285011665
  %mul285311666 = add i32 %add2852, %add285011665
  %add2854 = mul i32 %mul285311666, %add2850
  %add285411667 = add i32 %add2852, 1
  %add2856 = mul i32 %add2854, %add285411667
  %mul285711668 = add i32 %add2856, %add285411667
  %add2858 = mul i32 %mul285711668, %add2854
  %add285811669 = add i32 %add2856, 1
  %add2860 = mul i32 %add2858, %add285811669
  %mul286111670 = add i32 %add2860, %add285811669
  %add2862 = mul i32 %mul286111670, %add2858
  %add286211671 = add i32 %add2860, 1
  %add2864 = mul i32 %add2862, %add286211671
  %mul286511672 = add i32 %add2864, %add286211671
  %add2866 = mul i32 %mul286511672, %add2862
  %add286611673 = add i32 %add2864, 1
  %add2868 = mul i32 %add2866, %add286611673
  %mul286911674 = add i32 %add2868, %add286611673
  %add2870 = mul i32 %mul286911674, %add2866
  %add287011675 = add i32 %add2868, 1
  %add2872 = mul i32 %add2870, %add287011675
  %mul287311676 = add i32 %add2872, %add287011675
  %add2874 = mul i32 %mul287311676, %add2870
  %add287411677 = add i32 %add2872, 1
  %add2876 = mul i32 %add2874, %add287411677
  %mul287711678 = add i32 %add2876, %add287411677
  %add2878 = mul i32 %mul287711678, %add2874
  %add287811679 = add i32 %add2876, 1
  %add2880 = mul i32 %add2878, %add287811679
  %mul288111680 = add i32 %add2880, %add287811679
  %add2882 = mul i32 %mul288111680, %add2878
  %add288211681 = add i32 %add2880, 1
  %add2884 = mul i32 %add2882, %add288211681
  %mul288511682 = add i32 %add2884, %add288211681
  %add2886 = mul i32 %mul288511682, %add2882
  %add288611683 = add i32 %add2884, 1
  %add2888 = mul i32 %add2886, %add288611683
  %mul288911684 = add i32 %add2888, %add288611683
  %add2890 = mul i32 %mul288911684, %add2886
  %add289011685 = add i32 %add2888, 1
  %add2892 = mul i32 %add2890, %add289011685
  %mul289311686 = add i32 %add2892, %add289011685
  %add2894 = mul i32 %mul289311686, %add2890
  %add289411687 = add i32 %add2892, 1
  %add2896 = mul i32 %add2894, %add289411687
  %mul289711688 = add i32 %add2896, %add289411687
  %add2898 = mul i32 %mul289711688, %add2894
  %add289811689 = add i32 %add2896, 1
  %add2900 = mul i32 %add2898, %add289811689
  %mul290111690 = add i32 %add2900, %add289811689
  %add2902 = mul i32 %mul290111690, %add2898
  %add290211691 = add i32 %add2900, 1
  %add2904 = mul i32 %add2902, %add290211691
  %mul290511692 = add i32 %add2904, %add290211691
  %add2906 = mul i32 %mul290511692, %add2902
  %add290611693 = add i32 %add2904, 1
  %add2908 = mul i32 %add2906, %add290611693
  %mul290911694 = add i32 %add2908, %add290611693
  %add2910 = mul i32 %mul290911694, %add2906
  %add291011695 = add i32 %add2908, 1
  %add2912 = mul i32 %add2910, %add291011695
  %mul291311696 = add i32 %add2912, %add291011695
  %add2914 = mul i32 %mul291311696, %add2910
  %add291411697 = add i32 %add2912, 1
  %add2916 = mul i32 %add2914, %add291411697
  %mul291711698 = add i32 %add2916, %add291411697
  %add2918 = mul i32 %mul291711698, %add2914
  %add291811699 = add i32 %add2916, 1
  %add2920 = mul i32 %add2918, %add291811699
  %mul292111700 = add i32 %add2920, %add291811699
  %add2922 = mul i32 %mul292111700, %add2918
  %add292211701 = add i32 %add2920, 1
  %add2924 = mul i32 %add2922, %add292211701
  %mul292511702 = add i32 %add2924, %add292211701
  %add2926 = mul i32 %mul292511702, %add2922
  %add292611703 = add i32 %add2924, 1
  %add2928 = mul i32 %add2926, %add292611703
  %mul292911704 = add i32 %add2928, %add292611703
  %add2930 = mul i32 %mul292911704, %add2926
  %add293011705 = add i32 %add2928, 1
  %add2932 = mul i32 %add2930, %add293011705
  %mul293311706 = add i32 %add2932, %add293011705
  %add2934 = mul i32 %mul293311706, %add2930
  %add293411707 = add i32 %add2932, 1
  %add2936 = mul i32 %add2934, %add293411707
  %mul293711708 = add i32 %add2936, %add293411707
  %add2938 = mul i32 %mul293711708, %add2934
  %add293811709 = add i32 %add2936, 1
  %add2940 = mul i32 %add2938, %add293811709
  %mul294111710 = add i32 %add2940, %add293811709
  %add2942 = mul i32 %mul294111710, %add2938
  %add294211711 = add i32 %add2940, 1
  %add2944 = mul i32 %add2942, %add294211711
  %mul294511712 = add i32 %add2944, %add294211711
  %add2946 = mul i32 %mul294511712, %add2942
  %add294611713 = add i32 %add2944, 1
  %add2948 = mul i32 %add2946, %add294611713
  %mul294911714 = add i32 %add2948, %add294611713
  %add2950 = mul i32 %mul294911714, %add2946
  %add295011715 = add i32 %add2948, 1
  %add2952 = mul i32 %add2950, %add295011715
  %mul295311716 = add i32 %add2952, %add295011715
  %add2954 = mul i32 %mul295311716, %add2950
  %add295411717 = add i32 %add2952, 1
  %add2956 = mul i32 %add2954, %add295411717
  %mul295711718 = add i32 %add2956, %add295411717
  %add2958 = mul i32 %mul295711718, %add2954
  %add295811719 = add i32 %add2956, 1
  %add2960 = mul i32 %add2958, %add295811719
  %mul296111720 = add i32 %add2960, %add295811719
  %add2962 = mul i32 %mul296111720, %add2958
  %add296211721 = add i32 %add2960, 1
  %add2964 = mul i32 %add2962, %add296211721
  %mul296511722 = add i32 %add2964, %add296211721
  %add2966 = mul i32 %mul296511722, %add2962
  %add296611723 = add i32 %add2964, 1
  %add2968 = mul i32 %add2966, %add296611723
  %mul296911724 = add i32 %add2968, %add296611723
  %add2970 = mul i32 %mul296911724, %add2966
  %add297011725 = add i32 %add2968, 1
  %add2972 = mul i32 %add2970, %add297011725
  %mul297311726 = add i32 %add2972, %add297011725
  %add2974 = mul i32 %mul297311726, %add2970
  %add297411727 = add i32 %add2972, 1
  %add2976 = mul i32 %add2974, %add297411727
  %mul297711728 = add i32 %add2976, %add297411727
  %add2978 = mul i32 %mul297711728, %add2974
  %add297811729 = add i32 %add2976, 1
  %add2980 = mul i32 %add2978, %add297811729
  %mul298111730 = add i32 %add2980, %add297811729
  %add2982 = mul i32 %mul298111730, %add2978
  %add298211731 = add i32 %add2980, 1
  %add2984 = mul i32 %add2982, %add298211731
  %mul298511732 = add i32 %add2984, %add298211731
  %add2986 = mul i32 %mul298511732, %add2982
  %add298611733 = add i32 %add2984, 1
  %add2988 = mul i32 %add2986, %add298611733
  %mul298911734 = add i32 %add2988, %add298611733
  %add2990 = mul i32 %mul298911734, %add2986
  %add299011735 = add i32 %add2988, 1
  %add2992 = mul i32 %add2990, %add299011735
  %mul299311736 = add i32 %add2992, %add299011735
  %add2994 = mul i32 %mul299311736, %add2990
  %add299411737 = add i32 %add2992, 1
  %add2996 = mul i32 %add2994, %add299411737
  %mul299711738 = add i32 %add2996, %add299411737
  %add2998 = mul i32 %mul299711738, %add2994
  %add299811739 = add i32 %add2996, 1
  %add3000 = mul i32 %add2998, %add299811739
  %mul300111740 = add i32 %add3000, %add299811739
  %add3002 = mul i32 %mul300111740, %add2998
  %add300211741 = add i32 %add3000, 1
  %add3004 = mul i32 %add3002, %add300211741
  %mul300511742 = add i32 %add3004, %add300211741
  %add3006 = mul i32 %mul300511742, %add3002
  %add300611743 = add i32 %add3004, 1
  %add3008 = mul i32 %add3006, %add300611743
  %mul300911744 = add i32 %add3008, %add300611743
  %add3010 = mul i32 %mul300911744, %add3006
  %add301011745 = add i32 %add3008, 1
  %add3012 = mul i32 %add3010, %add301011745
  %mul301311746 = add i32 %add3012, %add301011745
  %add3014 = mul i32 %mul301311746, %add3010
  %add301411747 = add i32 %add3012, 1
  %add3016 = mul i32 %add3014, %add301411747
  %mul301711748 = add i32 %add3016, %add301411747
  %add3018 = mul i32 %mul301711748, %add3014
  %add301811749 = add i32 %add3016, 1
  %add3020 = mul i32 %add3018, %add301811749
  %mul302111750 = add i32 %add3020, %add301811749
  %add3022 = mul i32 %mul302111750, %add3018
  %add302211751 = add i32 %add3020, 1
  %add3024 = mul i32 %add3022, %add302211751
  %mul302511752 = add i32 %add3024, %add302211751
  %add3026 = mul i32 %mul302511752, %add3022
  %add302611753 = add i32 %add3024, 1
  %add3028 = mul i32 %add3026, %add302611753
  %mul302911754 = add i32 %add3028, %add302611753
  %add3030 = mul i32 %mul302911754, %add3026
  %add303011755 = add i32 %add3028, 1
  %add3032 = mul i32 %add3030, %add303011755
  %mul303311756 = add i32 %add3032, %add303011755
  %add3034 = mul i32 %mul303311756, %add3030
  %add303411757 = add i32 %add3032, 1
  %add3036 = mul i32 %add3034, %add303411757
  %mul303711758 = add i32 %add3036, %add303411757
  %add3038 = mul i32 %mul303711758, %add3034
  %add303811759 = add i32 %add3036, 1
  %add3040 = mul i32 %add3038, %add303811759
  %mul304111760 = add i32 %add3040, %add303811759
  %add3042 = mul i32 %mul304111760, %add3038
  %add304211761 = add i32 %add3040, 1
  %add3044 = mul i32 %add3042, %add304211761
  %mul304511762 = add i32 %add3044, %add304211761
  %add3046 = mul i32 %mul304511762, %add3042
  %add304611763 = add i32 %add3044, 1
  %add3048 = mul i32 %add3046, %add304611763
  %mul304911764 = add i32 %add3048, %add304611763
  %add3050 = mul i32 %mul304911764, %add3046
  %add305011765 = add i32 %add3048, 1
  %add3052 = mul i32 %add3050, %add305011765
  %mul305311766 = add i32 %add3052, %add305011765
  %add3054 = mul i32 %mul305311766, %add3050
  %add305411767 = add i32 %add3052, 1
  %add3056 = mul i32 %add3054, %add305411767
  %mul305711768 = add i32 %add3056, %add305411767
  %add3058 = mul i32 %mul305711768, %add3054
  %add305811769 = add i32 %add3056, 1
  %add3060 = mul i32 %add3058, %add305811769
  %mul306111770 = add i32 %add3060, %add305811769
  %add3062 = mul i32 %mul306111770, %add3058
  %add306211771 = add i32 %add3060, 1
  %add3064 = mul i32 %add3062, %add306211771
  %mul306511772 = add i32 %add3064, %add306211771
  %add3066 = mul i32 %mul306511772, %add3062
  %add306611773 = add i32 %add3064, 1
  %add3068 = mul i32 %add3066, %add306611773
  %mul306911774 = add i32 %add3068, %add306611773
  %add3070 = mul i32 %mul306911774, %add3066
  %add307011775 = add i32 %add3068, 1
  %add3072 = mul i32 %add3070, %add307011775
  %mul307311776 = add i32 %add3072, %add307011775
  %add3074 = mul i32 %mul307311776, %add3070
  %add307411777 = add i32 %add3072, 1
  %add3076 = mul i32 %add3074, %add307411777
  %mul307711778 = add i32 %add3076, %add307411777
  %add3078 = mul i32 %mul307711778, %add3074
  %add307811779 = add i32 %add3076, 1
  %add3080 = mul i32 %add3078, %add307811779
  %mul308111780 = add i32 %add3080, %add307811779
  %add3082 = mul i32 %mul308111780, %add3078
  %add308211781 = add i32 %add3080, 1
  %add3084 = mul i32 %add3082, %add308211781
  %mul308511782 = add i32 %add3084, %add308211781
  %add3086 = mul i32 %mul308511782, %add3082
  %add308611783 = add i32 %add3084, 1
  %add3088 = mul i32 %add3086, %add308611783
  %mul308911784 = add i32 %add3088, %add308611783
  %add3090 = mul i32 %mul308911784, %add3086
  %add309011785 = add i32 %add3088, 1
  %add3092 = mul i32 %add3090, %add309011785
  %mul309311786 = add i32 %add3092, %add309011785
  %add3094 = mul i32 %mul309311786, %add3090
  %add309411787 = add i32 %add3092, 1
  %add3096 = mul i32 %add3094, %add309411787
  %mul309711788 = add i32 %add3096, %add309411787
  %add3098 = mul i32 %mul309711788, %add3094
  %add309811789 = add i32 %add3096, 1
  %add3100 = mul i32 %add3098, %add309811789
  %mul310111790 = add i32 %add3100, %add309811789
  %add3102 = mul i32 %mul310111790, %add3098
  %add310211791 = add i32 %add3100, 1
  %add3104 = mul i32 %add3102, %add310211791
  %mul310511792 = add i32 %add3104, %add310211791
  %add3106 = mul i32 %mul310511792, %add3102
  %add310611793 = add i32 %add3104, 1
  %add3108 = mul i32 %add3106, %add310611793
  %mul310911794 = add i32 %add3108, %add310611793
  %add3110 = mul i32 %mul310911794, %add3106
  %add311011795 = add i32 %add3108, 1
  %add3112 = mul i32 %add3110, %add311011795
  %mul311311796 = add i32 %add3112, %add311011795
  %add3114 = mul i32 %mul311311796, %add3110
  %add311411797 = add i32 %add3112, 1
  %add3116 = mul i32 %add3114, %add311411797
  %mul311711798 = add i32 %add3116, %add311411797
  %add3118 = mul i32 %mul311711798, %add3114
  %add311811799 = add i32 %add3116, 1
  %add3120 = mul i32 %add3118, %add311811799
  %mul312111800 = add i32 %add3120, %add311811799
  %add3122 = mul i32 %mul312111800, %add3118
  %add312211801 = add i32 %add3120, 1
  %add3124 = mul i32 %add3122, %add312211801
  %mul312511802 = add i32 %add3124, %add312211801
  %add3126 = mul i32 %mul312511802, %add3122
  %add312611803 = add i32 %add3124, 1
  %add3128 = mul i32 %add3126, %add312611803
  %mul312911804 = add i32 %add3128, %add312611803
  %add3130 = mul i32 %mul312911804, %add3126
  %add313011805 = add i32 %add3128, 1
  %add3132 = mul i32 %add3130, %add313011805
  %mul313311806 = add i32 %add3132, %add313011805
  %add3134 = mul i32 %mul313311806, %add3130
  %add313411807 = add i32 %add3132, 1
  %add3136 = mul i32 %add3134, %add313411807
  %mul313711808 = add i32 %add3136, %add313411807
  %add3138 = mul i32 %mul313711808, %add3134
  %add313811809 = add i32 %add3136, 1
  %add3140 = mul i32 %add3138, %add313811809
  %mul314111810 = add i32 %add3140, %add313811809
  %add3142 = mul i32 %mul314111810, %add3138
  %add314211811 = add i32 %add3140, 1
  %add3144 = mul i32 %add3142, %add314211811
  %mul314511812 = add i32 %add3144, %add314211811
  %add3146 = mul i32 %mul314511812, %add3142
  %add314611813 = add i32 %add3144, 1
  %add3148 = mul i32 %add3146, %add314611813
  %mul314911814 = add i32 %add3148, %add314611813
  %add3150 = mul i32 %mul314911814, %add3146
  %add315011815 = add i32 %add3148, 1
  %add3152 = mul i32 %add3150, %add315011815
  %mul315311816 = add i32 %add3152, %add315011815
  %add3154 = mul i32 %mul315311816, %add3150
  %add315411817 = add i32 %add3152, 1
  %add3156 = mul i32 %add3154, %add315411817
  %mul315711818 = add i32 %add3156, %add315411817
  %add3158 = mul i32 %mul315711818, %add3154
  %add315811819 = add i32 %add3156, 1
  %add3160 = mul i32 %add3158, %add315811819
  %mul316111820 = add i32 %add3160, %add315811819
  %add3162 = mul i32 %mul316111820, %add3158
  %add316211821 = add i32 %add3160, 1
  %add3164 = mul i32 %add3162, %add316211821
  %mul316511822 = add i32 %add3164, %add316211821
  %add3166 = mul i32 %mul316511822, %add3162
  %add316611823 = add i32 %add3164, 1
  %add3168 = mul i32 %add3166, %add316611823
  %mul316911824 = add i32 %add3168, %add316611823
  %add3170 = mul i32 %mul316911824, %add3166
  %add317011825 = add i32 %add3168, 1
  %add3172 = mul i32 %add3170, %add317011825
  %mul317311826 = add i32 %add3172, %add317011825
  %add3174 = mul i32 %mul317311826, %add3170
  %add317411827 = add i32 %add3172, 1
  %add3176 = mul i32 %add3174, %add317411827
  %mul317711828 = add i32 %add3176, %add317411827
  %add3178 = mul i32 %mul317711828, %add3174
  %add317811829 = add i32 %add3176, 1
  %add3180 = mul i32 %add3178, %add317811829
  %mul318111830 = add i32 %add3180, %add317811829
  %add3182 = mul i32 %mul318111830, %add3178
  %add318211831 = add i32 %add3180, 1
  %add3184 = mul i32 %add3182, %add318211831
  %mul318511832 = add i32 %add3184, %add318211831
  %add3186 = mul i32 %mul318511832, %add3182
  %add318611833 = add i32 %add3184, 1
  %add3188 = mul i32 %add3186, %add318611833
  %mul318911834 = add i32 %add3188, %add318611833
  %add3190 = mul i32 %mul318911834, %add3186
  %add319011835 = add i32 %add3188, 1
  %add3192 = mul i32 %add3190, %add319011835
  %mul319311836 = add i32 %add3192, %add319011835
  %add3194 = mul i32 %mul319311836, %add3190
  %add319411837 = add i32 %add3192, 1
  %add3196 = mul i32 %add3194, %add319411837
  %mul319711838 = add i32 %add3196, %add319411837
  %add3198 = mul i32 %mul319711838, %add3194
  %add319811839 = add i32 %add3196, 1
  %add3200 = mul i32 %add3198, %add319811839
  %mul320111840 = add i32 %add3200, %add319811839
  %add3202 = mul i32 %mul320111840, %add3198
  %add320211841 = add i32 %add3200, 1
  %add3204 = mul i32 %add3202, %add320211841
  %mul320511842 = add i32 %add3204, %add320211841
  %add3206 = mul i32 %mul320511842, %add3202
  %add320611843 = add i32 %add3204, 1
  %add3208 = mul i32 %add3206, %add320611843
  %mul320911844 = add i32 %add3208, %add320611843
  %add3210 = mul i32 %mul320911844, %add3206
  %add321011845 = add i32 %add3208, 1
  %add3212 = mul i32 %add3210, %add321011845
  %mul321311846 = add i32 %add3212, %add321011845
  %add3214 = mul i32 %mul321311846, %add3210
  %add321411847 = add i32 %add3212, 1
  %add3216 = mul i32 %add3214, %add321411847
  %mul321711848 = add i32 %add3216, %add321411847
  %add3218 = mul i32 %mul321711848, %add3214
  %add321811849 = add i32 %add3216, 1
  %add3220 = mul i32 %add3218, %add321811849
  %mul322111850 = add i32 %add3220, %add321811849
  %add3222 = mul i32 %mul322111850, %add3218
  %add322211851 = add i32 %add3220, 1
  %add3224 = mul i32 %add3222, %add322211851
  %mul322511852 = add i32 %add3224, %add322211851
  %add3226 = mul i32 %mul322511852, %add3222
  %add322611853 = add i32 %add3224, 1
  %add3228 = mul i32 %add3226, %add322611853
  %mul322911854 = add i32 %add3228, %add322611853
  %add3230 = mul i32 %mul322911854, %add3226
  %add323011855 = add i32 %add3228, 1
  %add3232 = mul i32 %add3230, %add323011855
  %mul323311856 = add i32 %add3232, %add323011855
  %add3234 = mul i32 %mul323311856, %add3230
  %add323411857 = add i32 %add3232, 1
  %add3236 = mul i32 %add3234, %add323411857
  %mul323711858 = add i32 %add3236, %add323411857
  %add3238 = mul i32 %mul323711858, %add3234
  %add323811859 = add i32 %add3236, 1
  %add3240 = mul i32 %add3238, %add323811859
  %mul324111860 = add i32 %add3240, %add323811859
  %add3242 = mul i32 %mul324111860, %add3238
  %add324211861 = add i32 %add3240, 1
  %add3244 = mul i32 %add3242, %add324211861
  %mul324511862 = add i32 %add3244, %add324211861
  %add3246 = mul i32 %mul324511862, %add3242
  %add324611863 = add i32 %add3244, 1
  %add3248 = mul i32 %add3246, %add324611863
  %mul324911864 = add i32 %add3248, %add324611863
  %add3250 = mul i32 %mul324911864, %add3246
  %add325011865 = add i32 %add3248, 1
  %add3252 = mul i32 %add3250, %add325011865
  %mul325311866 = add i32 %add3252, %add325011865
  %add3254 = mul i32 %mul325311866, %add3250
  %add325411867 = add i32 %add3252, 1
  %add3256 = mul i32 %add3254, %add325411867
  %mul325711868 = add i32 %add3256, %add325411867
  %add3258 = mul i32 %mul325711868, %add3254
  %add325811869 = add i32 %add3256, 1
  %add3260 = mul i32 %add3258, %add325811869
  %mul326111870 = add i32 %add3260, %add325811869
  %add3262 = mul i32 %mul326111870, %add3258
  %add326211871 = add i32 %add3260, 1
  %add3264 = mul i32 %add3262, %add326211871
  %mul326511872 = add i32 %add3264, %add326211871
  %add3266 = mul i32 %mul326511872, %add3262
  %add326611873 = add i32 %add3264, 1
  %add3268 = mul i32 %add3266, %add326611873
  %mul326911874 = add i32 %add3268, %add326611873
  %add3270 = mul i32 %mul326911874, %add3266
  %add327011875 = add i32 %add3268, 1
  %add3272 = mul i32 %add3270, %add327011875
  %mul327311876 = add i32 %add3272, %add327011875
  %add3274 = mul i32 %mul327311876, %add3270
  %add327411877 = add i32 %add3272, 1
  %add3276 = mul i32 %add3274, %add327411877
  %mul327711878 = add i32 %add3276, %add327411877
  %add3278 = mul i32 %mul327711878, %add3274
  %add327811879 = add i32 %add3276, 1
  %add3280 = mul i32 %add3278, %add327811879
  %mul328111880 = add i32 %add3280, %add327811879
  %add3282 = mul i32 %mul328111880, %add3278
  %add328211881 = add i32 %add3280, 1
  %add3284 = mul i32 %add3282, %add328211881
  %mul328511882 = add i32 %add3284, %add328211881
  %add3286 = mul i32 %mul328511882, %add3282
  %add328611883 = add i32 %add3284, 1
  %add3288 = mul i32 %add3286, %add328611883
  %mul328911884 = add i32 %add3288, %add328611883
  %add3290 = mul i32 %mul328911884, %add3286
  %add329011885 = add i32 %add3288, 1
  %add3292 = mul i32 %add3290, %add329011885
  %mul329311886 = add i32 %add3292, %add329011885
  %add3294 = mul i32 %mul329311886, %add3290
  %add329411887 = add i32 %add3292, 1
  %add3296 = mul i32 %add3294, %add329411887
  %mul329711888 = add i32 %add3296, %add329411887
  %add3298 = mul i32 %mul329711888, %add3294
  %add329811889 = add i32 %add3296, 1
  %add3300 = mul i32 %add3298, %add329811889
  %mul330111890 = add i32 %add3300, %add329811889
  %add3302 = mul i32 %mul330111890, %add3298
  %add330211891 = add i32 %add3300, 1
  %add3304 = mul i32 %add3302, %add330211891
  %mul330511892 = add i32 %add3304, %add330211891
  %add3306 = mul i32 %mul330511892, %add3302
  %add330611893 = add i32 %add3304, 1
  %add3308 = mul i32 %add3306, %add330611893
  %mul330911894 = add i32 %add3308, %add330611893
  %add3310 = mul i32 %mul330911894, %add3306
  %add331011895 = add i32 %add3308, 1
  %add3312 = mul i32 %add3310, %add331011895
  %mul331311896 = add i32 %add3312, %add331011895
  %add3314 = mul i32 %mul331311896, %add3310
  %add331411897 = add i32 %add3312, 1
  %add3316 = mul i32 %add3314, %add331411897
  %mul331711898 = add i32 %add3316, %add331411897
  %add3318 = mul i32 %mul331711898, %add3314
  %add331811899 = add i32 %add3316, 1
  %add3320 = mul i32 %add3318, %add331811899
  %mul332111900 = add i32 %add3320, %add331811899
  %add3322 = mul i32 %mul332111900, %add3318
  %add332211901 = add i32 %add3320, 1
  %add3324 = mul i32 %add3322, %add332211901
  %mul332511902 = add i32 %add3324, %add332211901
  %add3326 = mul i32 %mul332511902, %add3322
  %add332611903 = add i32 %add3324, 1
  %add3328 = mul i32 %add3326, %add332611903
  %mul332911904 = add i32 %add3328, %add332611903
  %add3330 = mul i32 %mul332911904, %add3326
  %add333011905 = add i32 %add3328, 1
  %add3332 = mul i32 %add3330, %add333011905
  %mul333311906 = add i32 %add3332, %add333011905
  %add3334 = mul i32 %mul333311906, %add3330
  %add333411907 = add i32 %add3332, 1
  %add3336 = mul i32 %add3334, %add333411907
  %mul333711908 = add i32 %add3336, %add333411907
  %add3338 = mul i32 %mul333711908, %add3334
  %add333811909 = add i32 %add3336, 1
  %add3340 = mul i32 %add3338, %add333811909
  %mul334111910 = add i32 %add3340, %add333811909
  %add3342 = mul i32 %mul334111910, %add3338
  %add334211911 = add i32 %add3340, 1
  %add3344 = mul i32 %add3342, %add334211911
  %mul334511912 = add i32 %add3344, %add334211911
  %add3346 = mul i32 %mul334511912, %add3342
  %add334611913 = add i32 %add3344, 1
  %add3348 = mul i32 %add3346, %add334611913
  %mul334911914 = add i32 %add3348, %add334611913
  %add3350 = mul i32 %mul334911914, %add3346
  %add335011915 = add i32 %add3348, 1
  %add3352 = mul i32 %add3350, %add335011915
  %mul335311916 = add i32 %add3352, %add335011915
  %add3354 = mul i32 %mul335311916, %add3350
  %add335411917 = add i32 %add3352, 1
  %add3356 = mul i32 %add3354, %add335411917
  %mul335711918 = add i32 %add3356, %add335411917
  %add3358 = mul i32 %mul335711918, %add3354
  %add335811919 = add i32 %add3356, 1
  %add3360 = mul i32 %add3358, %add335811919
  %mul336111920 = add i32 %add3360, %add335811919
  %add3362 = mul i32 %mul336111920, %add3358
  %add336211921 = add i32 %add3360, 1
  %add3364 = mul i32 %add3362, %add336211921
  %mul336511922 = add i32 %add3364, %add336211921
  %add3366 = mul i32 %mul336511922, %add3362
  %add336611923 = add i32 %add3364, 1
  %add3368 = mul i32 %add3366, %add336611923
  %mul336911924 = add i32 %add3368, %add336611923
  %add3370 = mul i32 %mul336911924, %add3366
  %add337011925 = add i32 %add3368, 1
  %add3372 = mul i32 %add3370, %add337011925
  %mul337311926 = add i32 %add3372, %add337011925
  %add3374 = mul i32 %mul337311926, %add3370
  %add337411927 = add i32 %add3372, 1
  %add3376 = mul i32 %add3374, %add337411927
  %mul337711928 = add i32 %add3376, %add337411927
  %add3378 = mul i32 %mul337711928, %add3374
  %add337811929 = add i32 %add3376, 1
  %add3380 = mul i32 %add3378, %add337811929
  %mul338111930 = add i32 %add3380, %add337811929
  %add3382 = mul i32 %mul338111930, %add3378
  %add338211931 = add i32 %add3380, 1
  %add3384 = mul i32 %add3382, %add338211931
  %mul338511932 = add i32 %add3384, %add338211931
  %add3386 = mul i32 %mul338511932, %add3382
  %add338611933 = add i32 %add3384, 1
  %add3388 = mul i32 %add3386, %add338611933
  %mul338911934 = add i32 %add3388, %add338611933
  %add3390 = mul i32 %mul338911934, %add3386
  %add339011935 = add i32 %add3388, 1
  %add3392 = mul i32 %add3390, %add339011935
  %mul339311936 = add i32 %add3392, %add339011935
  %add3394 = mul i32 %mul339311936, %add3390
  %add339411937 = add i32 %add3392, 1
  %add3396 = mul i32 %add3394, %add339411937
  %mul339711938 = add i32 %add3396, %add339411937
  %add3398 = mul i32 %mul339711938, %add3394
  %add339811939 = add i32 %add3396, 1
  %add3400 = mul i32 %add3398, %add339811939
  %mul340111940 = add i32 %add3400, %add339811939
  %add3402 = mul i32 %mul340111940, %add3398
  %add340211941 = add i32 %add3400, 1
  %add3404 = mul i32 %add3402, %add340211941
  %mul340511942 = add i32 %add3404, %add340211941
  %add3406 = mul i32 %mul340511942, %add3402
  %add340611943 = add i32 %add3404, 1
  %add3408 = mul i32 %add3406, %add340611943
  %mul340911944 = add i32 %add3408, %add340611943
  %add3410 = mul i32 %mul340911944, %add3406
  %add341011945 = add i32 %add3408, 1
  %add3412 = mul i32 %add3410, %add341011945
  %mul341311946 = add i32 %add3412, %add341011945
  %add3414 = mul i32 %mul341311946, %add3410
  %add341411947 = add i32 %add3412, 1
  %add3416 = mul i32 %add3414, %add341411947
  %mul341711948 = add i32 %add3416, %add341411947
  %add3418 = mul i32 %mul341711948, %add3414
  %add341811949 = add i32 %add3416, 1
  %add3420 = mul i32 %add3418, %add341811949
  %mul342111950 = add i32 %add3420, %add341811949
  %add3422 = mul i32 %mul342111950, %add3418
  %add342211951 = add i32 %add3420, 1
  %add3424 = mul i32 %add3422, %add342211951
  %mul342511952 = add i32 %add3424, %add342211951
  %add3426 = mul i32 %mul342511952, %add3422
  %add342611953 = add i32 %add3424, 1
  %add3428 = mul i32 %add3426, %add342611953
  %mul342911954 = add i32 %add3428, %add342611953
  %add3430 = mul i32 %mul342911954, %add3426
  %add343011955 = add i32 %add3428, 1
  %add3432 = mul i32 %add3430, %add343011955
  %mul343311956 = add i32 %add3432, %add343011955
  %add3434 = mul i32 %mul343311956, %add3430
  %add343411957 = add i32 %add3432, 1
  %add3436 = mul i32 %add3434, %add343411957
  %mul343711958 = add i32 %add3436, %add343411957
  %add3438 = mul i32 %mul343711958, %add3434
  %add343811959 = add i32 %add3436, 1
  %add3440 = mul i32 %add3438, %add343811959
  %mul344111960 = add i32 %add3440, %add343811959
  %add3442 = mul i32 %mul344111960, %add3438
  %add344211961 = add i32 %add3440, 1
  %add3444 = mul i32 %add3442, %add344211961
  %mul344511962 = add i32 %add3444, %add344211961
  %add3446 = mul i32 %mul344511962, %add3442
  %add344611963 = add i32 %add3444, 1
  %add3448 = mul i32 %add3446, %add344611963
  %mul344911964 = add i32 %add3448, %add344611963
  %add3450 = mul i32 %mul344911964, %add3446
  %add345011965 = add i32 %add3448, 1
  %add3452 = mul i32 %add3450, %add345011965
  %mul345311966 = add i32 %add3452, %add345011965
  %add3454 = mul i32 %mul345311966, %add3450
  %add345411967 = add i32 %add3452, 1
  %add3456 = mul i32 %add3454, %add345411967
  %mul345711968 = add i32 %add3456, %add345411967
  %add3458 = mul i32 %mul345711968, %add3454
  %add345811969 = add i32 %add3456, 1
  %add3460 = mul i32 %add3458, %add345811969
  %mul346111970 = add i32 %add3460, %add345811969
  %add3462 = mul i32 %mul346111970, %add3458
  %add346211971 = add i32 %add3460, 1
  %add3464 = mul i32 %add3462, %add346211971
  %mul346511972 = add i32 %add3464, %add346211971
  %add3466 = mul i32 %mul346511972, %add3462
  %add346611973 = add i32 %add3464, 1
  %add3468 = mul i32 %add3466, %add346611973
  %mul346911974 = add i32 %add3468, %add346611973
  %add3470 = mul i32 %mul346911974, %add3466
  %add347011975 = add i32 %add3468, 1
  %add3472 = mul i32 %add3470, %add347011975
  %mul347311976 = add i32 %add3472, %add347011975
  %add3474 = mul i32 %mul347311976, %add3470
  %add347411977 = add i32 %add3472, 1
  %add3476 = mul i32 %add3474, %add347411977
  %mul347711978 = add i32 %add3476, %add347411977
  %add3478 = mul i32 %mul347711978, %add3474
  %add347811979 = add i32 %add3476, 1
  %add3480 = mul i32 %add3478, %add347811979
  %mul348111980 = add i32 %add3480, %add347811979
  %add3482 = mul i32 %mul348111980, %add3478
  %add348211981 = add i32 %add3480, 1
  %add3484 = mul i32 %add3482, %add348211981
  %mul348511982 = add i32 %add3484, %add348211981
  %add3486 = mul i32 %mul348511982, %add3482
  %add348611983 = add i32 %add3484, 1
  %add3488 = mul i32 %add3486, %add348611983
  %mul348911984 = add i32 %add3488, %add348611983
  %add3490 = mul i32 %mul348911984, %add3486
  %add349011985 = add i32 %add3488, 1
  %add3492 = mul i32 %add3490, %add349011985
  %mul349311986 = add i32 %add3492, %add349011985
  %add3494 = mul i32 %mul349311986, %add3490
  %add349411987 = add i32 %add3492, 1
  %add3496 = mul i32 %add3494, %add349411987
  %mul349711988 = add i32 %add3496, %add349411987
  %add3498 = mul i32 %mul349711988, %add3494
  %add349811989 = add i32 %add3496, 1
  %add3500 = mul i32 %add3498, %add349811989
  %mul350111990 = add i32 %add3500, %add349811989
  %add3502 = mul i32 %mul350111990, %add3498
  %add350211991 = add i32 %add3500, 1
  %add3504 = mul i32 %add3502, %add350211991
  %mul350511992 = add i32 %add3504, %add350211991
  %add3506 = mul i32 %mul350511992, %add3502
  %add350611993 = add i32 %add3504, 1
  %add3508 = mul i32 %add3506, %add350611993
  %mul350911994 = add i32 %add3508, %add350611993
  %add3510 = mul i32 %mul350911994, %add3506
  %add351011995 = add i32 %add3508, 1
  %add3512 = mul i32 %add3510, %add351011995
  %mul351311996 = add i32 %add3512, %add351011995
  %add3514 = mul i32 %mul351311996, %add3510
  %add351411997 = add i32 %add3512, 1
  %add3516 = mul i32 %add3514, %add351411997
  %mul351711998 = add i32 %add3516, %add351411997
  %add3518 = mul i32 %mul351711998, %add3514
  %add351811999 = add i32 %add3516, 1
  %add3520 = mul i32 %add3518, %add351811999
  %mul352112000 = add i32 %add3520, %add351811999
  %add3522 = mul i32 %mul352112000, %add3518
  %add352212001 = add i32 %add3520, 1
  %add3524 = mul i32 %add3522, %add352212001
  %mul352512002 = add i32 %add3524, %add352212001
  %add3526 = mul i32 %mul352512002, %add3522
  %add352612003 = add i32 %add3524, 1
  %add3528 = mul i32 %add3526, %add352612003
  %mul352912004 = add i32 %add3528, %add352612003
  %add3530 = mul i32 %mul352912004, %add3526
  %add353012005 = add i32 %add3528, 1
  %add3532 = mul i32 %add3530, %add353012005
  %mul353312006 = add i32 %add3532, %add353012005
  %add3534 = mul i32 %mul353312006, %add3530
  %add353412007 = add i32 %add3532, 1
  %add3536 = mul i32 %add3534, %add353412007
  %mul353712008 = add i32 %add3536, %add353412007
  %add3538 = mul i32 %mul353712008, %add3534
  %add353812009 = add i32 %add3536, 1
  %add3540 = mul i32 %add3538, %add353812009
  %mul354112010 = add i32 %add3540, %add353812009
  %add3542 = mul i32 %mul354112010, %add3538
  %add354212011 = add i32 %add3540, 1
  %add3544 = mul i32 %add3542, %add354212011
  %mul354512012 = add i32 %add3544, %add354212011
  %add3546 = mul i32 %mul354512012, %add3542
  %add354612013 = add i32 %add3544, 1
  %add3548 = mul i32 %add3546, %add354612013
  %mul354912014 = add i32 %add3548, %add354612013
  %add3550 = mul i32 %mul354912014, %add3546
  %add355012015 = add i32 %add3548, 1
  %add3552 = mul i32 %add3550, %add355012015
  %mul355312016 = add i32 %add3552, %add355012015
  %add3554 = mul i32 %mul355312016, %add3550
  %add355412017 = add i32 %add3552, 1
  %add3556 = mul i32 %add3554, %add355412017
  %mul355712018 = add i32 %add3556, %add355412017
  %add3558 = mul i32 %mul355712018, %add3554
  %add355812019 = add i32 %add3556, 1
  %add3560 = mul i32 %add3558, %add355812019
  %mul356112020 = add i32 %add3560, %add355812019
  %add3562 = mul i32 %mul356112020, %add3558
  %add356212021 = add i32 %add3560, 1
  %add3564 = mul i32 %add3562, %add356212021
  %mul356512022 = add i32 %add3564, %add356212021
  %add3566 = mul i32 %mul356512022, %add3562
  %add356612023 = add i32 %add3564, 1
  %add3568 = mul i32 %add3566, %add356612023
  %mul356912024 = add i32 %add3568, %add356612023
  %add3570 = mul i32 %mul356912024, %add3566
  %add357012025 = add i32 %add3568, 1
  %add3572 = mul i32 %add3570, %add357012025
  %mul357312026 = add i32 %add3572, %add357012025
  %add3574 = mul i32 %mul357312026, %add3570
  %add357412027 = add i32 %add3572, 1
  %add3576 = mul i32 %add3574, %add357412027
  %mul357712028 = add i32 %add3576, %add357412027
  %add3578 = mul i32 %mul357712028, %add3574
  %add357812029 = add i32 %add3576, 1
  %add3580 = mul i32 %add3578, %add357812029
  %mul358112030 = add i32 %add3580, %add357812029
  %add3582 = mul i32 %mul358112030, %add3578
  %add358212031 = add i32 %add3580, 1
  %add3584 = mul i32 %add3582, %add358212031
  %mul358512032 = add i32 %add3584, %add358212031
  %add3586 = mul i32 %mul358512032, %add3582
  %add358612033 = add i32 %add3584, 1
  %add3588 = mul i32 %add3586, %add358612033
  %mul358912034 = add i32 %add3588, %add358612033
  %add3590 = mul i32 %mul358912034, %add3586
  %add359012035 = add i32 %add3588, 1
  %add3592 = mul i32 %add3590, %add359012035
  %mul359312036 = add i32 %add3592, %add359012035
  %add3594 = mul i32 %mul359312036, %add3590
  %add359412037 = add i32 %add3592, 1
  %add3596 = mul i32 %add3594, %add359412037
  %mul359712038 = add i32 %add3596, %add359412037
  %add3598 = mul i32 %mul359712038, %add3594
  %add359812039 = add i32 %add3596, 1
  %add3600 = mul i32 %add3598, %add359812039
  %mul360112040 = add i32 %add3600, %add359812039
  %add3602 = mul i32 %mul360112040, %add3598
  %add360212041 = add i32 %add3600, 1
  %add3604 = mul i32 %add3602, %add360212041
  %mul360512042 = add i32 %add3604, %add360212041
  %add3606 = mul i32 %mul360512042, %add3602
  %add360612043 = add i32 %add3604, 1
  %add3608 = mul i32 %add3606, %add360612043
  %mul360912044 = add i32 %add3608, %add360612043
  %add3610 = mul i32 %mul360912044, %add3606
  %add361012045 = add i32 %add3608, 1
  %add3612 = mul i32 %add3610, %add361012045
  %mul361312046 = add i32 %add3612, %add361012045
  %add3614 = mul i32 %mul361312046, %add3610
  %add361412047 = add i32 %add3612, 1
  %add3616 = mul i32 %add3614, %add361412047
  %mul361712048 = add i32 %add3616, %add361412047
  %add3618 = mul i32 %mul361712048, %add3614
  %add361812049 = add i32 %add3616, 1
  %add3620 = mul i32 %add3618, %add361812049
  %mul362112050 = add i32 %add3620, %add361812049
  %add3622 = mul i32 %mul362112050, %add3618
  %add362212051 = add i32 %add3620, 1
  %add3624 = mul i32 %add3622, %add362212051
  %mul362512052 = add i32 %add3624, %add362212051
  %add3626 = mul i32 %mul362512052, %add3622
  %add362612053 = add i32 %add3624, 1
  %add3628 = mul i32 %add3626, %add362612053
  %mul362912054 = add i32 %add3628, %add362612053
  %add3630 = mul i32 %mul362912054, %add3626
  %add363012055 = add i32 %add3628, 1
  %add3632 = mul i32 %add3630, %add363012055
  %mul363312056 = add i32 %add3632, %add363012055
  %add3634 = mul i32 %mul363312056, %add3630
  %add363412057 = add i32 %add3632, 1
  %add3636 = mul i32 %add3634, %add363412057
  %mul363712058 = add i32 %add3636, %add363412057
  %add3638 = mul i32 %mul363712058, %add3634
  %add363812059 = add i32 %add3636, 1
  %add3640 = mul i32 %add3638, %add363812059
  %mul364112060 = add i32 %add3640, %add363812059
  %add3642 = mul i32 %mul364112060, %add3638
  %add364212061 = add i32 %add3640, 1
  %add3644 = mul i32 %add3642, %add364212061
  %mul364512062 = add i32 %add3644, %add364212061
  %add3646 = mul i32 %mul364512062, %add3642
  %add364612063 = add i32 %add3644, 1
  %add3648 = mul i32 %add3646, %add364612063
  %mul364912064 = add i32 %add3648, %add364612063
  %add3650 = mul i32 %mul364912064, %add3646
  %add365012065 = add i32 %add3648, 1
  %add3652 = mul i32 %add3650, %add365012065
  %mul365312066 = add i32 %add3652, %add365012065
  %add3654 = mul i32 %mul365312066, %add3650
  %add365412067 = add i32 %add3652, 1
  %add3656 = mul i32 %add3654, %add365412067
  %mul365712068 = add i32 %add3656, %add365412067
  %add3658 = mul i32 %mul365712068, %add3654
  %add365812069 = add i32 %add3656, 1
  %add3660 = mul i32 %add3658, %add365812069
  %mul366112070 = add i32 %add3660, %add365812069
  %add3662 = mul i32 %mul366112070, %add3658
  %add366212071 = add i32 %add3660, 1
  %add3664 = mul i32 %add3662, %add366212071
  %mul366512072 = add i32 %add3664, %add366212071
  %add3666 = mul i32 %mul366512072, %add3662
  %add366612073 = add i32 %add3664, 1
  %add3668 = mul i32 %add3666, %add366612073
  %mul366912074 = add i32 %add3668, %add366612073
  %add3670 = mul i32 %mul366912074, %add3666
  %add367012075 = add i32 %add3668, 1
  %add3672 = mul i32 %add3670, %add367012075
  %mul367312076 = add i32 %add3672, %add367012075
  %add3674 = mul i32 %mul367312076, %add3670
  %add367412077 = add i32 %add3672, 1
  %add3676 = mul i32 %add3674, %add367412077
  %mul367712078 = add i32 %add3676, %add367412077
  %add3678 = mul i32 %mul367712078, %add3674
  %add367812079 = add i32 %add3676, 1
  %add3680 = mul i32 %add3678, %add367812079
  %mul368112080 = add i32 %add3680, %add367812079
  %add3682 = mul i32 %mul368112080, %add3678
  %add368212081 = add i32 %add3680, 1
  %add3684 = mul i32 %add3682, %add368212081
  %mul368512082 = add i32 %add3684, %add368212081
  %add3686 = mul i32 %mul368512082, %add3682
  %add368612083 = add i32 %add3684, 1
  %add3688 = mul i32 %add3686, %add368612083
  %mul368912084 = add i32 %add3688, %add368612083
  %add3690 = mul i32 %mul368912084, %add3686
  %add369012085 = add i32 %add3688, 1
  %add3692 = mul i32 %add3690, %add369012085
  %mul369312086 = add i32 %add3692, %add369012085
  %add3694 = mul i32 %mul369312086, %add3690
  %add369412087 = add i32 %add3692, 1
  %add3696 = mul i32 %add3694, %add369412087
  %mul369712088 = add i32 %add3696, %add369412087
  %add3698 = mul i32 %mul369712088, %add3694
  %add369812089 = add i32 %add3696, 1
  %add3700 = mul i32 %add3698, %add369812089
  %mul370112090 = add i32 %add3700, %add369812089
  %add3702 = mul i32 %mul370112090, %add3698
  %add370212091 = add i32 %add3700, 1
  %add3704 = mul i32 %add3702, %add370212091
  %mul370512092 = add i32 %add3704, %add370212091
  %add3706 = mul i32 %mul370512092, %add3702
  %add370612093 = add i32 %add3704, 1
  %add3708 = mul i32 %add3706, %add370612093
  %mul370912094 = add i32 %add3708, %add370612093
  %add3710 = mul i32 %mul370912094, %add3706
  %add371012095 = add i32 %add3708, 1
  %add3712 = mul i32 %add3710, %add371012095
  %mul371312096 = add i32 %add3712, %add371012095
  %add3714 = mul i32 %mul371312096, %add3710
  %add371412097 = add i32 %add3712, 1
  %add3716 = mul i32 %add3714, %add371412097
  %mul371712098 = add i32 %add3716, %add371412097
  %add3718 = mul i32 %mul371712098, %add3714
  %add371812099 = add i32 %add3716, 1
  %add3720 = mul i32 %add3718, %add371812099
  %mul372112100 = add i32 %add3720, %add371812099
  %add3722 = mul i32 %mul372112100, %add3718
  %add372212101 = add i32 %add3720, 1
  %add3724 = mul i32 %add3722, %add372212101
  %mul372512102 = add i32 %add3724, %add372212101
  %add3726 = mul i32 %mul372512102, %add3722
  %add372612103 = add i32 %add3724, 1
  %add3728 = mul i32 %add3726, %add372612103
  %mul372912104 = add i32 %add3728, %add372612103
  %add3730 = mul i32 %mul372912104, %add3726
  %add373012105 = add i32 %add3728, 1
  %add3732 = mul i32 %add3730, %add373012105
  %mul373312106 = add i32 %add3732, %add373012105
  %add3734 = mul i32 %mul373312106, %add3730
  %add373412107 = add i32 %add3732, 1
  %add3736 = mul i32 %add3734, %add373412107
  %mul373712108 = add i32 %add3736, %add373412107
  %add3738 = mul i32 %mul373712108, %add3734
  %add373812109 = add i32 %add3736, 1
  %add3740 = mul i32 %add3738, %add373812109
  %mul374112110 = add i32 %add3740, %add373812109
  %add3742 = mul i32 %mul374112110, %add3738
  %add374212111 = add i32 %add3740, 1
  %add3744 = mul i32 %add3742, %add374212111
  %mul374512112 = add i32 %add3744, %add374212111
  %add3746 = mul i32 %mul374512112, %add3742
  %add374612113 = add i32 %add3744, 1
  %add3748 = mul i32 %add3746, %add374612113
  %mul374912114 = add i32 %add3748, %add374612113
  %add3750 = mul i32 %mul374912114, %add3746
  %add375012115 = add i32 %add3748, 1
  %add3752 = mul i32 %add3750, %add375012115
  %mul375312116 = add i32 %add3752, %add375012115
  %add3754 = mul i32 %mul375312116, %add3750
  %add375412117 = add i32 %add3752, 1
  %add3756 = mul i32 %add3754, %add375412117
  %mul375712118 = add i32 %add3756, %add375412117
  %add3758 = mul i32 %mul375712118, %add3754
  %add375812119 = add i32 %add3756, 1
  %add3760 = mul i32 %add3758, %add375812119
  %mul376112120 = add i32 %add3760, %add375812119
  %add3762 = mul i32 %mul376112120, %add3758
  %add376212121 = add i32 %add3760, 1
  %add3764 = mul i32 %add3762, %add376212121
  %mul376512122 = add i32 %add3764, %add376212121
  %add3766 = mul i32 %mul376512122, %add3762
  %add376612123 = add i32 %add3764, 1
  %add3768 = mul i32 %add3766, %add376612123
  %mul376912124 = add i32 %add3768, %add376612123
  %add3770 = mul i32 %mul376912124, %add3766
  %add377012125 = add i32 %add3768, 1
  %add3772 = mul i32 %add3770, %add377012125
  %mul377312126 = add i32 %add3772, %add377012125
  %add3774 = mul i32 %mul377312126, %add3770
  %add377412127 = add i32 %add3772, 1
  %add3776 = mul i32 %add3774, %add377412127
  %mul377712128 = add i32 %add3776, %add377412127
  %add3778 = mul i32 %mul377712128, %add3774
  %add377812129 = add i32 %add3776, 1
  %add3780 = mul i32 %add3778, %add377812129
  %mul378112130 = add i32 %add3780, %add377812129
  %add3782 = mul i32 %mul378112130, %add3778
  %add378212131 = add i32 %add3780, 1
  %add3784 = mul i32 %add3782, %add378212131
  %mul378512132 = add i32 %add3784, %add378212131
  %add3786 = mul i32 %mul378512132, %add3782
  %add378612133 = add i32 %add3784, 1
  %add3788 = mul i32 %add3786, %add378612133
  %mul378912134 = add i32 %add3788, %add378612133
  %add3790 = mul i32 %mul378912134, %add3786
  %add379012135 = add i32 %add3788, 1
  %add3792 = mul i32 %add3790, %add379012135
  %mul379312136 = add i32 %add3792, %add379012135
  %add3794 = mul i32 %mul379312136, %add3790
  %add379412137 = add i32 %add3792, 1
  %add3796 = mul i32 %add3794, %add379412137
  %mul379712138 = add i32 %add3796, %add379412137
  %add3798 = mul i32 %mul379712138, %add3794
  %add379812139 = add i32 %add3796, 1
  %add3800 = mul i32 %add3798, %add379812139
  %mul380112140 = add i32 %add3800, %add379812139
  %add3802 = mul i32 %mul380112140, %add3798
  %add380212141 = add i32 %add3800, 1
  %add3804 = mul i32 %add3802, %add380212141
  %mul380512142 = add i32 %add3804, %add380212141
  %add3806 = mul i32 %mul380512142, %add3802
  %add380612143 = add i32 %add3804, 1
  %add3808 = mul i32 %add3806, %add380612143
  %mul380912144 = add i32 %add3808, %add380612143
  %add3810 = mul i32 %mul380912144, %add3806
  %add381012145 = add i32 %add3808, 1
  %add3812 = mul i32 %add3810, %add381012145
  %mul381312146 = add i32 %add3812, %add381012145
  %add3814 = mul i32 %mul381312146, %add3810
  %add381412147 = add i32 %add3812, 1
  %add3816 = mul i32 %add3814, %add381412147
  %mul381712148 = add i32 %add3816, %add381412147
  %add3818 = mul i32 %mul381712148, %add3814
  %add381812149 = add i32 %add3816, 1
  %add3820 = mul i32 %add3818, %add381812149
  %mul382112150 = add i32 %add3820, %add381812149
  %add3822 = mul i32 %mul382112150, %add3818
  %add382212151 = add i32 %add3820, 1
  %add3824 = mul i32 %add3822, %add382212151
  %mul382512152 = add i32 %add3824, %add382212151
  %add3826 = mul i32 %mul382512152, %add3822
  %add382612153 = add i32 %add3824, 1
  %add3828 = mul i32 %add3826, %add382612153
  %mul382912154 = add i32 %add3828, %add382612153
  %add3830 = mul i32 %mul382912154, %add3826
  %add383012155 = add i32 %add3828, 1
  %add3832 = mul i32 %add3830, %add383012155
  %mul383312156 = add i32 %add3832, %add383012155
  %add3834 = mul i32 %mul383312156, %add3830
  %add383412157 = add i32 %add3832, 1
  %add3836 = mul i32 %add3834, %add383412157
  %mul383712158 = add i32 %add3836, %add383412157
  %add3838 = mul i32 %mul383712158, %add3834
  %add383812159 = add i32 %add3836, 1
  %add3840 = mul i32 %add3838, %add383812159
  %mul384112160 = add i32 %add3840, %add383812159
  %add3842 = mul i32 %mul384112160, %add3838
  %add384212161 = add i32 %add3840, 1
  %add3844 = mul i32 %add3842, %add384212161
  %mul384512162 = add i32 %add3844, %add384212161
  %add3846 = mul i32 %mul384512162, %add3842
  %add384612163 = add i32 %add3844, 1
  %add3848 = mul i32 %add3846, %add384612163
  %mul384912164 = add i32 %add3848, %add384612163
  %add3850 = mul i32 %mul384912164, %add3846
  %add385012165 = add i32 %add3848, 1
  %add3852 = mul i32 %add3850, %add385012165
  %mul385312166 = add i32 %add3852, %add385012165
  %add3854 = mul i32 %mul385312166, %add3850
  %add385412167 = add i32 %add3852, 1
  %add3856 = mul i32 %add3854, %add385412167
  %mul385712168 = add i32 %add3856, %add385412167
  %add3858 = mul i32 %mul385712168, %add3854
  %add385812169 = add i32 %add3856, 1
  %add3860 = mul i32 %add3858, %add385812169
  %mul386112170 = add i32 %add3860, %add385812169
  %add3862 = mul i32 %mul386112170, %add3858
  %add386212171 = add i32 %add3860, 1
  %add3864 = mul i32 %add3862, %add386212171
  %mul386512172 = add i32 %add3864, %add386212171
  %add3866 = mul i32 %mul386512172, %add3862
  %add386612173 = add i32 %add3864, 1
  %add3868 = mul i32 %add3866, %add386612173
  %mul386912174 = add i32 %add3868, %add386612173
  %add3870 = mul i32 %mul386912174, %add3866
  %add387012175 = add i32 %add3868, 1
  %add3872 = mul i32 %add3870, %add387012175
  %mul387312176 = add i32 %add3872, %add387012175
  %add3874 = mul i32 %mul387312176, %add3870
  %add387412177 = add i32 %add3872, 1
  %add3876 = mul i32 %add3874, %add387412177
  %mul387712178 = add i32 %add3876, %add387412177
  %add3878 = mul i32 %mul387712178, %add3874
  %add387812179 = add i32 %add3876, 1
  %add3880 = mul i32 %add3878, %add387812179
  %mul388112180 = add i32 %add3880, %add387812179
  %add3882 = mul i32 %mul388112180, %add3878
  %add388212181 = add i32 %add3880, 1
  %add3884 = mul i32 %add3882, %add388212181
  %mul388512182 = add i32 %add3884, %add388212181
  %add3886 = mul i32 %mul388512182, %add3882
  %add388612183 = add i32 %add3884, 1
  %add3888 = mul i32 %add3886, %add388612183
  %mul388912184 = add i32 %add3888, %add388612183
  %add3890 = mul i32 %mul388912184, %add3886
  %add389012185 = add i32 %add3888, 1
  %add3892 = mul i32 %add3890, %add389012185
  %mul389312186 = add i32 %add3892, %add389012185
  %add3894 = mul i32 %mul389312186, %add3890
  %add389412187 = add i32 %add3892, 1
  %add3896 = mul i32 %add3894, %add389412187
  %mul389712188 = add i32 %add3896, %add389412187
  %add3898 = mul i32 %mul389712188, %add3894
  %add389812189 = add i32 %add3896, 1
  %add3900 = mul i32 %add3898, %add389812189
  %mul390112190 = add i32 %add3900, %add389812189
  %add3902 = mul i32 %mul390112190, %add3898
  %add390212191 = add i32 %add3900, 1
  %add3904 = mul i32 %add3902, %add390212191
  %mul390512192 = add i32 %add3904, %add390212191
  %add3906 = mul i32 %mul390512192, %add3902
  %add390612193 = add i32 %add3904, 1
  %add3908 = mul i32 %add3906, %add390612193
  %mul390912194 = add i32 %add3908, %add390612193
  %add3910 = mul i32 %mul390912194, %add3906
  %add391012195 = add i32 %add3908, 1
  %add3912 = mul i32 %add3910, %add391012195
  %mul391312196 = add i32 %add3912, %add391012195
  %add3914 = mul i32 %mul391312196, %add3910
  %add391412197 = add i32 %add3912, 1
  %add3916 = mul i32 %add3914, %add391412197
  %mul391712198 = add i32 %add3916, %add391412197
  %add3918 = mul i32 %mul391712198, %add3914
  %add391812199 = add i32 %add3916, 1
  %add3920 = mul i32 %add3918, %add391812199
  %mul392112200 = add i32 %add3920, %add391812199
  %add3922 = mul i32 %mul392112200, %add3918
  %add392212201 = add i32 %add3920, 1
  %add3924 = mul i32 %add3922, %add392212201
  %mul392512202 = add i32 %add3924, %add392212201
  %add3926 = mul i32 %mul392512202, %add3922
  %add392612203 = add i32 %add3924, 1
  %add3928 = mul i32 %add3926, %add392612203
  %mul392912204 = add i32 %add3928, %add392612203
  %add3930 = mul i32 %mul392912204, %add3926
  %add393012205 = add i32 %add3928, 1
  %add3932 = mul i32 %add3930, %add393012205
  %mul393312206 = add i32 %add3932, %add393012205
  %add3934 = mul i32 %mul393312206, %add3930
  %add393412207 = add i32 %add3932, 1
  %add3936 = mul i32 %add3934, %add393412207
  %mul393712208 = add i32 %add3936, %add393412207
  %add3938 = mul i32 %mul393712208, %add3934
  %add393812209 = add i32 %add3936, 1
  %add3940 = mul i32 %add3938, %add393812209
  %mul394112210 = add i32 %add3940, %add393812209
  %add3942 = mul i32 %mul394112210, %add3938
  %add394212211 = add i32 %add3940, 1
  %add3944 = mul i32 %add3942, %add394212211
  %mul394512212 = add i32 %add3944, %add394212211
  %add3946 = mul i32 %mul394512212, %add3942
  %add394612213 = add i32 %add3944, 1
  %add3948 = mul i32 %add3946, %add394612213
  %mul394912214 = add i32 %add3948, %add394612213
  %add3950 = mul i32 %mul394912214, %add3946
  %add395012215 = add i32 %add3948, 1
  %add3952 = mul i32 %add3950, %add395012215
  %mul395312216 = add i32 %add3952, %add395012215
  %add3954 = mul i32 %mul395312216, %add3950
  %add395412217 = add i32 %add3952, 1
  %add3956 = mul i32 %add3954, %add395412217
  %mul395712218 = add i32 %add3956, %add395412217
  %add3958 = mul i32 %mul395712218, %add3954
  %add395812219 = add i32 %add3956, 1
  %add3960 = mul i32 %add3958, %add395812219
  %mul396112220 = add i32 %add3960, %add395812219
  %add3962 = mul i32 %mul396112220, %add3958
  %add396212221 = add i32 %add3960, 1
  %add3964 = mul i32 %add3962, %add396212221
  %mul396512222 = add i32 %add3964, %add396212221
  %add3966 = mul i32 %mul396512222, %add3962
  %add396612223 = add i32 %add3964, 1
  %add3968 = mul i32 %add3966, %add396612223
  %mul396912224 = add i32 %add3968, %add396612223
  %add3970 = mul i32 %mul396912224, %add3966
  %add397012225 = add i32 %add3968, 1
  %add3972 = mul i32 %add3970, %add397012225
  %mul397312226 = add i32 %add3972, %add397012225
  %add3974 = mul i32 %mul397312226, %add3970
  %add397412227 = add i32 %add3972, 1
  %add3976 = mul i32 %add3974, %add397412227
  %mul397712228 = add i32 %add3976, %add397412227
  %add3978 = mul i32 %mul397712228, %add3974
  %add397812229 = add i32 %add3976, 1
  %add3980 = mul i32 %add3978, %add397812229
  %mul398112230 = add i32 %add3980, %add397812229
  %add3982 = mul i32 %mul398112230, %add3978
  %add398212231 = add i32 %add3980, 1
  %add3984 = mul i32 %add3982, %add398212231
  %mul398512232 = add i32 %add3984, %add398212231
  %add3986 = mul i32 %mul398512232, %add3982
  %add398612233 = add i32 %add3984, 1
  %add3988 = mul i32 %add3986, %add398612233
  %mul398912234 = add i32 %add3988, %add398612233
  %add3990 = mul i32 %mul398912234, %add3986
  %add399012235 = add i32 %add3988, 1
  %add3992 = mul i32 %add3990, %add399012235
  %mul399312236 = add i32 %add3992, %add399012235
  %add3994 = mul i32 %mul399312236, %add3990
  %add399412237 = add i32 %add3992, 1
  %add3996 = mul i32 %add3994, %add399412237
  %mul399712238 = add i32 %add3996, %add399412237
  %add3998 = mul i32 %mul399712238, %add3994
  %add399812239 = add i32 %add3996, 1
  %add4000 = mul i32 %add3998, %add399812239
  %mul400112240 = add i32 %add4000, %add399812239
  %add4002 = mul i32 %mul400112240, %add3998
  %add400212241 = add i32 %add4000, 1
  %add4004 = mul i32 %add4002, %add400212241
  %mul400512242 = add i32 %add4004, %add400212241
  %add4006 = mul i32 %mul400512242, %add4002
  %add400612243 = add i32 %add4004, 1
  %add4008 = mul i32 %add4006, %add400612243
  %mul400912244 = add i32 %add4008, %add400612243
  %add4010 = mul i32 %mul400912244, %add4006
  %add401012245 = add i32 %add4008, 1
  %add4012 = mul i32 %add4010, %add401012245
  %mul401312246 = add i32 %add4012, %add401012245
  %add4014 = mul i32 %mul401312246, %add4010
  %add401412247 = add i32 %add4012, 1
  %add4016 = mul i32 %add4014, %add401412247
  %mul401712248 = add i32 %add4016, %add401412247
  %add4018 = mul i32 %mul401712248, %add4014
  %add401812249 = add i32 %add4016, 1
  %add4020 = mul i32 %add4018, %add401812249
  %mul402112250 = add i32 %add4020, %add401812249
  %add4022 = mul i32 %mul402112250, %add4018
  %add402212251 = add i32 %add4020, 1
  %add4024 = mul i32 %add4022, %add402212251
  %mul402512252 = add i32 %add4024, %add402212251
  %add4026 = mul i32 %mul402512252, %add4022
  %add402612253 = add i32 %add4024, 1
  %add4028 = mul i32 %add4026, %add402612253
  %mul402912254 = add i32 %add4028, %add402612253
  %add4030 = mul i32 %mul402912254, %add4026
  %add403012255 = add i32 %add4028, 1
  %add4032 = mul i32 %add4030, %add403012255
  %mul403312256 = add i32 %add4032, %add403012255
  %add4034 = mul i32 %mul403312256, %add4030
  %add403412257 = add i32 %add4032, 1
  %add4036 = mul i32 %add4034, %add403412257
  %mul403712258 = add i32 %add4036, %add403412257
  %add4038 = mul i32 %mul403712258, %add4034
  %add403812259 = add i32 %add4036, 1
  %add4040 = mul i32 %add4038, %add403812259
  %mul404112260 = add i32 %add4040, %add403812259
  %add4042 = mul i32 %mul404112260, %add4038
  %add404212261 = add i32 %add4040, 1
  %add4044 = mul i32 %add4042, %add404212261
  %mul404512262 = add i32 %add4044, %add404212261
  %add4046 = mul i32 %mul404512262, %add4042
  %add404612263 = add i32 %add4044, 1
  %add4048 = mul i32 %add4046, %add404612263
  %mul404912264 = add i32 %add4048, %add404612263
  %add4050 = mul i32 %mul404912264, %add4046
  %add405012265 = add i32 %add4048, 1
  %add4052 = mul i32 %add4050, %add405012265
  %mul405312266 = add i32 %add4052, %add405012265
  %add4054 = mul i32 %mul405312266, %add4050
  %add405412267 = add i32 %add4052, 1
  %add4056 = mul i32 %add4054, %add405412267
  %mul405712268 = add i32 %add4056, %add405412267
  %add4058 = mul i32 %mul405712268, %add4054
  %add405812269 = add i32 %add4056, 1
  %add4060 = mul i32 %add4058, %add405812269
  %mul406112270 = add i32 %add4060, %add405812269
  %add4062 = mul i32 %mul406112270, %add4058
  %add406212271 = add i32 %add4060, 1
  %add4064 = mul i32 %add4062, %add406212271
  %mul406512272 = add i32 %add4064, %add406212271
  %add4066 = mul i32 %mul406512272, %add4062
  %add406612273 = add i32 %add4064, 1
  %add4068 = mul i32 %add4066, %add406612273
  %mul406912274 = add i32 %add4068, %add406612273
  %add4070 = mul i32 %mul406912274, %add4066
  %add407012275 = add i32 %add4068, 1
  %add4072 = mul i32 %add4070, %add407012275
  %mul407312276 = add i32 %add4072, %add407012275
  %add4074 = mul i32 %mul407312276, %add4070
  %add407412277 = add i32 %add4072, 1
  %add4076 = mul i32 %add4074, %add407412277
  %mul407712278 = add i32 %add4076, %add407412277
  %add4078 = mul i32 %mul407712278, %add4074
  %add407812279 = add i32 %add4076, 1
  %add4080 = mul i32 %add4078, %add407812279
  %mul408112280 = add i32 %add4080, %add407812279
  %add4082 = mul i32 %mul408112280, %add4078
  %add408212281 = add i32 %add4080, 1
  %add4084 = mul i32 %add4082, %add408212281
  %mul408512282 = add i32 %add4084, %add408212281
  %add4086 = mul i32 %mul408512282, %add4082
  %add408612283 = add i32 %add4084, 1
  %add4088 = mul i32 %add4086, %add408612283
  %mul408912284 = add i32 %add4088, %add408612283
  %add4090 = mul i32 %mul408912284, %add4086
  %add409012285 = add i32 %add4088, 1
  %add409212287 = add i32 %add4090, 1
  %mul409312286 = mul i32 %add4090, %add409012285
  %add4094 = mul i32 %mul409312286, %add409212287
  %i1 = tail call ptr addrspace(4) @llvm.amdgcn.implicitarg.ptr()
  %i2 = getelementptr inbounds i16, ptr addrspace(4) %i1, i64 6
  %i3 = load i16, ptr addrspace(4) %i2, align 4
  %i4 = zext i16 %i3 to i32
  %i5 = tail call i32 @llvm.amdgcn.workgroup.id.x()
  %i6 = mul i32 %i5, %i4
  %i7 = add i32 %i6, %i
  %i8 = getelementptr i64, ptr addrspace(4) %i1, i64 5
  %i9 = zext i32 %i7 to i64
  %i10 = load i64, ptr addrspace(4) %i8, align 8
  %i11 = add i64 %i10, %i9
  %arrayidx = getelementptr inbounds i32, ptr addrspace(1) %ptr, i64 %i11
  store i32 %add4094, ptr addrspace(1) %arrayidx, align 4
  ret void
}

declare align 4 ptr addrspace(4) @llvm.amdgcn.implicitarg.ptr() #1
declare i32 @llvm.amdgcn.workitem.id.x() #1
declare i32 @llvm.amdgcn.workgroup.id.x() #1

attributes #0 = { mustprogress nofree norecurse nosync nounwind willreturn memory(argmem: write) "amdgpu-flat-work-group-size"="1,256" "uniform-work-group-size"="true" }
attributes #1 = { nocallback nofree nosync nounwind speculatable willreturn memory(none) }

!0 = !{i32 0, i32 1024}

;; NOTE: These prefixes are unused and the list is autogenerated. Do not add tests below this line:
; GFX90A: {{.*}}
